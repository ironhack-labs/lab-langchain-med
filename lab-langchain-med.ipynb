{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d12218c",
   "metadata": {},
   "source": [
    "# Lab | LangChain Med\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- continue on with lesson 2' example, use different datasets to test what we did in class. Some datasets are suggested in the notebook but feel free to scout other datasets on HuggingFace or Kaggle.\n",
    "- Find another model on Hugging Face and compare it.\n",
    "- Modify the prompt to fit your selected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b28e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888472a",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "As you can see the notebook is ready to work with three different Datasets. Just uncomment the lines of the Dataset you want to use. \n",
    "\n",
    "I selected Datasets with News. Two of them have just a brief decription of the news, but the other contains the full text. \n",
    "\n",
    "As we are working in a free and limited space, I limited the number of news to use with the variable MAX_NEWS. Feel free to pull more if you have memory available. \n",
    "\n",
    "The name of the field containing the text of the new is stored in the variable *DOCUMENT* and the metadata in *TOPIC*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42699e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# news = pd.read_csv('/kaggle/input/topic-labeled-news-dataset/labelled_newscatcher_dataset.csv', sep=';')\n",
    "# MAX_NEWS = 1000\n",
    "# DOCUMENT=\"title\"\n",
    "# TOPIC=\"topic\"\n",
    "\n",
    "news = pd.read_csv(r'/notebooks/articles.csv')\n",
    "MAX_NEWS = 1000\n",
    "DOCUMENT=\"Article Body\"\n",
    "TOPIC=\"Article Body\"\n",
    "\n",
    "#news = pd.read_csv('/kaggle/input/mit-ai-news-published-till-2023/articles.csv')\n",
    "#MAX_NEWS = 100\n",
    "#DOCUMENT=\"Article Body\"\n",
    "#TOPIC=\"Article Header\"\n",
    "\n",
    "#news = \"PICK A DATASET\" #Ideally pick one from the commented ones above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd72e0b",
   "metadata": {},
   "source": [
    "ChromaDB requires that the data has a unique identifier. We can make it with this statement, which will create a new column called **Id**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b070a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Source</th>\n",
       "      <th>Article Header</th>\n",
       "      <th>Sub_Headings</th>\n",
       "      <th>Article Body</th>\n",
       "      <th>Url</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>July 7, 2023</td>\n",
       "      <td>Adam Zewe</td>\n",
       "      <td>MIT News Office</td>\n",
       "      <td>Learning the language of molecules to predict ...</td>\n",
       "      <td>This AI system only needs a small amount of da...</td>\n",
       "      <td>['Discovering new materials and drugs typicall...</td>\n",
       "      <td>https://news.mit.edu/2023/learning-language-mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>July 6, 2023</td>\n",
       "      <td>Alex Ouyang</td>\n",
       "      <td>Abdul Latif Jameel Clinic for Machine Learning...</td>\n",
       "      <td>MIT scientists build a system that can generat...</td>\n",
       "      <td>BioAutoMATED, an open-source, automated machin...</td>\n",
       "      <td>['Is it possible to build machine-learning mod...</td>\n",
       "      <td>https://news.mit.edu/2023/bioautomated-open-so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>June 30, 2023</td>\n",
       "      <td>Jennifer Michalowski</td>\n",
       "      <td>McGovern Institute for Brain Research</td>\n",
       "      <td>When computer vision works more like a brain, ...</td>\n",
       "      <td>Training artificial neural networks with data ...</td>\n",
       "      <td>['From cameras to self-driving cars, many of t...</td>\n",
       "      <td>https://news.mit.edu/2023/when-computer-vision...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>June 30, 2023</td>\n",
       "      <td>Mary Beth Gallagher</td>\n",
       "      <td>School of Engineering</td>\n",
       "      <td>Educating national security leaders on artific...</td>\n",
       "      <td>Experts from MIT’s School of Engineering, Schw...</td>\n",
       "      <td>['Understanding artificial intelligence and ho...</td>\n",
       "      <td>https://news.mit.edu/2023/educating-national-s...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>June 30, 2023</td>\n",
       "      <td>Adam Zewe</td>\n",
       "      <td>MIT News Office</td>\n",
       "      <td>Researchers teach an AI to write better chart ...</td>\n",
       "      <td>A new dataset can help scientists develop auto...</td>\n",
       "      <td>['Chart captions that explain complex trends a...</td>\n",
       "      <td>https://news.mit.edu/2023/researchers-chart-ca...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Published Date                Author  \\\n",
       "0           0   July 7, 2023             Adam Zewe   \n",
       "1           1   July 6, 2023           Alex Ouyang   \n",
       "2           2  June 30, 2023  Jennifer Michalowski   \n",
       "3           3  June 30, 2023   Mary Beth Gallagher   \n",
       "4           4  June 30, 2023             Adam Zewe   \n",
       "\n",
       "                                              Source  \\\n",
       "0                                    MIT News Office   \n",
       "1  Abdul Latif Jameel Clinic for Machine Learning...   \n",
       "2              McGovern Institute for Brain Research   \n",
       "3                              School of Engineering   \n",
       "4                                    MIT News Office   \n",
       "\n",
       "                                      Article Header  \\\n",
       "0  Learning the language of molecules to predict ...   \n",
       "1  MIT scientists build a system that can generat...   \n",
       "2  When computer vision works more like a brain, ...   \n",
       "3  Educating national security leaders on artific...   \n",
       "4  Researchers teach an AI to write better chart ...   \n",
       "\n",
       "                                        Sub_Headings  \\\n",
       "0  This AI system only needs a small amount of da...   \n",
       "1  BioAutoMATED, an open-source, automated machin...   \n",
       "2  Training artificial neural networks with data ...   \n",
       "3  Experts from MIT’s School of Engineering, Schw...   \n",
       "4  A new dataset can help scientists develop auto...   \n",
       "\n",
       "                                        Article Body  \\\n",
       "0  ['Discovering new materials and drugs typicall...   \n",
       "1  ['Is it possible to build machine-learning mod...   \n",
       "2  ['From cameras to self-driving cars, many of t...   \n",
       "3  ['Understanding artificial intelligence and ho...   \n",
       "4  ['Chart captions that explain complex trends a...   \n",
       "\n",
       "                                                 Url  id  \n",
       "0  https://news.mit.edu/2023/learning-language-mo...   0  \n",
       "1  https://news.mit.edu/2023/bioautomated-open-so...   1  \n",
       "2  https://news.mit.edu/2023/when-computer-vision...   2  \n",
       "3  https://news.mit.edu/2023/educating-national-s...   3  \n",
       "4  https://news.mit.edu/2023/researchers-chart-ca...   4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "news[\"id\"] = news.index\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beead1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Source</th>\n",
       "      <th>Article Header</th>\n",
       "      <th>Sub_Headings</th>\n",
       "      <th>Article Body</th>\n",
       "      <th>Url</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>July 7, 2023</td>\n",
       "      <td>Adam Zewe</td>\n",
       "      <td>MIT News Office</td>\n",
       "      <td>Learning the language of molecules to predict ...</td>\n",
       "      <td>This AI system only needs a small amount of da...</td>\n",
       "      <td>['Discovering new materials and drugs typicall...</td>\n",
       "      <td>https://news.mit.edu/2023/learning-language-mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>July 6, 2023</td>\n",
       "      <td>Alex Ouyang</td>\n",
       "      <td>Abdul Latif Jameel Clinic for Machine Learning...</td>\n",
       "      <td>MIT scientists build a system that can generat...</td>\n",
       "      <td>BioAutoMATED, an open-source, automated machin...</td>\n",
       "      <td>['Is it possible to build machine-learning mod...</td>\n",
       "      <td>https://news.mit.edu/2023/bioautomated-open-so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>June 30, 2023</td>\n",
       "      <td>Jennifer Michalowski</td>\n",
       "      <td>McGovern Institute for Brain Research</td>\n",
       "      <td>When computer vision works more like a brain, ...</td>\n",
       "      <td>Training artificial neural networks with data ...</td>\n",
       "      <td>['From cameras to self-driving cars, many of t...</td>\n",
       "      <td>https://news.mit.edu/2023/when-computer-vision...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>June 30, 2023</td>\n",
       "      <td>Mary Beth Gallagher</td>\n",
       "      <td>School of Engineering</td>\n",
       "      <td>Educating national security leaders on artific...</td>\n",
       "      <td>Experts from MIT’s School of Engineering, Schw...</td>\n",
       "      <td>['Understanding artificial intelligence and ho...</td>\n",
       "      <td>https://news.mit.edu/2023/educating-national-s...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>June 30, 2023</td>\n",
       "      <td>Adam Zewe</td>\n",
       "      <td>MIT News Office</td>\n",
       "      <td>Researchers teach an AI to write better chart ...</td>\n",
       "      <td>A new dataset can help scientists develop auto...</td>\n",
       "      <td>['Chart captions that explain complex trends a...</td>\n",
       "      <td>https://news.mit.edu/2023/researchers-chart-ca...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Published Date                Author  \\\n",
       "0           0   July 7, 2023             Adam Zewe   \n",
       "1           1   July 6, 2023           Alex Ouyang   \n",
       "2           2  June 30, 2023  Jennifer Michalowski   \n",
       "3           3  June 30, 2023   Mary Beth Gallagher   \n",
       "4           4  June 30, 2023             Adam Zewe   \n",
       "\n",
       "                                              Source  \\\n",
       "0                                    MIT News Office   \n",
       "1  Abdul Latif Jameel Clinic for Machine Learning...   \n",
       "2              McGovern Institute for Brain Research   \n",
       "3                              School of Engineering   \n",
       "4                                    MIT News Office   \n",
       "\n",
       "                                      Article Header  \\\n",
       "0  Learning the language of molecules to predict ...   \n",
       "1  MIT scientists build a system that can generat...   \n",
       "2  When computer vision works more like a brain, ...   \n",
       "3  Educating national security leaders on artific...   \n",
       "4  Researchers teach an AI to write better chart ...   \n",
       "\n",
       "                                        Sub_Headings  \\\n",
       "0  This AI system only needs a small amount of da...   \n",
       "1  BioAutoMATED, an open-source, automated machin...   \n",
       "2  Training artificial neural networks with data ...   \n",
       "3  Experts from MIT’s School of Engineering, Schw...   \n",
       "4  A new dataset can help scientists develop auto...   \n",
       "\n",
       "                                        Article Body  \\\n",
       "0  ['Discovering new materials and drugs typicall...   \n",
       "1  ['Is it possible to build machine-learning mod...   \n",
       "2  ['From cameras to self-driving cars, many of t...   \n",
       "3  ['Understanding artificial intelligence and ho...   \n",
       "4  ['Chart captions that explain complex trends a...   \n",
       "\n",
       "                                                 Url  id  \n",
       "0  https://news.mit.edu/2023/learning-language-mo...   0  \n",
       "1  https://news.mit.edu/2023/bioautomated-open-so...   1  \n",
       "2  https://news.mit.edu/2023/when-computer-vision...   2  \n",
       "3  https://news.mit.edu/2023/educating-national-s...   3  \n",
       "4  https://news.mit.edu/2023/researchers-chart-ca...   4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Because it is just a course we select a small portion of News.\n",
    "subset_news = news.head(MAX_NEWS)\n",
    "subset_news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e2081",
   "metadata": {},
   "source": [
    "## Import and configure the Vector Database\n",
    "I'm going to use ChromaDB, the most popular OpenSource embedding Database. \n",
    "\n",
    "First we need to import ChromaDB, and after that import the **Settings** class from **chromadb.config** module. This class allows us to change the setting for the ChromaDB system, and customize its behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654fb527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Downloading chromadb-0.5.15-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.10.14)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.115.3-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.3)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.9.0)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.19.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.1)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.66.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.6.0)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Downloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.60.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.9.0)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb)\n",
      "  Downloading tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting PyYAML>=6.0.0 (from chromadb)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-5.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Downloading orjson-3.10.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting httpx>=0.27.0 (from chromadb)\n",
      "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting rich>=10.11.0 (from chromadb)\n",
      "  Downloading rich-13.9.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (23.2)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.41.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.2.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from httpx>=0.27.0->chromadb) (2020.6.20)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.0->chromadb)\n",
      "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /usr/lib/python3/dist-packages (from httpx>=0.27.0->chromadb) (3.3)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->chromadb)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.26.2)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.57.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.31.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.7)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.23.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<=8.4.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.27.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Downloading opentelemetry_proto-1.27.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.0.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb)\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.17.2)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.20.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-13.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.8)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (1.0.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/lib/python3/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.8)\n",
      "Downloading chromadb-0.5.15-py3-none-any.whl (607 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.0/607.0 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bcrypt-4.2.0-cp39-abi3-manylinux_2_28_x86_64.whl (273 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading fastapi-0.115.3-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.0.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.19.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.27.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.27.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.27.0-py3-none-any.whl (17 kB)\n",
      "Downloading opentelemetry_proto-1.27.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl (11 kB)\n",
      "Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl (29 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl (15 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl (149 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_util_http-0.48b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.27.0-py3-none-any.whl (110 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.10.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading posthog-3.7.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.9.3-py3-none-any.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.2/242.2 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.9/220.9 kB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading starlette-0.41.0-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (425 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.5/425.5 kB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-13.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (164 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=4216fa247a38870f1292baf44d62852664674720305451ad2e780ee852f3033b\n",
      "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, monotonic, durationpy, websockets, uvloop, tenacity, PyYAML, python-dotenv, pyproject_hooks, orjson, opentelemetry-util-http, opentelemetry-proto, oauthlib, mmh3, mdurl, importlib-resources, importlib-metadata, humanfriendly, httptools, h11, googleapis-common-protos, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, markdown-it-py, httpcore, coloredlogs, build, rich, opentelemetry-semantic-conventions, opentelemetry-instrumentation, onnxruntime, kubernetes, httpx, fastapi, opentelemetry-sdk, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 5.4.1\n",
      "    Uninstalling PyYAML-5.4.1:\n",
      "      Successfully uninstalled PyYAML-5.4.1\n",
      "  Attempting uninstall: oauthlib\n",
      "    Found existing installation: oauthlib 3.2.0\n",
      "    Uninstalling oauthlib-3.2.0:\n",
      "      Successfully uninstalled oauthlib-3.2.0\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.6.4\n",
      "    Uninstalling importlib-metadata-4.6.4:\n",
      "      Successfully uninstalled importlib-metadata-4.6.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\n",
      "gradient 2.0.6 requires PyYAML==5.*, but you have pyyaml 6.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyYAML-6.0.2 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.15 coloredlogs-15.0.1 deprecated-1.2.14 durationpy-0.9 fastapi-0.115.3 googleapis-common-protos-1.65.0 h11-0.14.0 httpcore-1.0.6 httptools-0.6.4 httpx-0.27.2 humanfriendly-10.0 importlib-metadata-8.4.0 importlib-resources-6.4.5 kubernetes-31.0.0 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.0.1 monotonic-1.6 oauthlib-3.2.2 onnxruntime-1.19.2 opentelemetry-api-1.27.0 opentelemetry-exporter-otlp-proto-common-1.27.0 opentelemetry-exporter-otlp-proto-grpc-1.27.0 opentelemetry-instrumentation-0.48b0 opentelemetry-instrumentation-asgi-0.48b0 opentelemetry-instrumentation-fastapi-0.48b0 opentelemetry-proto-1.27.0 opentelemetry-sdk-1.27.0 opentelemetry-semantic-conventions-0.48b0 opentelemetry-util-http-0.48b0 orjson-3.10.10 posthog-3.7.0 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.0.1 rich-13.9.3 starlette-0.41.0 tenacity-9.0.0 uvicorn-0.32.0 uvloop-0.21.0 watchfiles-0.24.0 websockets-13.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c7605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50968e76",
   "metadata": {},
   "source": [
    "Now we need to create the seetings object calling the **Settings** function imported previously. We store the object in the variable **settings_chroma**.\n",
    "\n",
    "Is necessary to inform two parameters \n",
    "* chroma_db_impl. Here we specify the database implementation and the format how store the data. I choose ***duckdb***, because his high-performace. It operate primarly in memory. And is fully compatible with SQL. The store format ***parquet*** is good for tabular data. With good compression rates and performance. \n",
    "\n",
    "* persist_directory: It just contains the directory where the data will be stored. Is possible work without a directory and the data will be stored in memory without persistece, but Kaggle dosn't support that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157c0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"/path/to/persist/directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8903425",
   "metadata": {},
   "source": [
    "## Filling and Querying the ChromaDB Database\n",
    "The Data in ChromaDB is stored in collections. If the collection exist we need to delete it. \n",
    "\n",
    "In the next lines, we are creating the collection by calling the ***create_collection*** function in the ***chroma_client*** created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0addce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"news_collection\"\n",
    "if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n",
    "        chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "collection = chroma_client.create_collection(name=collection_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692246d0",
   "metadata": {},
   "source": [
    "It's time to add the data to the collection. Using the function ***add*** we need to inform, at least ***documents***, ***metadatas*** and ***ids***. \n",
    "* In the **document** we store the big text, it's a different column in each Dataset. \n",
    "* In **metadatas**, we can informa a list of topics. \n",
    "* In **id** we need to inform an unique identificator for each row. It MUST be unique! I'm creating the ID using the range of MAX_NEWS. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1db5978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.cache/chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz: 100%|██████████| 79.3M/79.3M [00:00<00:00, 106MiB/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "collection.add(\n",
    "    documents=subset_news[DOCUMENT].tolist(),\n",
    "    metadatas=[{TOPIC: topic} for topic in subset_news[TOPIC].tolist()],\n",
    "    ids=[f\"id{x}\" for x in range(MAX_NEWS)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94634753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id237', 'id572', 'id659', 'id539', 'id735', 'id9', 'id573', 'id100', 'id334', 'id602']], 'embeddings': None, 'documents': [[\"['When humans look at a scene, they see objects and the relationships between them. On top of your desk, there might be a laptop that is sitting to the left of a phone, which is in front of a computer monitor.', '', 'Many deep learning models struggle to see the world this way because they don’t understand the entangled relationships between individual objects. Without knowledge of these relationships, a robot designed to help someone in a kitchen would have difficulty following a command like “pick up the spatula that is to the left of the stove and place it on top of the cutting board.”', '', 'In an effort to solve this problem, MIT researchers have developed a model that understands the underlying relationships between objects in a scene. Their model represents individual relationships one at a time, then combines these representations to describe the overall scene. This enables the model to generate more accurate images from text descriptions, even when the scene includes several objects that are arranged in different relationships with one another.', '', 'This work could be applied in situations where industrial robots must perform intricate, multistep manipulation tasks, like stacking items in a warehouse or assembling appliances. It also moves the field one step closer to enabling machines that can learn from and interact with their environments more like humans do.', '', '“When I look at a table, I can’t say that there is an object at XYZ location. Our minds don’t work like that. In our minds, when we understand a scene, we really understand it based on the relationships between the objects. We think that by building a system that can understand the relationships between objects, we could use that system to more effectively manipulate and change our environments,” says Yilun Du, a PhD student in the Computer Science and Artificial Intelligence Laboratory (CSAIL) and co-lead author of the paper.', '', 'Du wrote the paper with co-lead authors Shuang Li, a CSAIL PhD student, and Nan Liu, a graduate student at the University of Illinois at Urbana-Champaign; as well as Joshua B. Tenenbaum, a professor of computational cognitive science in the Department of Brain and Cognitive Sciences and a member of CSAIL; and senior author Antonio Torralba, the Delta Electronics Professor of Electrical Engineering and Computer Science and a member of CSAIL. The research will be presented at the Conference on Neural Information Processing Systems in December.', '', 'One relationship at a time', '', 'The framework the researchers developed can generate an image of a scene based on a text description of objects and their relationships, like “A wood table to the left of a blue stool. A red couch to the right of a blue stool.”', '', 'Their system would break these sentences down into two smaller pieces that describe each individual relationship (“a wood table to the left of a blue stool” and “a red couch to the right of a blue stool”), and then model each part separately. Those pieces are then combined through an optimization process that generates an image of the scene.', '', 'The researchers used a machine-learning technique called energy-based models to represent the individual object relationships in a scene description. This technique enables them to use one energy-based model to encode each relational description, and then compose them together in a way that infers all objects and relationships.', '', 'By breaking the sentences down into shorter pieces for each relationship, the system can recombine them in a variety of ways, so it is better able to adapt to scene descriptions it hasn’t seen before, Li explains.', '', '“Other systems would take all the relations holistically and generate the image one-shot from the description. However, such approaches fail when we have out-of-distribution descriptions, such as descriptions with more relations, since these model can’t really adapt one shot to generate images containing more relationships. However, as we are composing these separate, smaller models together, we can model a larger number of relationships and adapt to novel combinations,” Du says.', '', 'The system also works in reverse — given an image, it can find text descriptions that match the relationships between objects in the scene. In addition, their model can be used to edit an image by rearranging the objects in the scene so they match a new description.', '', 'Understanding complex scenes', '', 'The researchers compared their model to other deep learning methods that were given text descriptions and tasked with generating images that displayed the corresponding objects and their relationships. In each instance, their model outperformed the baselines.', '', 'They also asked humans to evaluate whether the generated images matched the original scene description. In the most complex examples, where descriptions contained three relationships, 91 percent of participants concluded that the new model performed better.', '', '“One interesting thing we found is that for our model, we can increase our sentence from having one relation description to having two, or three, or even four descriptions, and our approach continues to be able to generate images that are correctly described by those descriptions, while other methods fail,” Du says.', '', 'The researchers also showed the model images of scenes it hadn’t seen before, as well as several different text descriptions of each image, and it was able to successfully identify the description that best matched the object relationships in the image.', '', 'And when the researchers gave the system two relational scene descriptions that described the same image but in different ways, the model was able to understand that the descriptions were equivalent.', '', 'The researchers were impressed by the robustness of their model, especially when working with descriptions it hadn’t encountered before.', '', '“This is very promising because that is closer to how humans work. Humans may only see several examples, but we can extract useful information from just those few examples and combine them together to create infinite combinations. And our model has such a property that allows it to learn from fewer data but generalize to more complex scenes or image generations,” Li says.', '', 'While these early results are encouraging, the researchers would like to see how their model performs on real-world images that are more complex, with noisy backgrounds and objects that are blocking one another.', '', 'They are also interested in eventually incorporating their model into robotics systems, enabling a robot to infer object relationships from videos and then apply this knowledge to manipulate objects in the world.', '', '“Developing visual representations that can deal with the compositional nature of the world around us is one of the key open problems in computer vision. This paper makes significant progress on this problem by proposing an energy-based model that explicitly models multiple relations among the objects depicted in the image. The results are really impressive,” says Josef Sivic, a distinguished researcher at the Czech Institute of Informatics, Robotics, and Cybernetics at Czech Technical University, who was not involved with this research.', '', 'This research is supported, in part, by Raytheon BBN Technologies Corp., Mitsubishi Electric Research Laboratory, the National Science Foundation, the Office of Naval Research, and the IBM Thomas J. Watson Research Center.']\", \"['Computers have become so pervasive in today’s world that preparing students to work and assume leadership roles in this shifting landscape requires giving them a better understanding of how computers work, how to use them, and how they affect every aspect of society. That’s the reasoning behind the creation of the new MIT Stephen A. Schwarzman College of Computing, and it was the theme of many of the presentations and panel discussions in this week’s three-day celebration of the new college.', '“We’re in the midst of a global transformation that’s catalyzed by the rapid acceleration of digital technologies, including unprecedented access to computation and data,” said Farnam Jahanian, president of Carnegie Mellon University, in a keynote address on Wednesday. “The scale and scope and pace of these advances are truly unprecedented in human history.”', '“The impact of these technologies is ubiquitous,” he said, “with a wide range of applications from health care to transportation, finance, energy, manufacturing, and far beyond. … The pace of innovation is accelerating dramatically.”', 'These changes require a profound rethinking of the role of education in this rapidly changing environment, Jahanian said. “Imagine a day when by integrating emerging technologies, such as AI-enabled learning techniques and inverted classrooms, we can achieve personalized, outcome-based education,” he said.', 'MIT Provost Martin Schmidt, in a discussion with reporters, said that in creating the MIT Schwarzman College of Computing, “one of the things that’s really critical to us is that not only should this advance computation, but it should really link to all the disciplines across the campus.” The college will “strengthen those disciplines in their use of these new tools,” he said, “but also when we learn things about how we apply those tools to the disciplines, that knowledge flows back … and informs the next generation” of computing research.', 'Schmidt added that in planning the new college, a key question was how MIT will deliver on its promise of making sure that the college “has in its DNA” an awareness of the societal impact of current and future advances in computing. This appreciation “should inform our educational agenda, what our undergraduates and graduates learn in the classroom, and it should inform our research agenda,” he said. “It should shape how the research is performed, and the kind of content we produce that informs policies and informs governments on how they should respond to the deployment of these technologies.”', 'The new college was founded partly in response to the fact that “there really was a transformation occurring across the campus,” with computation increasingly forming a key part of the work in amost all disciplines, Schmidt said. While about 40 percent of MIT students major in computer science, there was a clear need for an even greater integration of computation and data science early and deeply into every aspect of education.', 'Melissa Nobles, dean of MIT’s School of Humanities, Arts and Social Sciences, who also participated in the discussion, told reporters that students in that school were very excited to take part in this increased integration of their disciplines with computation. She cited examples of classes where mixed groups of computer science students and those majoring in arts, economics, or literature worked on problems that combined their different kinds of expertise. In one class, for example, the students studied in exhaustive detail the way writers of 19th century novels used male and female pronouns and how that related to the genders of the author and the main characters. The project required both computer expertise to analyze thousands of texts, and a knowledge of the literature in order to provide context for their findings.', 'Also during the discussion, Maria Klawe, president of Harvey Mudd College in California and another keynote speaker, pointed out that a deep understanding of computers and their impact is increasingly needed in a rapidly changing world where it is estimated that many of the jobs people perform today “are just going to disappear” within the next few decades. That makes interdisciplinary education more important than ever, she said.', 'Regarding the creation of the new college, she said, “I see this as an incredibly important step for MIT, and I think it’s going to influence other institutions to do similar things.”', 'The goals of the college reach far beyond just helping people in other disciplines to use computers more effectively, Nobles and others emphasized. It’s also important, they said, to make sure that the skills and knowledge from other fields flow back into computer science, influencing the ethical, political, and social implications of the work in that field — not just as an afterthought but as a fundamental part of thinking and planning.', 'For example, while it is tempting to make use of massive sets of data collected by social media, the use of such datasets can raise serious concerns about privacy and informed consent. Such issues may be relatively new territory for computer scientists, but they are longstanding issues that have been dealt with extensively by social scientists and philosophers whose expertise can help inform the data collection and analysis procedures.', 'The speakers at Wednesday’s symposium, representing many different fields and institutions, shared a sense of excitement about the potential for the MIT Schwarzman College of Computing to bring about significant innovations. “MIT continues to be a world-class institution that offers a distinctive education and research, of course,” Jahanian said in his keynote, “and this latest development will certainly increase its impact in this changing world.”']\", '[\\'“Who is Bram Stoker?” Those three words demonstrated the amazing potential of artificial intelligence. It was the answer to a final question in a particularly memorable 2011 episode of\\\\xa0Jeopardy!. The three competitors were former champions Brad Rutter and Ken Jennings, and Watson, a super computer developed by IBM. By answering the final question correctly, Watson became the first computer to beat a human on the famous quiz show.\\', \\'“In a way, Watson winning\\\\xa0Jeopardy!\\\\xa0seemed unfair to people,” says Jeehwan Kim, the Class ‘47 Career Development Professor and a faculty member of the MIT departments of Mechanical Engineering and Materials Science and Engineering. “At the time, Watson was connected to a super computer the size of a room while the human brain is just a few pounds. But the ability to replicate a human brain’s ability to learn is incredibly difficult.”\\', \\'Kim specializes in machine learning, which relies on algorithms to teach computers how to learn like a human brain. “Machine learning is cognitive computing,” he explains. “Your computer recognizes things without you telling the computer what it’s looking at.”\\', \\'Machine learning is one example of artificial intelligence in practice. While the phrase “machine learning” often conjures up science fiction typified in shows like \"Westworld\" or \"Battlestar Galactica,\" smart systems and devices are already pervasive in the fabric of our daily lives. Computers and phones use face recognition to unlock. Systems sense and adjust the temperature in our homes. Devices answer questions or play our favorite music on demand. Nearly every major car company has entered the race to develop a safe self-driving car.\\', \\'For any of these products to work, the software and hardware both have to work in perfect synchrony. Cameras, tactile sensors, radar, and light detection all need to function properly to feed information back to computers. Algorithms need to be designed so these machines can process these sensory data and make decisions based on the highest probability of success.\\', \\'Kim and the much of the faculty at MIT’s Department of Mechanical Engineering are creating new software that connects with hardware to create intelligent devices. Rather than building the sentient robots romanticized in popular culture, these researchers are working on projects that improve everyday life and make humans safer, more efficient, and better informed.\\', \\'Making portable devices smarter\\', \\'Jeehwan Kim holds up sheet of paper. If he and his team are successful, one day the power of a super computer like IBM’s Watson will be shrunk down to the size of one sheet of paper. “We are trying to build an actual physical neural network on a letter paper size,” explains Kim.\\', \\'To date, most neural networks have been software-based and made using the conventional method known as the Von Neumann computing method. Kim however has been using neuromorphic computing methods.\\', \\'“Neuromorphic computer means portable AI,” says Kim. “So, you build artificial neurons and synapses on a small-scale wafer.” The result is a so-called ‘brain-on-a-chip.’\\\\nRather than compute information from binary signaling, Kim’s neural network processes information like an analog device. Signals act like artificial neurons and move across thousands of arrays to particular cross points, which function like synapses. With thousands of arrays connected, vast amounts of information could be processed at once. For the first time, a portable piece of equipment could mimic the processing power of the brain.\\', \\'“The key with this method is you really need to control the artificial synapses well. When you’re talking about thousands of cross points, this poses challenges,” says Kim.\\', \\'According to Kim, the design and materials that have been used to make these artificial synapses thus far have been less than ideal. The amorphous materials used in neuromorphic chips make it incredibly difficult to control the ions once voltage is applied.\\', \\'In a\\\\xa0Nature Materials\\\\xa0study published earlier this year, Kim found that when his team made a chip out of silicon germanium they were able to control the current flowing out of the synapse and reduce variability to 1 percent. With control over how the synapses react to stimuli, it was time to put their chip to the test.\\', \\'“We envision that if we build up the actual neural network with material we can actually do handwriting recognition,” says Kim. In a computer simulation of their new artificial neural network design, they provided thousands of handwriting samples. Their neural network was able to accurately recognize 95 percent of the samples.\\', \\'“If you have a camera and an algorithm for the handwriting data set connected to our neural network, you can achieve handwriting recognition,” explains Kim.\\', \\'While building the physical neural network for handwriting recognition is the next step for Kim’s team, the potential of this new technology goes beyond handwriting recognition. “Shrinking the power of a super computer down to a portable size could revolutionize the products we use,” says Kim. “The potential is limitless – we can integrate this technology in our phones, computers, and robots to make them substantially smarter.”\\', \\'Making homes smarter\\', \\'While Kim is working on making our portable products more intelligent, Professor Sanjay Sarma and Research Scientist Josh Siegel hope to integrate smart devices within the biggest product we own: our homes.\\', \\'One evening, Sarma was in his home when one of his circuit breakers kept going off. This circuit breaker — known as an arc-fault circuit interrupter (AFCI) — was designed to shut off power when an electric arc is detected to prevent fires. While AFCIs are great at preventing fires, in Sarma’s case there didn’t seem to be an issue. “There was no discernible reason for it to keep going off,” recalls Sarma. “It was incredibly distracting.”\\', \"AFCIs are notorious for such ‘nuisance trips,’ which disconnect safe objects unnecessarily. Sarma, who also serves as MIT\\'s vice president for open learning, turned his frustration into opportunity. If he could embed the AFCI with smart technologies and connect it to the ‘internet of things,’ he could teach the circuit breaker to learn when a product is safe or when a product actually poses a fire risk.\\\\n“Think of it like a virus scanner,” explains Siegel. “Virus scanners are connected to a system that updates them with new virus definitions over time.” If Sarma and Siegel could embed similar technology into AFCIs, the circuit breakers could detect exactly what product is being plugged in and learn new object definitions over time.\", \\'If, for example, a new vacuum cleaner is plugged into the circuit breaker and the power shuts off without reason, the smart AFCI can learn that it’s safe and add it to a list of known safe objects. The AFCI learns these definitions with the aid of a neural network. But, unlike Jeewhan Kim’s physical neural network, this network is software-based.\\', \\'The neural network is built by gathering thousands of data points during simulations of arcing. Algorithms are then written to help the network assess its environment, recognize patterns, and make decisions based on the probability of achieving the desired outcome. With the help of a $35 microcomputer and a sound card, the team can cheaply integrate this technology into circuit breakers.\\', \"As the smart AFCI learns about the devices it encounters, it can simultaneously distribute its knowledge and definitions to every other home using the internet of things.\\\\n“Internet of things could just as well be called \\'intelligence of things,” says Sarma. “Smart, local technologies with the aid of the cloud can make our environments adaptive and the user experience seamless.”\", \\'Circuit breakers are just one of many ways neural networks can be used to make homes smarter. This kind of technology can control the temperature of your house, detect when there’s an anomaly such as an intrusion or burst pipe, and run diagnostics to see when things are in need of repair.\\', \\'“We’re developing software for monitoring mechanical systems that’s self-learned,” explains Siegel. “You don’t teach these devices all the rules, you teach them how to learn the rules.”\\', \\'Making manufacturing and design smarter\\\\nArtificial intelligence can not only help improve how users interact with products, devices, and environments. It can also improve the efficiency with which objects are made by optimizing the manufacturing and design process.\\', \\'“Growth in automation along with complementary technologies including 3-D printing, AI, and machine learning compels us to, in the long run, rethink how we design factories and supply chains,” says Associate Professor A. John Hart.\\', \\'Hart, who has done extensive research in 3-D printing, sees AI as a way to improve quality assurance in manufacturing. 3-D printers incorporating high-performance sensors, that are capable of analyzing data on the fly, will help accelerate the adoption of 3-D printing for mass production.\\', \\'“Having 3-D printers that learn how to create parts with fewer defects and inspect parts as they make them will be a really big deal — especially when the products you’re making have critical properties such as medical devices or parts for aircraft engines,” Hart explains.\\', \\'The very process of designing the structure of these parts can also benefit from intelligent software. Associate Professor Maria Yang has been looking at how designers can use automation tools to design more efficiently. “We call it hybrid intelligence for design,” says Yang. “The goal is to enable effective collaboration between intelligent tools and human designers.”\\', \\'In a recent study, Yang and graduate student Edward Burnell tested a design tool with varying levels of automation. Participants used the software to pick nodes for a 2-D truss of either a stop sign or a bridge. The tool would then automatically come up with optimized solutions based on intelligent algorithms for where to connect nodes and the width of each part.\\', \\'“We’re trying to design smart algorithms that fit with the ways designers already think,” says Burnell.\\', \\'Making robots smarter\\', \\'If there is anything on MIT’s campus that most closely resembles the futuristic robots of science fiction, it would be Professor Sangbae Kim’s robotic cheetah. The four-legged creature senses its surrounding environment using LIDAR technologies and moves in response to this information. Much like its namesake, it can run and leap over obstacles.\\', \\'Kim’s primary focus is on navigation. “We are building a very unique system specially designed for dynamic movement of the robot,” explains Kim. “I believe it is going to reshape the interactive robots in the world. You can think of all kinds of applications — medical, health care, factories.”\\', \\'Kim sees opportunity to eventually connect his research with the physical neural network his colleague Jeewhan Kim is working on. “If you want the cheetah to recognize people, voice, or gestures, you need a lot of learning and processing,” he says. “Jeewhan’s neural network hardware could possibly enable that someday.”\\', \\'Combining the power of a portable neural network with a robot capable of skillfully navigating its surroundings could open up a new world of possibilities for human and AI interaction. This is just one example of how researchers in mechanical engineering can one-day collaborate to bring AI research to next level.\\', \\'While we may be decades away from interacting with intelligent robots, artificial intelligence and machine learning has already found its way into our routines. Whether it’s using face and handwriting recognition to protect our information, tapping into the internet of things to keep our homes safe, or helping engineers build and design more efficiently, the benefits of AI technologies are pervasive.\\', \\'The science fiction fantasy of a world overtaken by robots is far from the truth. “There’s this romantic notion that everything is going to be automatic,” adds Maria Yang. “But I think the reality is you’re going to have tools that will work with people and help make their daily life a bit easier.”\\']', \"['To demystify artificial intelligence (AI) and unlock its benefits, the MIT Quest for Intelligence created the Quest Bridge to bring new intelligence tools and ideas into classrooms, labs, and homes. This spring, more than a dozen\\\\xa0Undergraduate Research Opportunities Program\\\\xa0(UROP) students joined the project in its mission to make AI accessible to all. Undergraduates worked on applications designed to teach kids about AI, improve access to AI programs and infrastructure, and harness AI to improve literacy and mental health. Six projects are highlighted here.', 'Project Athena for cloud computing', 'Training an AI model often requires remote servers to handle the heavy number-crunching, but getting projects to the cloud and back is no trivial matter. To simplify the process, an undergraduate club called the\\\\xa0MIT Machine Intelligence Community\\\\xa0(MIC) is building an interface modeled after MIT’s\\\\xa0Project Athena, which brought desktop computing to campus in the 1980s.', 'Amanda Li stumbled on the MIC during orientation last fall. She was looking for computer power to train an AI language model she had built to identify the nationality of non-native English speakers. The club had a bank of cloud credits, she learned, but no practical system for giving them away. A plan to build such a system, tentatively named “Monkey,” quickly took shape.', 'The system would have to send a student’s training data and AI model to the cloud, put the project in a queue, train the model, and send the finished project back to MIT. It would also have to track individual usage to make sure cloud credits were evenly distributed.', 'This spring, Monkey became a UROP project, and Li and sophomore Sebastian Rodriguez continued to work on it under the guidance of the Quest Bridge. So far, the students have created four modules in GitHub that will eventually become the foundation for a distributed system.', '“The coding isn’t the difficult part,” says Li. “It’s the exploring the server side of machine learning — Docker, Google Cloud, and the API. The most important thing I’ve learned is how to efficiently design and pipeline a project as big as this.”', 'A launch is expected\\\\xa0sometime next year.\\\\xa0“This is a huge project, with some timely problems that industry is also trying to address,” says Quest Bridge AI engineer Steven Shriver, who is supervising the project. “I have no doubt the students will figure it out: I’m here to help when they need it.”', 'An easy-to-use AI program for segmenting images', 'The ability to divide an image into its component parts underlies more complicated AI tasks like picking out proteins in pictures of microscopic cells, or stress fractures in shattered materials. Although fundamental, image segmentation programs are still hard for non-engineers to navigate. In a project with the Quest Bridge, first-year Marco Fleming helped to build a Jupyter notebook for image segmentation, part of the Quest Bridge’s broader mission to develop a set of AI building blocks that researchers can tailor for specific applications.', 'Fleming came to the project with self-taught coding skills, but no experience with machine learning, GitHub, or using a command-line interface. Working with Katherine Gallagher, an AI engineer with the Quest Bridge, and a more experienced classmate, Sule Kahraman, Fleming became fluent in convolutional neural networks, the workhorse for many machine vision tasks. “It’s kind of weird,” he explains. “You take a picture and do a lot of math to it, and the machine learns where the edges are.” Bound for a summer internship at Allstate this summer, Fleming says the project gave him a confidence boost.', 'His participation also benefitted the Quest Bridge, says Gallagher. “We’re developing these notebooks for people like Marco, a freshman with no machine learning experience. Seeing where Marco got tripped up was really valuable.”', 'An automated image classifier: no coding required', 'Anyone can build apps that impact the world. That’s the motto of the\\\\xa0MIT AppInventor, a programming environment founded by\\\\xa0Hal Abelson, the Class of 1922 Professor in MIT’s\\\\xa0Department of Electrical Engineering and Computer Science. Working in Abelson’s lab over Independent Activity Period, sophomore Yuria Utsumi developed a web interface that lets anyone build a deep learning classifier to sort pictures of, say, happy faces and sad faces, or apples and oranges.', 'In four steps, the\\\\xa0Image Classification Explorer\\\\xa0lets users label and upload their images to the web, select a customizable model, add testing data, and see the results. Utsumi built the app with a pre-trained classifier that she restructured to learn from a set of new and unfamiliar images. Once users retrain the classifier on the new images, they can upload the model to AppInventor to view it on their smartphones.', 'In a recent test run of the Explorer app, students at Boston Latin Academy uploaded selfies shot on their laptop webcams and classified their facial expressions. For Utsumi, who picked the project hoping to gain practical web development and programming skills, it was a moment of triumph. “This is the first time I’m solving an algorithms problem in real life!” she says.\\\\xa0“It was fun to see the students become more comfortable with machine learning,” she adds. “I’m excited to help expand the platform to teach more concepts.”', 'Introducing kids to machine-generated art', 'One of the hottest trends in AI is a new method for creating computer-generated art using generative adversarial networks, or GANs. A pair of neural networks work together to create a photorealistic image while letting the artist add their unique twist. One AI program called\\\\xa0GANpaint, developed in the lab of MIT Quest for Intelligence Director\\\\xa0Antonio Torralba, lets users add trees, clouds, and doors, among other features, to a set of pre-drawn images.', 'In a project with the Quest Bridge, sophomore Maya Nigrin is helping to adapt GANpaint to the popular coding platform for kids,\\\\xa0Scratch. The work involves training a new GAN on pictures of castles and developing custom Scratch extensions to integrate GANpaint with Scratch. The students are also developing Jupyter notebooks to teach others how to think critically about GANs as the technology makes it easier to make and share doctored images.', 'A former babysitter and piano teacher who now tutors middle and high school students in computer science, Nigrin says she picked the project for its emphasis on K-12 education.\\\\xa0Asked for the most important takeaway, she says: “If you can’t solve the problem, go around it.”', 'Learning to problem-solve is a key skill for any software engineer, says Gallagher, who supervised the project. “It can be challenging,” she says, “but that’s part of the fun. The students\\\\xa0will hopefully\\\\xa0come away with a realistic sense of what software development entails.”', 'A robot that lifts you up when you’re feeling blue', 'Anxiety and depression are on the rise as more of our time is spent staring at screens. But if technology is the problem, it might also be the answer, according to\\\\xa0Cynthia Breazeal, an associate professor of media arts and sciences at the\\\\xa0MIT Media Lab.', 'In a new project, Breazeal is rebooting her home robot Jibo as a personal wellness coach. (The MIT spinoff that commercialized Jibo closed last fall, but MIT has a license to use Jibo for applied research). MIT junior Kika Arias spent the last semester helped to design interactions for Jibo to read and respond to people’s moods with personalized bits of advice. If Jibo senses you’re down, for example, it might suggest a “wellness” chat and some positive psychology exercises, like writing down something you feel grateful for.', 'Jibo the wellness coach will face its first test in a pilot study with MIT students this summer. To get it ready, Arias designed and assembled what she calls a “glorified robot chair,” a portable mount for Jibo and its suite of instruments: a camera, microphone, computer, and tablet. She has translated scripts written for Jibo by a human life coach into his playful but laid-back voice. And she has made a widely used scale for self-reported emotions, which study participants will use to rate their mood, more engaging.', '“I’m not a hardcore machine learning, cloud-computing type, but I’ve discovered I’m capable of a lot more than I thought,” she says. “I’ve always felt a strong desire to help people, so when I found this lab, I thought this is exactly where I’m supposed to be.”', 'A storytelling robot that helps kids learn to read', 'Kids who are read-to aloud tend to pick up reading easier, but not all parents themselves know how to read or have time to regularly read stories to their children. What if a home robot could fill in, or even promote higher-quality parent-child reading time?', 'In the first phase of a larger project, researchers in Breazeal’s lab are recording parents as they read aloud to their children, and are analyzing video, audio, and physiological data from the reading sessions. “These interactions play a big\\\\xa0role in a child’s literacy later in life,” says first-year student Shreya Pandit, who worked on the project this semester.\\\\xa0“There’s a sharing of emotion, and exchange of questions and answers during the telling of the story.”', 'These sidebar conversations are critical for learning, says Breazeal. Ideally, the robot is there to strengthen the parent-child bond and provide helpful prompts for both parent and child.', 'To understand how a robot can augment learning,\\\\xa0Pandit has helped to develop parent surveys, run behavioral experiments, analyze data, and integrate multiple data streams. One surprise, she says, has been learning how much work is self-directed: She looks for a problem, researches solutions, and runs them by others in the lab before picking one — for example, an algorithm for splitting audio files based on who’s speaking, or a way of scoring the complexity of the stories being read aloud.', '“I try to set goals for myself and report something back after each session,” she says. “It’s cool to look at this data and try to figure out what it can tell us about improving literacy.”', 'These Quest for Intelligence UROP projects were funded by\\\\xa0Eric Schmidt, technical adviser to Alphabet Inc., and his wife, Wendy.']\", \"['When Armando Solar-Lezama was a third grader in Mexico City, his science class did a unit on electrical circuits. The students were divided into teams of three, and each team member had to bring in a light bulb, a battery, or a switch.', 'Solar-Lezama, whose father worked for an electronics company, volunteered to provide the switch. Using electrical components his father had brought home from work, Solar-Lezama built a “flip-flop” circuit and attached it to a touch-sensitive field effect transistor. When the circuit was off, touching the transistor turned it on, and when it was on, touching the transistor turned it off. “I was pretty proud of my circuit,” says Solar-Lezama, now an MIT professor of electrical engineering and computer science.', 'By the time he got to school, however, one of his soldered connections had come loose, and the circuit’s performance was erratic. “They failed the whole group,” Solar-Lezama says. “And everybody was like, ‘Why couldn’t you just go to the store and get a switch like normal people do?’”', 'The next year, in an introductory computer science class, Solar-Lezama was assigned to write a simple program that would send a few lines of text to a printer. Instead, he wrote a program that asked the user a series of questions, each question predicated on the response to the one before. The answer to the final question determined the text that would be sent to the printer.', 'This time, the program worked perfectly. But “the teacher failed me because that’s not what the assignment was supposed to be,” Solar-Lezama says. “The educational system was not particularly flexible.”', 'At that point, Solar-Lezama abandoned trying to import his extracurricular interests into the classroom. “I sort of brushed it off,” he recalls. “I was doing my own thing. As long as school didn’t take too much of my time, it was fine.”', 'So, in 1997, when Solar-Lezama’s father moved the family to College Station, Texas — the Mexican economy was still in the throes of the three-year-old Mexican peso crisis — the 15-year-old Armando began to teach himself calculus and linear algebra.', 'Accustomed to the autonomy of living in a huge city with a subway he could take anywhere, Solar-Lezama bridled at having to depend on rides from his parents to so much as go to the library. “For the first three years that I was in Texas, I was convinced that as soon as I turned 18, I was going to go back to Mexico,” he says. “Because what was I doing in this place in the middle of nowhere?” He began systematically educating himself in everything he would need to ace the Mexican college entrance exams.', 'At his Texan high school, however, he was placed by default in the lowest of the school’s three academic tracks, which is where most immigrants with imperfect English found themselves. Against the recommendations of the school administrators, he insisted on taking physics; within two weeks, his physics teacher had moved him up to a higher-level class.', 'By his junior year, Solar-Lezama was enrolled in the most demanding math and science classes the school offered, in most of which his classmates were seniors. But in the humanities, where he still struggled with the language — and, he admits, his own lack of interest — he remained on the lower track.', '“In the time I was there, I got to move from one track to the other,” Solar-Lezama says. “It was really shocking to realize how different these tracks were.”', 'Outside the classroom, Solar-Lezama was a member of a team that finished second in the nation in the Department of Energy’s Science Bowl competition. He also won a regional science fair held at Texas A&M with a computer simulation he’d whipped up in an afternoon, when he and some friends realized that they wouldn’t be able to get a scrap-built hovercraft working by the fair deadline. And he started working for a local software startup, doing database coding.', 'But inside the classroom, “my record was very bimodal,” he says. Though he excelled in math and science, he ended his senior year ranked only about 100th in a class of 400.', 'Still, he decided to put his return to Mexico on hold. “By the time I was a senior in high school, I sort of found my place,” he says. “I was learning lots of things that I was interested in, and I decided that, ‘Okay, maybe I’ll stay here for college, and then I’ll go back.’”', 'His spotty academic performance, however, was an obstacle. MIT was one of several universities that denied him undergraduate admission. But the father of one of his Science Bowl teammates taught nuclear engineering at Texas A&M and, recognizing Solar-Lezama’s talent, encouraged him to apply for a generous scholarship offered through the department.', 'To ensure that international students could navigate the transition to a new educational system and, often, a new language, the university restricted the number of units they could carry as freshmen, and Solar-Lezama, his three years of American high school notwithstanding, counted as an international student. So to keep himself busy, he audited several courses outside the nuclear-engineering curriculum.', 'One of these was Introduction to Algorithms. Although he wasn’t formally enrolled at the time, Solar-Lezama did all the homework and took all the exams, and he ended up with the highest grade in the class.', '“Before that point, I thought of programming as a useful skill,” Solar-Lezama says. “One of the things that really excited me about this class was that you could prove things about algorithms and get some guarantees about how something is going to work, and I found that extremely appealing. So I decided to switch majors to computer science.”', 'Graduating in three years, Solar-Lezama decided to postpone his return to Mexico a little longer, applying to graduate programs at MIT, Carnegie Mellon University, and the University of California at Berkeley. “I thought, if I don’t get in to any of them, fine, I’ll go back to Mexico,” he says. Once again, MIT turned him down, as did CMU. But he got into Berkeley.', 'Solar-Lezama arrived at Berkeley planning to continue his work on large parallel computing systems, but his conversations with his advisor, Ras Bodik, quickly took a different turn. Different types of simulations generally required different computational strategies. But implementing those strategies often required reshuffling the same low-level processes. Was it possible, Bodik and Solar-Lezama wondered, to devise a way to formulate the strategies broadly and automate the reshuffling?', 'Solar-Lezama thus found himself part of a small community of researchers working on “program synthesis,” or the automatic generation of computer programs. His \\\\xa0thesis project was a language called Sketch, which lets programmers describe program functionality in general terms and automatically fills in the computational details.', 'Sketch treats program synthesis as a search problem: The task is to search the space of all possible programs for one that can meet the requirements imposed by the general description. The chief innovation behind Sketch was a set of algorithms for rapidly paring down the search space, so that a satisfactory program could be found in real time.', '“There were three or four of us who were pushing this area and telling everybody who would listen that this was the right direction for programming systems research, and for a long time there was a lot of hostility toward these kinds of ideas,” Solar-Lezama says. “Little by little, we started converting a few more people, and all of a sudden they reached a critical mass, and now it’s an extremely active area of research.”', 'After graduating from Berkeley, Solar-Lezama went on the job market, and MIT finally made him an offer. In his seven years at the Institute, where he recently earned tenure, Sketch has remained the foundation of his research, which has developed along three parallel tracks.', 'The first track is the extension of Sketch, so that it can handle more diverse and complex computations. The second is the application of Sketch and its underlying machinery to particular problems — such as orienting new members of large programming teams toward the existing code base, automatically grading programming homework, and parallelizing code for faster execution on multicore chips.', 'Recently, Solar-Lezama’s group has also begun investigating the application of program synthesis to machine learning. Machine learning involves teaching a computer system to perform some classification task by presenting it with training examples. But suppose that the training data consists of a row of three squares and a row of three circles. Which image belongs to the same class, a row of three stars or four circles arranged in a square?', 'Existing machine-learning systems are good at learning to recognize circles from examples of circles, but they’re not as good at the kind of abstract pattern matching that humans do intuitively. A program synthesizer, however, is much more likely to converge on a program for producing three-object rows than one that sometimes produces rows and sometimes produces squares.', 'Having finally made it to city with a good subway system, Solar-Lezama no longer has any plans to move back to Mexico. His wife has a Mexican father and spent much of her childhood in Mexico, but her mother is from Minnesota, and she had planned on settling in the U.S. when she and Solar-Lezama met in Berkeley. Their children, ages 6 and 3, might also find it hard to adjust to life in Mexico. Although they speak Spanish exclusively at home, they speak English at their school in Medford, Massachusetts, and, says Solar-Lezama, “they’re developing a Boston accent.”']\", '[\\'How will advances in computing transform human society?\\', \\'MIT students contemplated this impending question as part of the Envisioning the Future of Computing Prize — an essay contest in which they were challenged to imagine ways that computing technologies could improve our lives, as well as the pitfalls and dangers associated with them.\\', \\'Offered for the first time this year, the Institute-wide competition invited MIT undergraduate and graduate students to share their ideas, aspirations, and vision for what they think a future propelled by advancements in computing holds. Nearly 60 students put pen to paper, including those majoring in mathematics, philosophy, electrical engineering and computer science, brain and cognitive sciences, chemical engineering, urban studies and planning, and management, and entered their submissions.\\', \\'Students dreamed up highly inventive scenarios for how the technologies of today and tomorrow could impact society, for better or worse. Some recurring themes emerged, such as tackling issues in climate change and health care. Others proposed ideas for particular technologies that ranged from digital twins as a tool for navigating the deluge of information online to a cutting-edge platform powered by artificial intelligence, machine learning, and biosensors to create personalized storytelling films that help individuals understand themselves and others.\\', \\'Conceived of by the Social and Ethical Responsibilities of Computing (SERC), a cross-cutting initiative of the MIT Schwarzman College of Computing in collaboration with the School of Humanities, Arts, and Social Sciences (SHASS), the intent of the competition was “to create a space for students to think in a creative, informed, and rigorous way about the societal benefits and costs of the technologies they are or will be developing,” says Caspar Hare, professor of philosophy, co-associate dean of SERC, and the lead organizer of the Envisioning the Future of Computing Prize. “We also wanted to convey that MIT values such thinking.”\\', \\'Prize winners\\', \\'The contest implemented a two-stage evaluation process wherein all essays were reviewed anonymously by a panel of MIT faculty members from the college and SHASS for the initial round. Three qualifiers were then invited to present their entries at an awards ceremony on May 8, followed by a Q&A with a judging panel and live in-person audience for the final round.\\', \"The winning entry was awarded to Robert Cunningham \\'23, a recent graduate in math and physics, for his paper on the implications of a personalized language model that is fine-tuned to predict an individual’s writing based on their past texts and emails. Told from the perspective of three fictional characters: Laura, founder of the tech startup ScribeAI, and Margaret and Vincent, a couple in college who are frequent users of the platform, readers gained insights into the societal shifts that take place and the unforeseen repercussions of the technology.\", \\'Cunningham, who took home the grand prize of $10,000, says he came up with the concept for his essay in late January while thinking about the upcoming release of GPT-4 and how it might be applied. Created by the developers of ChatGPT — an AI chatbot that has managed to capture popular imagination for its capacity to imitate human-like text, images, audio, and code — GPT-4, which was unveiled in March, is the newest version of OpenAI’s language model systems.\\', \"“GPT-4 is wild in reality, but some rumors before it launched were even wilder, and I had a few long\\\\xa0plane rides to\\\\xa0think about them! I enjoyed this opportunity to solidify a vague notion into a piece of writing, and since some of my favorite works of science fiction are short stories, I figured I\\'d take the chance to write one,” Cunningham says.\", \"The other two finalists, awarded $5,000 each, included Gabrielle Kaili-May Liu \\'23, a recent graduate in mathematics with computer science, and brain and cognitive sciences, for her entry on using the reinforcement learning with human feedback technique as a tool for transforming human interactions with AI; and Abigail Thwaites and Eliot Matthew Watkins, graduate students in the Department of Philosophy and Linguistics, for their joint submission on automatic fact checkers, an AI-driven software that they argue could potentially help mitigate the spread of misinformation and be a profound social good.\", \\'“We were so excited to see the amazing response to this contest. It made clear how much students at MIT, contrary to stereotype, really care about the wider implications of technology, says Daniel Jackson, professor of computer science and one of the final-round judges. “So many of the essays were incredibly thoughtful and creative. Robert’s story was a chilling, but entirely plausible take on our AI future; Abigail and Eliot’s analysis brought new clarity to what harms misinformation actually causes; and Gabrielle’s piece gave a lucid overview of a prominent new technology. I hope we’ll be able to run this contest every year, and that it will encourage all our students to broaden their perspectives even further.”\\', \\'Fellow judge Graham Jones, professor of anthropology, adds: “The winning entries reflected the incredible breadth of our students’ engagement with socially responsible computing. They challenge us to think differently about how to design computational technologies, conceptualize social impacts, and imagine future scenarios. Working with a cross-disciplinary panel of judges catalyzed lots of new conversations. As a sci-fi fan, I was thrilled that the top prize went to a such a stunning piece of speculative fiction!”\\', \\'Other judges on the panel for the final round included:\\', \\'Honorable mentions\\', \\'In addition to the grand prize winner and runners up, 12 students were recognized with honorable mentions for their entries, with each receiving $500.\\', \\'The honorees and the title of their essays include:\\', \\'The Envisioning the Future of Computing Prize was supported by MAC3 Impact Philanthropies.\\']', '[\\'With a box of popcorn in one hand, Hal Abelson, a renowned computer scientist, strolled through the first floor of the Ray and Maria Stata Center studying the machine learning exhibits that surrounded him on the afternoon of Feb. 26. Everywhere he looked he saw evidence of the remarkable things MIT students can do when given access to computing resources.\\', \\'“Computing tools and infrastructure have gotten to a place where students can outperform professional researchers. You are constrained mostly by your imagination. It’s just an amazing time,” said Abelson, the Class of 1922 Professor of Computer Science and Engineering.\\', \\'Abelson, and a crowd of hundreds, was witnessing the kickoff of a three-day celebration of the MIT Stephen A. Schwarzman College of Computing. The afternoon event was an exposition of projects that transformed the student street lobby area of the Stata Center into a computing fairground of sorts, replete with courtesy popcorn, bubble tea, lemon squares, brownies, celebratory stickers, and a host of student exhibits that crossed disciplines, broke barriers, and inspired new thinking.\\', \\'For Kadeem Khan, a graduate student in urban studies and planning and an expo participant, the day was special. “I wanted to do a project focused on machine learning and the developing world,” he said. Khan applied machine learning to generate useful insights on poverty in Nairobi by analyzing data from multiple sources, including census, satellite imagery, and data from a geographic information system.\\', \\'“The poverty exhibit is an example of what I was just saying,” said Abelson. “Somehow the resources are here now to allow students to bring things to the next level.” Abeslson and Nicholas Roy, a professor of aeronautics and astronautics, CSAIL researcher, and director of the Bridge in the Quest for Intelligence, helped judge the teams during the monthlong student computing challenges leading up to yesterday.\\', \"Like Khan, MIT electrical engineering and computer science graduate student Natalie Lao embarked on a winning project with the potential to make transformative change in the world. “My background is in AI — but I\\'m also very interested in ethics and fairness and the risks involved when applying AI to the real world,” she said. Her team’s project uses network propagation and analysis to automatically discover and potentially halt the spread of fake news across a variety of media platforms. “We’re talking to the Department of Defense and various companies and trying to see how we can get the solution out in the world,” she said.\", \\'The MIT Schwarzman College of Computing, which represents a $1 billion commitment to addressing the global opportunities and challenges presented by the prevalence of computing and the rise of artificial intelligence, will provide students with unprecedented computing resources, including access to large data sets and the tools to learn from them. Yesterday, top entrants spoke in excited tones about the data sets they accessed during the Machine Learning Across Disciplines Challenge, which, along with the Connect Arts, Community, and Computing Challenge, was funded by the MIT-IBM Watson AI Lab.\\', \\'Graduate students Agni Orfanoudaki and Antonin Dauvin, who are both studying operations research at the MIT Sloan School of Management, applied machine learning and techniques developed at MIT Operations Research Center to patient data from Boston Medical Center spanning two decades. They are developing an analytic approach to understanding the impact of different anti-hypertensive drugs.\\', \\'Senior Sarah Wooders, an undergraduate in math and computer science, has collected a dataset of over 4 million product images and descriptions scraped from online sources. She then trained models to collectively label over 90 important clothing attributes and is is now building a system that can automatically label new clothing products. “It’s really exciting to see all the applications of AI,” said Wooders, also a top entrant. “My project feels like such an obvious idea but this type of system hasn’t been created yet. It seems the same thing is true for a lot of things in AI right now. And so someone like me can come along and do it.”\\']', \"['Ask a smart home device for the weather forecast, and it takes several seconds for the device to respond. One reason this latency occurs is because connected devices don’t have enough memory or power to store and run the enormous machine-learning models needed for the device to understand what a user is asking of it. The model is stored in a data center that may be hundreds of miles away, where the answer is computed and sent to the device.', '', 'MIT researchers have created a new method for computing directly on these devices, which drastically reduces this latency. Their technique shifts the memory-intensive steps of running a machine-learning model to a central server where components of the model are encoded onto light waves.', '', 'The waves are transmitted to a connected device using fiber optics, which enables tons of data to be sent lightning-fast through a network. The receiver then employs a simple optical device that rapidly performs computations using the parts of a model carried by those light waves.', '', 'This technique leads to more than a hundredfold improvement in energy efficiency when compared to other methods. It could also improve security, since a user’s data do not need to be transferred to a central location for computation.', '', 'This method could enable a self-driving car to make decisions in real-time while using just a tiny percentage of the energy currently required by power-hungry computers. It could also allow a user to have a latency-free conversation with their smart home device, be used for live video processing over cellular networks, or even enable high-speed image classification on a spacecraft millions of miles from Earth.', '', '“Every time you want to run a neural network, you have to run the program, and how fast you can run the program depends on how fast you can pipe the program in from memory. Our pipe is massive — it corresponds to sending a full feature-length movie over the internet every millisecond or so. That is how fast data comes into our system. And it can compute as fast as that,” says senior author Dirk Englund, an associate professor in the Department of Electrical Engineering and Computer Science (EECS) and member of the MIT Research Laboratory of Electronics.', '', 'Joining Englund on the paper is lead author and EECS grad student Alexander Sludds; EECS grad student Saumil Bandyopadhyay, Research Scientist Ryan Hamerly, as well as others from MIT, the MIT Lincoln Laboratory, and Nokia Corporation. The research is published today in Science.', '', 'Lightening the load', '', 'Neural networks are machine-learning models that use layers of connected nodes, or neurons, to recognize patterns in datasets and perform tasks, like classifying images or recognizing speech. But these models can contain billions of weight parameters, which are numeric values that transform input data as they are processed. These weights must be stored in memory. At the same time, the data transformation process involves billions of algebraic computations, which require a great deal of power to perform.', '', 'The process of fetching data (the weights of the neural network, in this case) from memory and moving them to the parts of a computer that do the actual computation is one of the biggest limiting factors to speed and energy efficiency, says Sludds.', '', '“So our thought was, why don’t we take all that heavy lifting — the process of fetching billions of weights from memory — move it away from the edge device and put it someplace where we have abundant access to power and memory, which gives us the ability to fetch those weights quickly?” he says.', '', 'The neural network architecture they developed, Netcast, involves storing weights in a central server that is connected to a novel piece of hardware called a smart transceiver. This smart transceiver, a thumb-sized chip that can receive and transmit data, uses technology known as silicon photonics to fetch trillions of weights from memory each second.', '', 'It receives weights as electrical signals and imprints them onto light waves. Since the weight data are encoded as bits (1s and 0s) the transceiver converts them by switching lasers; a laser is turned on for a 1 and off for a 0. It combines these light waves and then periodically transfers them through a fiber optic network so a client device doesn’t need to query the server to receive them.', '', '“Optics is great because there are many ways to carry data within optics. For instance, you can put data on different colors of light, and that enables a much higher data throughput and greater bandwidth than with electronics,” explains Bandyopadhyay.', '', 'Trillions per second', '', 'Once the light waves arrive at the client device, a simple optical component known as a broadband “Mach-Zehnder” modulator uses them to perform super-fast, analog computation. This involves encoding input data from the device, such as sensor information, onto the weights. Then it sends each individual wavelength to a receiver that detects the light and measures the result of the computation.', '', 'The researchers devised a way to use this modulator to do trillions of multiplications per second, which vastly increases the speed of computation on the device while using only a tiny amount of power.', '', '“In order to make something faster, you need to make it more energy efficient. But there is a trade-off. We’ve built a system that can operate with about a milliwatt of power but still do trillions of multiplications per second. In terms of both speed and energy efficiency, that is a gain of orders of magnitude,” Sludds says.', '', 'They tested this architecture by sending weights over an 86-kilometer fiber that connects their lab to MIT Lincoln Laboratory. Netcast enabled machine-learning with high accuracy — 98.7 percent for image classification and 98.8 percent for digit recognition — at rapid speeds.', '', '“We had to do some calibration, but I was surprised by how little work we had to do to achieve such high accuracy out of the box. We were able to get commercially relevant accuracy,” adds Hamerly.', '', 'Moving forward, the researchers want to iterate on the smart transceiver chip to achieve even better performance. They also want to miniaturize the receiver, which is currently the size of a shoe box, down to the size of a single chip so it could fit onto a smart device like a cell phone.', '', '“Using photonics and light as a platform for computing is a really exciting area of research with potentially huge implications on the speed and efficiency of our information technology landscape,” says Euan Allen, a Royal Academy of Engineering Research Fellow at the University of Bath, who was not involved with this work. “The work of Sludds et al. is an exciting step toward seeing real-world implementations of such devices, introducing a new and practical edge-computing scheme whilst also exploring some of the fundamental limitations of computation at very low (single-photon) light levels.”', '', 'The research is funded, in part, by NTT Research, the National Science Foundation, the Air Force Office of Scientific Research, the Air Force Research Laboratory, and the Army Research Office.']\", '[\\'Traditional computer scientists and engineers are trained to develop solutions for specific needs, but aren’t always trained to consider their broader implications. Each new technology generation, and particularly the rise of artificial intelligence, leads to new kinds of systems, new ways of creating tools, and new forms of data, for which norms, rules, and laws frequently have yet to catch up. The kinds of impact that such innovations have in the world has often not been apparent until many years later.\\', \\'As part of the efforts in Social and Ethical Responsibilities of Computing (SERC) within the MIT Stephen A. Schwarzman College of Computing, a new case studies series examines social, ethical, and policy challenges of present-day efforts in computing with the aim of facilitating the development of responsible “habits of mind and action” for those who create and deploy computing technologies.\\', \\'“Advances in computing have undeniably changed much of how we live and work. Understanding and incorporating broader social context is becoming ever more critical,” says Daniel Huttenlocher, dean of the MIT Schwarzman College of Computing. “This case study series is designed to be a basis for discussions in the classroom and beyond, regarding social, ethical, economic, and other implications so that students and researchers can pursue the development of technology across domains in a holistic manner that addresses these important issues.”\\', \\'A modular system\\', \\'By design, the case studies are brief and modular to allow users to mix and match the content to fit a variety of pedagogical needs. Series editors David Kaiser and Julie Shah, who are the associate deans for SERC, structured the cases primarily to be appropriate for undergraduate instruction across a range of classes and fields of study.\\', \\'“Our goal was to provide a seamless way for instructors to integrate cases into an existing course or cluster several cases together to support a broader module within a course. They might also use the cases as a starting point to design new courses that focus squarely on themes of social and ethical responsibilities of computing,” says Kaiser, the Germeshausen Professor of the History of Science and professor of physics.\\', \\'Shah, an associate professor of aeronautics and astronautics and a roboticist who designs systems in which humans and machines operate side by side, expects that the cases will also be of interest to those outside of academia, including computing professionals, policy specialists, and general readers. In curating the series, Shah says that “we interpret ‘social and ethical responsibilities of computing’ broadly to focus on perspectives of people who are affected by various technologies, as well as focus on perspectives of designers and engineers.”\\', \\'The cases are not limited to a particular format and can take shape in various forms — from a magazine-like feature article or Socratic dialogues to choose-your-own-adventure stories or role-playing games grounded in empirical research. Each case study is brief, but includes accompanying notes and references to facilitate more in-depth exploration of a given topic. Multimedia projects will also be considered. “The main goal is to present important material — based on original research — in engaging ways to broad audiences of non-specialists,” says Kaiser.\\', \\'The SERC case studies are specially commissioned and written by scholars who conduct research centrally on the subject of the piece. Kaiser and Shah approached researchers from within MIT as well as from other academic institutions to bring in a mix of diverse voices on a spectrum of topics. Some cases focus on a particular technology or on trends across platforms, while others assess social, historical, philosophical, legal, and cultural facets that are relevant for thinking critically about current efforts in computing and data sciences.\\', \\'The cases published in the inaugural issue place readers in various settings that challenge them to consider the social and ethical implications of computing technologies, such as how social media services and surveillance tools are built; the racial disparities that can arise from deploying facial recognition technology in unregulated, real-world settings; the biases of risk prediction algorithms in the criminal justice system; and the politicization of data collection.\\', \\'\"Most of us agree that we want computing to work for social good, but which good? Whose good? Whose needs and values and worldviews are prioritized and whose are overlooked?” says Catherine D’Ignazio, an assistant professor of urban science and planning and director of the Data + Feminism Lab at MIT.\\', \\'D’Ignazio’s case for the series, co-authored with Lauren Klein, an associate professor in the English and Quantitative Theory and Methods departments at Emory University, introduces readers to the idea that while data are useful, they are not always neutral. “These case studies help us understand the unequal histories that shape our technological systems as well as study their disparate outcomes and effects. They are an exciting step towards holistic, sociotechnical thinking and making.\"\\', \\'Rigorously reviewed\\', \\'Kaiser and Shah formed an editorial board composed of 55 faculty members and senior researchers associated with 19 departments, labs, and centers at MIT, and instituted a rigorous peer-review policy model commonly adopted by specialized journals. Members of the editorial board will also help commission topics for new cases and help identify authors for a given topic.\\', \\'For each submission, the series editors collect four to six peer reviews, with reviewers mostly drawn from the editorial board. For each case, half the reviewers come from fields in computing and data sciences and half from fields in the humanities, arts, and social sciences, to ensure balance of topics and presentation within a given case study and across the series.\\', \\'“Over the past two decades I’ve become a\\\\xa0bit jaded when it comes to the academic review process, and so I was\\\\xa0particularly heartened to see such care and thought put into all of the reviews,\" says Hany Farid, a professor at the University of California at Berkeley with a joint appointment in the Department of Electrical Engineering and Computer Sciences and the School of Information. “The constructive review process made our case study significantly stronger.”\\', \\'Farid’s case, “The Dangers of Risk Prediction in the Criminal Justice System,” which he penned with Julia Dressel, recently a student of computer science at Dartmouth College, is one of the four commissioned pieces featured in the inaugural issue.\\', \\'Cases are additionally reviewed by undergraduate volunteers, who help the series editors gauge each submission for balance, accessibility for students in multiple fields of study, and possibilities for adoption in specific courses. The students also work with them to create original homework problems and active learning projects to accompany each case study, to further facilitate adoption of the original materials across a range of existing undergraduate subjects.\\', \"“I volunteered to work with this group because I believe that it\\'s incredibly important for those working in computer science to include thinking about ethics not as an afterthought, but integrated into every step and decision that is made, says Annie Snyder, a mathematical economics sophomore and a member of the MIT Schwarzman College of Computing’s Undergraduate Advisory Group. “While this is a massive issue to take on, this project is an amazing opportunity to start building an ethical culture amongst the incredibly talented students at MIT who will hopefully carry it forward into their own projects and workplace.”\", \\'New sets of case studies, produced with support from\\\\xa0the MIT Press’ Open Publishing Services program, will be published twice a year via the Knowledge Futures Group’s\\\\xa0PubPub\\\\xa0platform. The SERC case studies are made available for free on an open-access basis, under Creative Commons licensing terms. Authors retain copyright, enabling them to reuse and republish their work in more specialized scholarly publications.\\', \\'“It was important to us to approach this project in an inclusive way and lower the barrier for people to be able to access this content. These are complex issues that we need to deal with, and we hope that by making the cases widely available, more people will engage in social and ethical considerations as they’re studying and developing computing technologies,” says Shah.\\']', '[\\'“The Laughing Room,”\\\\xa0an interactive art installation by author, illustrator, and MIT graduate student Jonathan \"Jonny\" Sun, looks like a typical living room: couches, armchairs, coffee table, soft lighting. This cozy scene, however, sits in a glass-enclosed space, flanked by bright lights and a microphone, with a bank of laptops and a video camera positioned across the room. People wander in, take a seat, begin chatting. After a pause in the conversation, a riot of canned laughter rings out, prompting genuine giggles from the group.\\', \\'Presented at the Cambridge Public Library in Cambridge, Massachusetts, Nov. 16-18, \"The Laughing Room\" was an artificially intelligent room programmed to play an audio laugh track whenever participants said something that its algorithm deemed funny. Sun, who is currently on leave from his PhD program within the MIT Department of Urban Studies and Planning, is an affiliate at the Berkman Klein Center for Internet and Society at Harvard University, and creative researcher at the metaLAB at Harvard, created the project to explore the increasingly social and cultural roles of technology in public and private spaces, users’ agency within and dependence on such technology, and the issues of privacy raised by these systems. The installations were presented as part of ARTificial Intelligence, an ongoing program led by MIT associate professor of literature Stephanie Frampton that fosters public dialogue about the emerging ethical and social implications of artificial intelligence (AI) through art and design.\\']']], 'uris': None, 'data': None, 'metadatas': [[{'Article Body': \"['When humans look at a scene, they see objects and the relationships between them. On top of your desk, there might be a laptop that is sitting to the left of a phone, which is in front of a computer monitor.', '', 'Many deep learning models struggle to see the world this way because they don’t understand the entangled relationships between individual objects. Without knowledge of these relationships, a robot designed to help someone in a kitchen would have difficulty following a command like “pick up the spatula that is to the left of the stove and place it on top of the cutting board.”', '', 'In an effort to solve this problem, MIT researchers have developed a model that understands the underlying relationships between objects in a scene. Their model represents individual relationships one at a time, then combines these representations to describe the overall scene. This enables the model to generate more accurate images from text descriptions, even when the scene includes several objects that are arranged in different relationships with one another.', '', 'This work could be applied in situations where industrial robots must perform intricate, multistep manipulation tasks, like stacking items in a warehouse or assembling appliances. It also moves the field one step closer to enabling machines that can learn from and interact with their environments more like humans do.', '', '“When I look at a table, I can’t say that there is an object at XYZ location. Our minds don’t work like that. In our minds, when we understand a scene, we really understand it based on the relationships between the objects. We think that by building a system that can understand the relationships between objects, we could use that system to more effectively manipulate and change our environments,” says Yilun Du, a PhD student in the Computer Science and Artificial Intelligence Laboratory (CSAIL) and co-lead author of the paper.', '', 'Du wrote the paper with co-lead authors Shuang Li, a CSAIL PhD student, and Nan Liu, a graduate student at the University of Illinois at Urbana-Champaign; as well as Joshua B. Tenenbaum, a professor of computational cognitive science in the Department of Brain and Cognitive Sciences and a member of CSAIL; and senior author Antonio Torralba, the Delta Electronics Professor of Electrical Engineering and Computer Science and a member of CSAIL. The research will be presented at the Conference on Neural Information Processing Systems in December.', '', 'One relationship at a time', '', 'The framework the researchers developed can generate an image of a scene based on a text description of objects and their relationships, like “A wood table to the left of a blue stool. A red couch to the right of a blue stool.”', '', 'Their system would break these sentences down into two smaller pieces that describe each individual relationship (“a wood table to the left of a blue stool” and “a red couch to the right of a blue stool”), and then model each part separately. Those pieces are then combined through an optimization process that generates an image of the scene.', '', 'The researchers used a machine-learning technique called energy-based models to represent the individual object relationships in a scene description. This technique enables them to use one energy-based model to encode each relational description, and then compose them together in a way that infers all objects and relationships.', '', 'By breaking the sentences down into shorter pieces for each relationship, the system can recombine them in a variety of ways, so it is better able to adapt to scene descriptions it hasn’t seen before, Li explains.', '', '“Other systems would take all the relations holistically and generate the image one-shot from the description. However, such approaches fail when we have out-of-distribution descriptions, such as descriptions with more relations, since these model can’t really adapt one shot to generate images containing more relationships. However, as we are composing these separate, smaller models together, we can model a larger number of relationships and adapt to novel combinations,” Du says.', '', 'The system also works in reverse — given an image, it can find text descriptions that match the relationships between objects in the scene. In addition, their model can be used to edit an image by rearranging the objects in the scene so they match a new description.', '', 'Understanding complex scenes', '', 'The researchers compared their model to other deep learning methods that were given text descriptions and tasked with generating images that displayed the corresponding objects and their relationships. In each instance, their model outperformed the baselines.', '', 'They also asked humans to evaluate whether the generated images matched the original scene description. In the most complex examples, where descriptions contained three relationships, 91 percent of participants concluded that the new model performed better.', '', '“One interesting thing we found is that for our model, we can increase our sentence from having one relation description to having two, or three, or even four descriptions, and our approach continues to be able to generate images that are correctly described by those descriptions, while other methods fail,” Du says.', '', 'The researchers also showed the model images of scenes it hadn’t seen before, as well as several different text descriptions of each image, and it was able to successfully identify the description that best matched the object relationships in the image.', '', 'And when the researchers gave the system two relational scene descriptions that described the same image but in different ways, the model was able to understand that the descriptions were equivalent.', '', 'The researchers were impressed by the robustness of their model, especially when working with descriptions it hadn’t encountered before.', '', '“This is very promising because that is closer to how humans work. Humans may only see several examples, but we can extract useful information from just those few examples and combine them together to create infinite combinations. And our model has such a property that allows it to learn from fewer data but generalize to more complex scenes or image generations,” Li says.', '', 'While these early results are encouraging, the researchers would like to see how their model performs on real-world images that are more complex, with noisy backgrounds and objects that are blocking one another.', '', 'They are also interested in eventually incorporating their model into robotics systems, enabling a robot to infer object relationships from videos and then apply this knowledge to manipulate objects in the world.', '', '“Developing visual representations that can deal with the compositional nature of the world around us is one of the key open problems in computer vision. This paper makes significant progress on this problem by proposing an energy-based model that explicitly models multiple relations among the objects depicted in the image. The results are really impressive,” says Josef Sivic, a distinguished researcher at the Czech Institute of Informatics, Robotics, and Cybernetics at Czech Technical University, who was not involved with this research.', '', 'This research is supported, in part, by Raytheon BBN Technologies Corp., Mitsubishi Electric Research Laboratory, the National Science Foundation, the Office of Naval Research, and the IBM Thomas J. Watson Research Center.']\"}, {'Article Body': \"['Computers have become so pervasive in today’s world that preparing students to work and assume leadership roles in this shifting landscape requires giving them a better understanding of how computers work, how to use them, and how they affect every aspect of society. That’s the reasoning behind the creation of the new MIT Stephen A. Schwarzman College of Computing, and it was the theme of many of the presentations and panel discussions in this week’s three-day celebration of the new college.', '“We’re in the midst of a global transformation that’s catalyzed by the rapid acceleration of digital technologies, including unprecedented access to computation and data,” said Farnam Jahanian, president of Carnegie Mellon University, in a keynote address on Wednesday. “The scale and scope and pace of these advances are truly unprecedented in human history.”', '“The impact of these technologies is ubiquitous,” he said, “with a wide range of applications from health care to transportation, finance, energy, manufacturing, and far beyond. … The pace of innovation is accelerating dramatically.”', 'These changes require a profound rethinking of the role of education in this rapidly changing environment, Jahanian said. “Imagine a day when by integrating emerging technologies, such as AI-enabled learning techniques and inverted classrooms, we can achieve personalized, outcome-based education,” he said.', 'MIT Provost Martin Schmidt, in a discussion with reporters, said that in creating the MIT Schwarzman College of Computing, “one of the things that’s really critical to us is that not only should this advance computation, but it should really link to all the disciplines across the campus.” The college will “strengthen those disciplines in their use of these new tools,” he said, “but also when we learn things about how we apply those tools to the disciplines, that knowledge flows back … and informs the next generation” of computing research.', 'Schmidt added that in planning the new college, a key question was how MIT will deliver on its promise of making sure that the college “has in its DNA” an awareness of the societal impact of current and future advances in computing. This appreciation “should inform our educational agenda, what our undergraduates and graduates learn in the classroom, and it should inform our research agenda,” he said. “It should shape how the research is performed, and the kind of content we produce that informs policies and informs governments on how they should respond to the deployment of these technologies.”', 'The new college was founded partly in response to the fact that “there really was a transformation occurring across the campus,” with computation increasingly forming a key part of the work in amost all disciplines, Schmidt said. While about 40 percent of MIT students major in computer science, there was a clear need for an even greater integration of computation and data science early and deeply into every aspect of education.', 'Melissa Nobles, dean of MIT’s School of Humanities, Arts and Social Sciences, who also participated in the discussion, told reporters that students in that school were very excited to take part in this increased integration of their disciplines with computation. She cited examples of classes where mixed groups of computer science students and those majoring in arts, economics, or literature worked on problems that combined their different kinds of expertise. In one class, for example, the students studied in exhaustive detail the way writers of 19th century novels used male and female pronouns and how that related to the genders of the author and the main characters. The project required both computer expertise to analyze thousands of texts, and a knowledge of the literature in order to provide context for their findings.', 'Also during the discussion, Maria Klawe, president of Harvey Mudd College in California and another keynote speaker, pointed out that a deep understanding of computers and their impact is increasingly needed in a rapidly changing world where it is estimated that many of the jobs people perform today “are just going to disappear” within the next few decades. That makes interdisciplinary education more important than ever, she said.', 'Regarding the creation of the new college, she said, “I see this as an incredibly important step for MIT, and I think it’s going to influence other institutions to do similar things.”', 'The goals of the college reach far beyond just helping people in other disciplines to use computers more effectively, Nobles and others emphasized. It’s also important, they said, to make sure that the skills and knowledge from other fields flow back into computer science, influencing the ethical, political, and social implications of the work in that field — not just as an afterthought but as a fundamental part of thinking and planning.', 'For example, while it is tempting to make use of massive sets of data collected by social media, the use of such datasets can raise serious concerns about privacy and informed consent. Such issues may be relatively new territory for computer scientists, but they are longstanding issues that have been dealt with extensively by social scientists and philosophers whose expertise can help inform the data collection and analysis procedures.', 'The speakers at Wednesday’s symposium, representing many different fields and institutions, shared a sense of excitement about the potential for the MIT Schwarzman College of Computing to bring about significant innovations. “MIT continues to be a world-class institution that offers a distinctive education and research, of course,” Jahanian said in his keynote, “and this latest development will certainly increase its impact in this changing world.”']\"}, {'Article Body': '[\\'“Who is Bram Stoker?” Those three words demonstrated the amazing potential of artificial intelligence. It was the answer to a final question in a particularly memorable 2011 episode of\\\\xa0Jeopardy!. The three competitors were former champions Brad Rutter and Ken Jennings, and Watson, a super computer developed by IBM. By answering the final question correctly, Watson became the first computer to beat a human on the famous quiz show.\\', \\'“In a way, Watson winning\\\\xa0Jeopardy!\\\\xa0seemed unfair to people,” says Jeehwan Kim, the Class ‘47 Career Development Professor and a faculty member of the MIT departments of Mechanical Engineering and Materials Science and Engineering. “At the time, Watson was connected to a super computer the size of a room while the human brain is just a few pounds. But the ability to replicate a human brain’s ability to learn is incredibly difficult.”\\', \\'Kim specializes in machine learning, which relies on algorithms to teach computers how to learn like a human brain. “Machine learning is cognitive computing,” he explains. “Your computer recognizes things without you telling the computer what it’s looking at.”\\', \\'Machine learning is one example of artificial intelligence in practice. While the phrase “machine learning” often conjures up science fiction typified in shows like \"Westworld\" or \"Battlestar Galactica,\" smart systems and devices are already pervasive in the fabric of our daily lives. Computers and phones use face recognition to unlock. Systems sense and adjust the temperature in our homes. Devices answer questions or play our favorite music on demand. Nearly every major car company has entered the race to develop a safe self-driving car.\\', \\'For any of these products to work, the software and hardware both have to work in perfect synchrony. Cameras, tactile sensors, radar, and light detection all need to function properly to feed information back to computers. Algorithms need to be designed so these machines can process these sensory data and make decisions based on the highest probability of success.\\', \\'Kim and the much of the faculty at MIT’s Department of Mechanical Engineering are creating new software that connects with hardware to create intelligent devices. Rather than building the sentient robots romanticized in popular culture, these researchers are working on projects that improve everyday life and make humans safer, more efficient, and better informed.\\', \\'Making portable devices smarter\\', \\'Jeehwan Kim holds up sheet of paper. If he and his team are successful, one day the power of a super computer like IBM’s Watson will be shrunk down to the size of one sheet of paper. “We are trying to build an actual physical neural network on a letter paper size,” explains Kim.\\', \\'To date, most neural networks have been software-based and made using the conventional method known as the Von Neumann computing method. Kim however has been using neuromorphic computing methods.\\', \\'“Neuromorphic computer means portable AI,” says Kim. “So, you build artificial neurons and synapses on a small-scale wafer.” The result is a so-called ‘brain-on-a-chip.’\\\\nRather than compute information from binary signaling, Kim’s neural network processes information like an analog device. Signals act like artificial neurons and move across thousands of arrays to particular cross points, which function like synapses. With thousands of arrays connected, vast amounts of information could be processed at once. For the first time, a portable piece of equipment could mimic the processing power of the brain.\\', \\'“The key with this method is you really need to control the artificial synapses well. When you’re talking about thousands of cross points, this poses challenges,” says Kim.\\', \\'According to Kim, the design and materials that have been used to make these artificial synapses thus far have been less than ideal. The amorphous materials used in neuromorphic chips make it incredibly difficult to control the ions once voltage is applied.\\', \\'In a\\\\xa0Nature Materials\\\\xa0study published earlier this year, Kim found that when his team made a chip out of silicon germanium they were able to control the current flowing out of the synapse and reduce variability to 1 percent. With control over how the synapses react to stimuli, it was time to put their chip to the test.\\', \\'“We envision that if we build up the actual neural network with material we can actually do handwriting recognition,” says Kim. In a computer simulation of their new artificial neural network design, they provided thousands of handwriting samples. Their neural network was able to accurately recognize 95 percent of the samples.\\', \\'“If you have a camera and an algorithm for the handwriting data set connected to our neural network, you can achieve handwriting recognition,” explains Kim.\\', \\'While building the physical neural network for handwriting recognition is the next step for Kim’s team, the potential of this new technology goes beyond handwriting recognition. “Shrinking the power of a super computer down to a portable size could revolutionize the products we use,” says Kim. “The potential is limitless – we can integrate this technology in our phones, computers, and robots to make them substantially smarter.”\\', \\'Making homes smarter\\', \\'While Kim is working on making our portable products more intelligent, Professor Sanjay Sarma and Research Scientist Josh Siegel hope to integrate smart devices within the biggest product we own: our homes.\\', \\'One evening, Sarma was in his home when one of his circuit breakers kept going off. This circuit breaker — known as an arc-fault circuit interrupter (AFCI) — was designed to shut off power when an electric arc is detected to prevent fires. While AFCIs are great at preventing fires, in Sarma’s case there didn’t seem to be an issue. “There was no discernible reason for it to keep going off,” recalls Sarma. “It was incredibly distracting.”\\', \"AFCIs are notorious for such ‘nuisance trips,’ which disconnect safe objects unnecessarily. Sarma, who also serves as MIT\\'s vice president for open learning, turned his frustration into opportunity. If he could embed the AFCI with smart technologies and connect it to the ‘internet of things,’ he could teach the circuit breaker to learn when a product is safe or when a product actually poses a fire risk.\\\\n“Think of it like a virus scanner,” explains Siegel. “Virus scanners are connected to a system that updates them with new virus definitions over time.” If Sarma and Siegel could embed similar technology into AFCIs, the circuit breakers could detect exactly what product is being plugged in and learn new object definitions over time.\", \\'If, for example, a new vacuum cleaner is plugged into the circuit breaker and the power shuts off without reason, the smart AFCI can learn that it’s safe and add it to a list of known safe objects. The AFCI learns these definitions with the aid of a neural network. But, unlike Jeewhan Kim’s physical neural network, this network is software-based.\\', \\'The neural network is built by gathering thousands of data points during simulations of arcing. Algorithms are then written to help the network assess its environment, recognize patterns, and make decisions based on the probability of achieving the desired outcome. With the help of a $35 microcomputer and a sound card, the team can cheaply integrate this technology into circuit breakers.\\', \"As the smart AFCI learns about the devices it encounters, it can simultaneously distribute its knowledge and definitions to every other home using the internet of things.\\\\n“Internet of things could just as well be called \\'intelligence of things,” says Sarma. “Smart, local technologies with the aid of the cloud can make our environments adaptive and the user experience seamless.”\", \\'Circuit breakers are just one of many ways neural networks can be used to make homes smarter. This kind of technology can control the temperature of your house, detect when there’s an anomaly such as an intrusion or burst pipe, and run diagnostics to see when things are in need of repair.\\', \\'“We’re developing software for monitoring mechanical systems that’s self-learned,” explains Siegel. “You don’t teach these devices all the rules, you teach them how to learn the rules.”\\', \\'Making manufacturing and design smarter\\\\nArtificial intelligence can not only help improve how users interact with products, devices, and environments. It can also improve the efficiency with which objects are made by optimizing the manufacturing and design process.\\', \\'“Growth in automation along with complementary technologies including 3-D printing, AI, and machine learning compels us to, in the long run, rethink how we design factories and supply chains,” says Associate Professor A. John Hart.\\', \\'Hart, who has done extensive research in 3-D printing, sees AI as a way to improve quality assurance in manufacturing. 3-D printers incorporating high-performance sensors, that are capable of analyzing data on the fly, will help accelerate the adoption of 3-D printing for mass production.\\', \\'“Having 3-D printers that learn how to create parts with fewer defects and inspect parts as they make them will be a really big deal — especially when the products you’re making have critical properties such as medical devices or parts for aircraft engines,” Hart explains.\\', \\'The very process of designing the structure of these parts can also benefit from intelligent software. Associate Professor Maria Yang has been looking at how designers can use automation tools to design more efficiently. “We call it hybrid intelligence for design,” says Yang. “The goal is to enable effective collaboration between intelligent tools and human designers.”\\', \\'In a recent study, Yang and graduate student Edward Burnell tested a design tool with varying levels of automation. Participants used the software to pick nodes for a 2-D truss of either a stop sign or a bridge. The tool would then automatically come up with optimized solutions based on intelligent algorithms for where to connect nodes and the width of each part.\\', \\'“We’re trying to design smart algorithms that fit with the ways designers already think,” says Burnell.\\', \\'Making robots smarter\\', \\'If there is anything on MIT’s campus that most closely resembles the futuristic robots of science fiction, it would be Professor Sangbae Kim’s robotic cheetah. The four-legged creature senses its surrounding environment using LIDAR technologies and moves in response to this information. Much like its namesake, it can run and leap over obstacles.\\', \\'Kim’s primary focus is on navigation. “We are building a very unique system specially designed for dynamic movement of the robot,” explains Kim. “I believe it is going to reshape the interactive robots in the world. You can think of all kinds of applications — medical, health care, factories.”\\', \\'Kim sees opportunity to eventually connect his research with the physical neural network his colleague Jeewhan Kim is working on. “If you want the cheetah to recognize people, voice, or gestures, you need a lot of learning and processing,” he says. “Jeewhan’s neural network hardware could possibly enable that someday.”\\', \\'Combining the power of a portable neural network with a robot capable of skillfully navigating its surroundings could open up a new world of possibilities for human and AI interaction. This is just one example of how researchers in mechanical engineering can one-day collaborate to bring AI research to next level.\\', \\'While we may be decades away from interacting with intelligent robots, artificial intelligence and machine learning has already found its way into our routines. Whether it’s using face and handwriting recognition to protect our information, tapping into the internet of things to keep our homes safe, or helping engineers build and design more efficiently, the benefits of AI technologies are pervasive.\\', \\'The science fiction fantasy of a world overtaken by robots is far from the truth. “There’s this romantic notion that everything is going to be automatic,” adds Maria Yang. “But I think the reality is you’re going to have tools that will work with people and help make their daily life a bit easier.”\\']'}, {'Article Body': \"['To demystify artificial intelligence (AI) and unlock its benefits, the MIT Quest for Intelligence created the Quest Bridge to bring new intelligence tools and ideas into classrooms, labs, and homes. This spring, more than a dozen\\\\xa0Undergraduate Research Opportunities Program\\\\xa0(UROP) students joined the project in its mission to make AI accessible to all. Undergraduates worked on applications designed to teach kids about AI, improve access to AI programs and infrastructure, and harness AI to improve literacy and mental health. Six projects are highlighted here.', 'Project Athena for cloud computing', 'Training an AI model often requires remote servers to handle the heavy number-crunching, but getting projects to the cloud and back is no trivial matter. To simplify the process, an undergraduate club called the\\\\xa0MIT Machine Intelligence Community\\\\xa0(MIC) is building an interface modeled after MIT’s\\\\xa0Project Athena, which brought desktop computing to campus in the 1980s.', 'Amanda Li stumbled on the MIC during orientation last fall. She was looking for computer power to train an AI language model she had built to identify the nationality of non-native English speakers. The club had a bank of cloud credits, she learned, but no practical system for giving them away. A plan to build such a system, tentatively named “Monkey,” quickly took shape.', 'The system would have to send a student’s training data and AI model to the cloud, put the project in a queue, train the model, and send the finished project back to MIT. It would also have to track individual usage to make sure cloud credits were evenly distributed.', 'This spring, Monkey became a UROP project, and Li and sophomore Sebastian Rodriguez continued to work on it under the guidance of the Quest Bridge. So far, the students have created four modules in GitHub that will eventually become the foundation for a distributed system.', '“The coding isn’t the difficult part,” says Li. “It’s the exploring the server side of machine learning — Docker, Google Cloud, and the API. The most important thing I’ve learned is how to efficiently design and pipeline a project as big as this.”', 'A launch is expected\\\\xa0sometime next year.\\\\xa0“This is a huge project, with some timely problems that industry is also trying to address,” says Quest Bridge AI engineer Steven Shriver, who is supervising the project. “I have no doubt the students will figure it out: I’m here to help when they need it.”', 'An easy-to-use AI program for segmenting images', 'The ability to divide an image into its component parts underlies more complicated AI tasks like picking out proteins in pictures of microscopic cells, or stress fractures in shattered materials. Although fundamental, image segmentation programs are still hard for non-engineers to navigate. In a project with the Quest Bridge, first-year Marco Fleming helped to build a Jupyter notebook for image segmentation, part of the Quest Bridge’s broader mission to develop a set of AI building blocks that researchers can tailor for specific applications.', 'Fleming came to the project with self-taught coding skills, but no experience with machine learning, GitHub, or using a command-line interface. Working with Katherine Gallagher, an AI engineer with the Quest Bridge, and a more experienced classmate, Sule Kahraman, Fleming became fluent in convolutional neural networks, the workhorse for many machine vision tasks. “It’s kind of weird,” he explains. “You take a picture and do a lot of math to it, and the machine learns where the edges are.” Bound for a summer internship at Allstate this summer, Fleming says the project gave him a confidence boost.', 'His participation also benefitted the Quest Bridge, says Gallagher. “We’re developing these notebooks for people like Marco, a freshman with no machine learning experience. Seeing where Marco got tripped up was really valuable.”', 'An automated image classifier: no coding required', 'Anyone can build apps that impact the world. That’s the motto of the\\\\xa0MIT AppInventor, a programming environment founded by\\\\xa0Hal Abelson, the Class of 1922 Professor in MIT’s\\\\xa0Department of Electrical Engineering and Computer Science. Working in Abelson’s lab over Independent Activity Period, sophomore Yuria Utsumi developed a web interface that lets anyone build a deep learning classifier to sort pictures of, say, happy faces and sad faces, or apples and oranges.', 'In four steps, the\\\\xa0Image Classification Explorer\\\\xa0lets users label and upload their images to the web, select a customizable model, add testing data, and see the results. Utsumi built the app with a pre-trained classifier that she restructured to learn from a set of new and unfamiliar images. Once users retrain the classifier on the new images, they can upload the model to AppInventor to view it on their smartphones.', 'In a recent test run of the Explorer app, students at Boston Latin Academy uploaded selfies shot on their laptop webcams and classified their facial expressions. For Utsumi, who picked the project hoping to gain practical web development and programming skills, it was a moment of triumph. “This is the first time I’m solving an algorithms problem in real life!” she says.\\\\xa0“It was fun to see the students become more comfortable with machine learning,” she adds. “I’m excited to help expand the platform to teach more concepts.”', 'Introducing kids to machine-generated art', 'One of the hottest trends in AI is a new method for creating computer-generated art using generative adversarial networks, or GANs. A pair of neural networks work together to create a photorealistic image while letting the artist add their unique twist. One AI program called\\\\xa0GANpaint, developed in the lab of MIT Quest for Intelligence Director\\\\xa0Antonio Torralba, lets users add trees, clouds, and doors, among other features, to a set of pre-drawn images.', 'In a project with the Quest Bridge, sophomore Maya Nigrin is helping to adapt GANpaint to the popular coding platform for kids,\\\\xa0Scratch. The work involves training a new GAN on pictures of castles and developing custom Scratch extensions to integrate GANpaint with Scratch. The students are also developing Jupyter notebooks to teach others how to think critically about GANs as the technology makes it easier to make and share doctored images.', 'A former babysitter and piano teacher who now tutors middle and high school students in computer science, Nigrin says she picked the project for its emphasis on K-12 education.\\\\xa0Asked for the most important takeaway, she says: “If you can’t solve the problem, go around it.”', 'Learning to problem-solve is a key skill for any software engineer, says Gallagher, who supervised the project. “It can be challenging,” she says, “but that’s part of the fun. The students\\\\xa0will hopefully\\\\xa0come away with a realistic sense of what software development entails.”', 'A robot that lifts you up when you’re feeling blue', 'Anxiety and depression are on the rise as more of our time is spent staring at screens. But if technology is the problem, it might also be the answer, according to\\\\xa0Cynthia Breazeal, an associate professor of media arts and sciences at the\\\\xa0MIT Media Lab.', 'In a new project, Breazeal is rebooting her home robot Jibo as a personal wellness coach. (The MIT spinoff that commercialized Jibo closed last fall, but MIT has a license to use Jibo for applied research). MIT junior Kika Arias spent the last semester helped to design interactions for Jibo to read and respond to people’s moods with personalized bits of advice. If Jibo senses you’re down, for example, it might suggest a “wellness” chat and some positive psychology exercises, like writing down something you feel grateful for.', 'Jibo the wellness coach will face its first test in a pilot study with MIT students this summer. To get it ready, Arias designed and assembled what she calls a “glorified robot chair,” a portable mount for Jibo and its suite of instruments: a camera, microphone, computer, and tablet. She has translated scripts written for Jibo by a human life coach into his playful but laid-back voice. And she has made a widely used scale for self-reported emotions, which study participants will use to rate their mood, more engaging.', '“I’m not a hardcore machine learning, cloud-computing type, but I’ve discovered I’m capable of a lot more than I thought,” she says. “I’ve always felt a strong desire to help people, so when I found this lab, I thought this is exactly where I’m supposed to be.”', 'A storytelling robot that helps kids learn to read', 'Kids who are read-to aloud tend to pick up reading easier, but not all parents themselves know how to read or have time to regularly read stories to their children. What if a home robot could fill in, or even promote higher-quality parent-child reading time?', 'In the first phase of a larger project, researchers in Breazeal’s lab are recording parents as they read aloud to their children, and are analyzing video, audio, and physiological data from the reading sessions. “These interactions play a big\\\\xa0role in a child’s literacy later in life,” says first-year student Shreya Pandit, who worked on the project this semester.\\\\xa0“There’s a sharing of emotion, and exchange of questions and answers during the telling of the story.”', 'These sidebar conversations are critical for learning, says Breazeal. Ideally, the robot is there to strengthen the parent-child bond and provide helpful prompts for both parent and child.', 'To understand how a robot can augment learning,\\\\xa0Pandit has helped to develop parent surveys, run behavioral experiments, analyze data, and integrate multiple data streams. One surprise, she says, has been learning how much work is self-directed: She looks for a problem, researches solutions, and runs them by others in the lab before picking one — for example, an algorithm for splitting audio files based on who’s speaking, or a way of scoring the complexity of the stories being read aloud.', '“I try to set goals for myself and report something back after each session,” she says. “It’s cool to look at this data and try to figure out what it can tell us about improving literacy.”', 'These Quest for Intelligence UROP projects were funded by\\\\xa0Eric Schmidt, technical adviser to Alphabet Inc., and his wife, Wendy.']\"}, {'Article Body': \"['When Armando Solar-Lezama was a third grader in Mexico City, his science class did a unit on electrical circuits. The students were divided into teams of three, and each team member had to bring in a light bulb, a battery, or a switch.', 'Solar-Lezama, whose father worked for an electronics company, volunteered to provide the switch. Using electrical components his father had brought home from work, Solar-Lezama built a “flip-flop” circuit and attached it to a touch-sensitive field effect transistor. When the circuit was off, touching the transistor turned it on, and when it was on, touching the transistor turned it off. “I was pretty proud of my circuit,” says Solar-Lezama, now an MIT professor of electrical engineering and computer science.', 'By the time he got to school, however, one of his soldered connections had come loose, and the circuit’s performance was erratic. “They failed the whole group,” Solar-Lezama says. “And everybody was like, ‘Why couldn’t you just go to the store and get a switch like normal people do?’”', 'The next year, in an introductory computer science class, Solar-Lezama was assigned to write a simple program that would send a few lines of text to a printer. Instead, he wrote a program that asked the user a series of questions, each question predicated on the response to the one before. The answer to the final question determined the text that would be sent to the printer.', 'This time, the program worked perfectly. But “the teacher failed me because that’s not what the assignment was supposed to be,” Solar-Lezama says. “The educational system was not particularly flexible.”', 'At that point, Solar-Lezama abandoned trying to import his extracurricular interests into the classroom. “I sort of brushed it off,” he recalls. “I was doing my own thing. As long as school didn’t take too much of my time, it was fine.”', 'So, in 1997, when Solar-Lezama’s father moved the family to College Station, Texas — the Mexican economy was still in the throes of the three-year-old Mexican peso crisis — the 15-year-old Armando began to teach himself calculus and linear algebra.', 'Accustomed to the autonomy of living in a huge city with a subway he could take anywhere, Solar-Lezama bridled at having to depend on rides from his parents to so much as go to the library. “For the first three years that I was in Texas, I was convinced that as soon as I turned 18, I was going to go back to Mexico,” he says. “Because what was I doing in this place in the middle of nowhere?” He began systematically educating himself in everything he would need to ace the Mexican college entrance exams.', 'At his Texan high school, however, he was placed by default in the lowest of the school’s three academic tracks, which is where most immigrants with imperfect English found themselves. Against the recommendations of the school administrators, he insisted on taking physics; within two weeks, his physics teacher had moved him up to a higher-level class.', 'By his junior year, Solar-Lezama was enrolled in the most demanding math and science classes the school offered, in most of which his classmates were seniors. But in the humanities, where he still struggled with the language — and, he admits, his own lack of interest — he remained on the lower track.', '“In the time I was there, I got to move from one track to the other,” Solar-Lezama says. “It was really shocking to realize how different these tracks were.”', 'Outside the classroom, Solar-Lezama was a member of a team that finished second in the nation in the Department of Energy’s Science Bowl competition. He also won a regional science fair held at Texas A&M with a computer simulation he’d whipped up in an afternoon, when he and some friends realized that they wouldn’t be able to get a scrap-built hovercraft working by the fair deadline. And he started working for a local software startup, doing database coding.', 'But inside the classroom, “my record was very bimodal,” he says. Though he excelled in math and science, he ended his senior year ranked only about 100th in a class of 400.', 'Still, he decided to put his return to Mexico on hold. “By the time I was a senior in high school, I sort of found my place,” he says. “I was learning lots of things that I was interested in, and I decided that, ‘Okay, maybe I’ll stay here for college, and then I’ll go back.’”', 'His spotty academic performance, however, was an obstacle. MIT was one of several universities that denied him undergraduate admission. But the father of one of his Science Bowl teammates taught nuclear engineering at Texas A&M and, recognizing Solar-Lezama’s talent, encouraged him to apply for a generous scholarship offered through the department.', 'To ensure that international students could navigate the transition to a new educational system and, often, a new language, the university restricted the number of units they could carry as freshmen, and Solar-Lezama, his three years of American high school notwithstanding, counted as an international student. So to keep himself busy, he audited several courses outside the nuclear-engineering curriculum.', 'One of these was Introduction to Algorithms. Although he wasn’t formally enrolled at the time, Solar-Lezama did all the homework and took all the exams, and he ended up with the highest grade in the class.', '“Before that point, I thought of programming as a useful skill,” Solar-Lezama says. “One of the things that really excited me about this class was that you could prove things about algorithms and get some guarantees about how something is going to work, and I found that extremely appealing. So I decided to switch majors to computer science.”', 'Graduating in three years, Solar-Lezama decided to postpone his return to Mexico a little longer, applying to graduate programs at MIT, Carnegie Mellon University, and the University of California at Berkeley. “I thought, if I don’t get in to any of them, fine, I’ll go back to Mexico,” he says. Once again, MIT turned him down, as did CMU. But he got into Berkeley.', 'Solar-Lezama arrived at Berkeley planning to continue his work on large parallel computing systems, but his conversations with his advisor, Ras Bodik, quickly took a different turn. Different types of simulations generally required different computational strategies. But implementing those strategies often required reshuffling the same low-level processes. Was it possible, Bodik and Solar-Lezama wondered, to devise a way to formulate the strategies broadly and automate the reshuffling?', 'Solar-Lezama thus found himself part of a small community of researchers working on “program synthesis,” or the automatic generation of computer programs. His \\\\xa0thesis project was a language called Sketch, which lets programmers describe program functionality in general terms and automatically fills in the computational details.', 'Sketch treats program synthesis as a search problem: The task is to search the space of all possible programs for one that can meet the requirements imposed by the general description. The chief innovation behind Sketch was a set of algorithms for rapidly paring down the search space, so that a satisfactory program could be found in real time.', '“There were three or four of us who were pushing this area and telling everybody who would listen that this was the right direction for programming systems research, and for a long time there was a lot of hostility toward these kinds of ideas,” Solar-Lezama says. “Little by little, we started converting a few more people, and all of a sudden they reached a critical mass, and now it’s an extremely active area of research.”', 'After graduating from Berkeley, Solar-Lezama went on the job market, and MIT finally made him an offer. In his seven years at the Institute, where he recently earned tenure, Sketch has remained the foundation of his research, which has developed along three parallel tracks.', 'The first track is the extension of Sketch, so that it can handle more diverse and complex computations. The second is the application of Sketch and its underlying machinery to particular problems — such as orienting new members of large programming teams toward the existing code base, automatically grading programming homework, and parallelizing code for faster execution on multicore chips.', 'Recently, Solar-Lezama’s group has also begun investigating the application of program synthesis to machine learning. Machine learning involves teaching a computer system to perform some classification task by presenting it with training examples. But suppose that the training data consists of a row of three squares and a row of three circles. Which image belongs to the same class, a row of three stars or four circles arranged in a square?', 'Existing machine-learning systems are good at learning to recognize circles from examples of circles, but they’re not as good at the kind of abstract pattern matching that humans do intuitively. A program synthesizer, however, is much more likely to converge on a program for producing three-object rows than one that sometimes produces rows and sometimes produces squares.', 'Having finally made it to city with a good subway system, Solar-Lezama no longer has any plans to move back to Mexico. His wife has a Mexican father and spent much of her childhood in Mexico, but her mother is from Minnesota, and she had planned on settling in the U.S. when she and Solar-Lezama met in Berkeley. Their children, ages 6 and 3, might also find it hard to adjust to life in Mexico. Although they speak Spanish exclusively at home, they speak English at their school in Medford, Massachusetts, and, says Solar-Lezama, “they’re developing a Boston accent.”']\"}, {'Article Body': '[\\'How will advances in computing transform human society?\\', \\'MIT students contemplated this impending question as part of the Envisioning the Future of Computing Prize — an essay contest in which they were challenged to imagine ways that computing technologies could improve our lives, as well as the pitfalls and dangers associated with them.\\', \\'Offered for the first time this year, the Institute-wide competition invited MIT undergraduate and graduate students to share their ideas, aspirations, and vision for what they think a future propelled by advancements in computing holds. Nearly 60 students put pen to paper, including those majoring in mathematics, philosophy, electrical engineering and computer science, brain and cognitive sciences, chemical engineering, urban studies and planning, and management, and entered their submissions.\\', \\'Students dreamed up highly inventive scenarios for how the technologies of today and tomorrow could impact society, for better or worse. Some recurring themes emerged, such as tackling issues in climate change and health care. Others proposed ideas for particular technologies that ranged from digital twins as a tool for navigating the deluge of information online to a cutting-edge platform powered by artificial intelligence, machine learning, and biosensors to create personalized storytelling films that help individuals understand themselves and others.\\', \\'Conceived of by the Social and Ethical Responsibilities of Computing (SERC), a cross-cutting initiative of the MIT Schwarzman College of Computing in collaboration with the School of Humanities, Arts, and Social Sciences (SHASS), the intent of the competition was “to create a space for students to think in a creative, informed, and rigorous way about the societal benefits and costs of the technologies they are or will be developing,” says Caspar Hare, professor of philosophy, co-associate dean of SERC, and the lead organizer of the Envisioning the Future of Computing Prize. “We also wanted to convey that MIT values such thinking.”\\', \\'Prize winners\\', \\'The contest implemented a two-stage evaluation process wherein all essays were reviewed anonymously by a panel of MIT faculty members from the college and SHASS for the initial round. Three qualifiers were then invited to present their entries at an awards ceremony on May 8, followed by a Q&A with a judging panel and live in-person audience for the final round.\\', \"The winning entry was awarded to Robert Cunningham \\'23, a recent graduate in math and physics, for his paper on the implications of a personalized language model that is fine-tuned to predict an individual’s writing based on their past texts and emails. Told from the perspective of three fictional characters: Laura, founder of the tech startup ScribeAI, and Margaret and Vincent, a couple in college who are frequent users of the platform, readers gained insights into the societal shifts that take place and the unforeseen repercussions of the technology.\", \\'Cunningham, who took home the grand prize of $10,000, says he came up with the concept for his essay in late January while thinking about the upcoming release of GPT-4 and how it might be applied. Created by the developers of ChatGPT — an AI chatbot that has managed to capture popular imagination for its capacity to imitate human-like text, images, audio, and code — GPT-4, which was unveiled in March, is the newest version of OpenAI’s language model systems.\\', \"“GPT-4 is wild in reality, but some rumors before it launched were even wilder, and I had a few long\\\\xa0plane rides to\\\\xa0think about them! I enjoyed this opportunity to solidify a vague notion into a piece of writing, and since some of my favorite works of science fiction are short stories, I figured I\\'d take the chance to write one,” Cunningham says.\", \"The other two finalists, awarded $5,000 each, included Gabrielle Kaili-May Liu \\'23, a recent graduate in mathematics with computer science, and brain and cognitive sciences, for her entry on using the reinforcement learning with human feedback technique as a tool for transforming human interactions with AI; and Abigail Thwaites and Eliot Matthew Watkins, graduate students in the Department of Philosophy and Linguistics, for their joint submission on automatic fact checkers, an AI-driven software that they argue could potentially help mitigate the spread of misinformation and be a profound social good.\", \\'“We were so excited to see the amazing response to this contest. It made clear how much students at MIT, contrary to stereotype, really care about the wider implications of technology, says Daniel Jackson, professor of computer science and one of the final-round judges. “So many of the essays were incredibly thoughtful and creative. Robert’s story was a chilling, but entirely plausible take on our AI future; Abigail and Eliot’s analysis brought new clarity to what harms misinformation actually causes; and Gabrielle’s piece gave a lucid overview of a prominent new technology. I hope we’ll be able to run this contest every year, and that it will encourage all our students to broaden their perspectives even further.”\\', \\'Fellow judge Graham Jones, professor of anthropology, adds: “The winning entries reflected the incredible breadth of our students’ engagement with socially responsible computing. They challenge us to think differently about how to design computational technologies, conceptualize social impacts, and imagine future scenarios. Working with a cross-disciplinary panel of judges catalyzed lots of new conversations. As a sci-fi fan, I was thrilled that the top prize went to a such a stunning piece of speculative fiction!”\\', \\'Other judges on the panel for the final round included:\\', \\'Honorable mentions\\', \\'In addition to the grand prize winner and runners up, 12 students were recognized with honorable mentions for their entries, with each receiving $500.\\', \\'The honorees and the title of their essays include:\\', \\'The Envisioning the Future of Computing Prize was supported by MAC3 Impact Philanthropies.\\']'}, {'Article Body': '[\\'With a box of popcorn in one hand, Hal Abelson, a renowned computer scientist, strolled through the first floor of the Ray and Maria Stata Center studying the machine learning exhibits that surrounded him on the afternoon of Feb. 26. Everywhere he looked he saw evidence of the remarkable things MIT students can do when given access to computing resources.\\', \\'“Computing tools and infrastructure have gotten to a place where students can outperform professional researchers. You are constrained mostly by your imagination. It’s just an amazing time,” said Abelson, the Class of 1922 Professor of Computer Science and Engineering.\\', \\'Abelson, and a crowd of hundreds, was witnessing the kickoff of a three-day celebration of the MIT Stephen A. Schwarzman College of Computing. The afternoon event was an exposition of projects that transformed the student street lobby area of the Stata Center into a computing fairground of sorts, replete with courtesy popcorn, bubble tea, lemon squares, brownies, celebratory stickers, and a host of student exhibits that crossed disciplines, broke barriers, and inspired new thinking.\\', \\'For Kadeem Khan, a graduate student in urban studies and planning and an expo participant, the day was special. “I wanted to do a project focused on machine learning and the developing world,” he said. Khan applied machine learning to generate useful insights on poverty in Nairobi by analyzing data from multiple sources, including census, satellite imagery, and data from a geographic information system.\\', \\'“The poverty exhibit is an example of what I was just saying,” said Abelson. “Somehow the resources are here now to allow students to bring things to the next level.” Abeslson and Nicholas Roy, a professor of aeronautics and astronautics, CSAIL researcher, and director of the Bridge in the Quest for Intelligence, helped judge the teams during the monthlong student computing challenges leading up to yesterday.\\', \"Like Khan, MIT electrical engineering and computer science graduate student Natalie Lao embarked on a winning project with the potential to make transformative change in the world. “My background is in AI — but I\\'m also very interested in ethics and fairness and the risks involved when applying AI to the real world,” she said. Her team’s project uses network propagation and analysis to automatically discover and potentially halt the spread of fake news across a variety of media platforms. “We’re talking to the Department of Defense and various companies and trying to see how we can get the solution out in the world,” she said.\", \\'The MIT Schwarzman College of Computing, which represents a $1 billion commitment to addressing the global opportunities and challenges presented by the prevalence of computing and the rise of artificial intelligence, will provide students with unprecedented computing resources, including access to large data sets and the tools to learn from them. Yesterday, top entrants spoke in excited tones about the data sets they accessed during the Machine Learning Across Disciplines Challenge, which, along with the Connect Arts, Community, and Computing Challenge, was funded by the MIT-IBM Watson AI Lab.\\', \\'Graduate students Agni Orfanoudaki and Antonin Dauvin, who are both studying operations research at the MIT Sloan School of Management, applied machine learning and techniques developed at MIT Operations Research Center to patient data from Boston Medical Center spanning two decades. They are developing an analytic approach to understanding the impact of different anti-hypertensive drugs.\\', \\'Senior Sarah Wooders, an undergraduate in math and computer science, has collected a dataset of over 4 million product images and descriptions scraped from online sources. She then trained models to collectively label over 90 important clothing attributes and is is now building a system that can automatically label new clothing products. “It’s really exciting to see all the applications of AI,” said Wooders, also a top entrant. “My project feels like such an obvious idea but this type of system hasn’t been created yet. It seems the same thing is true for a lot of things in AI right now. And so someone like me can come along and do it.”\\']'}, {'Article Body': \"['Ask a smart home device for the weather forecast, and it takes several seconds for the device to respond. One reason this latency occurs is because connected devices don’t have enough memory or power to store and run the enormous machine-learning models needed for the device to understand what a user is asking of it. The model is stored in a data center that may be hundreds of miles away, where the answer is computed and sent to the device.', '', 'MIT researchers have created a new method for computing directly on these devices, which drastically reduces this latency. Their technique shifts the memory-intensive steps of running a machine-learning model to a central server where components of the model are encoded onto light waves.', '', 'The waves are transmitted to a connected device using fiber optics, which enables tons of data to be sent lightning-fast through a network. The receiver then employs a simple optical device that rapidly performs computations using the parts of a model carried by those light waves.', '', 'This technique leads to more than a hundredfold improvement in energy efficiency when compared to other methods. It could also improve security, since a user’s data do not need to be transferred to a central location for computation.', '', 'This method could enable a self-driving car to make decisions in real-time while using just a tiny percentage of the energy currently required by power-hungry computers. It could also allow a user to have a latency-free conversation with their smart home device, be used for live video processing over cellular networks, or even enable high-speed image classification on a spacecraft millions of miles from Earth.', '', '“Every time you want to run a neural network, you have to run the program, and how fast you can run the program depends on how fast you can pipe the program in from memory. Our pipe is massive — it corresponds to sending a full feature-length movie over the internet every millisecond or so. That is how fast data comes into our system. And it can compute as fast as that,” says senior author Dirk Englund, an associate professor in the Department of Electrical Engineering and Computer Science (EECS) and member of the MIT Research Laboratory of Electronics.', '', 'Joining Englund on the paper is lead author and EECS grad student Alexander Sludds; EECS grad student Saumil Bandyopadhyay, Research Scientist Ryan Hamerly, as well as others from MIT, the MIT Lincoln Laboratory, and Nokia Corporation. The research is published today in Science.', '', 'Lightening the load', '', 'Neural networks are machine-learning models that use layers of connected nodes, or neurons, to recognize patterns in datasets and perform tasks, like classifying images or recognizing speech. But these models can contain billions of weight parameters, which are numeric values that transform input data as they are processed. These weights must be stored in memory. At the same time, the data transformation process involves billions of algebraic computations, which require a great deal of power to perform.', '', 'The process of fetching data (the weights of the neural network, in this case) from memory and moving them to the parts of a computer that do the actual computation is one of the biggest limiting factors to speed and energy efficiency, says Sludds.', '', '“So our thought was, why don’t we take all that heavy lifting — the process of fetching billions of weights from memory — move it away from the edge device and put it someplace where we have abundant access to power and memory, which gives us the ability to fetch those weights quickly?” he says.', '', 'The neural network architecture they developed, Netcast, involves storing weights in a central server that is connected to a novel piece of hardware called a smart transceiver. This smart transceiver, a thumb-sized chip that can receive and transmit data, uses technology known as silicon photonics to fetch trillions of weights from memory each second.', '', 'It receives weights as electrical signals and imprints them onto light waves. Since the weight data are encoded as bits (1s and 0s) the transceiver converts them by switching lasers; a laser is turned on for a 1 and off for a 0. It combines these light waves and then periodically transfers them through a fiber optic network so a client device doesn’t need to query the server to receive them.', '', '“Optics is great because there are many ways to carry data within optics. For instance, you can put data on different colors of light, and that enables a much higher data throughput and greater bandwidth than with electronics,” explains Bandyopadhyay.', '', 'Trillions per second', '', 'Once the light waves arrive at the client device, a simple optical component known as a broadband “Mach-Zehnder” modulator uses them to perform super-fast, analog computation. This involves encoding input data from the device, such as sensor information, onto the weights. Then it sends each individual wavelength to a receiver that detects the light and measures the result of the computation.', '', 'The researchers devised a way to use this modulator to do trillions of multiplications per second, which vastly increases the speed of computation on the device while using only a tiny amount of power.', '', '“In order to make something faster, you need to make it more energy efficient. But there is a trade-off. We’ve built a system that can operate with about a milliwatt of power but still do trillions of multiplications per second. In terms of both speed and energy efficiency, that is a gain of orders of magnitude,” Sludds says.', '', 'They tested this architecture by sending weights over an 86-kilometer fiber that connects their lab to MIT Lincoln Laboratory. Netcast enabled machine-learning with high accuracy — 98.7 percent for image classification and 98.8 percent for digit recognition — at rapid speeds.', '', '“We had to do some calibration, but I was surprised by how little work we had to do to achieve such high accuracy out of the box. We were able to get commercially relevant accuracy,” adds Hamerly.', '', 'Moving forward, the researchers want to iterate on the smart transceiver chip to achieve even better performance. They also want to miniaturize the receiver, which is currently the size of a shoe box, down to the size of a single chip so it could fit onto a smart device like a cell phone.', '', '“Using photonics and light as a platform for computing is a really exciting area of research with potentially huge implications on the speed and efficiency of our information technology landscape,” says Euan Allen, a Royal Academy of Engineering Research Fellow at the University of Bath, who was not involved with this work. “The work of Sludds et al. is an exciting step toward seeing real-world implementations of such devices, introducing a new and practical edge-computing scheme whilst also exploring some of the fundamental limitations of computation at very low (single-photon) light levels.”', '', 'The research is funded, in part, by NTT Research, the National Science Foundation, the Air Force Office of Scientific Research, the Air Force Research Laboratory, and the Army Research Office.']\"}, {'Article Body': '[\\'Traditional computer scientists and engineers are trained to develop solutions for specific needs, but aren’t always trained to consider their broader implications. Each new technology generation, and particularly the rise of artificial intelligence, leads to new kinds of systems, new ways of creating tools, and new forms of data, for which norms, rules, and laws frequently have yet to catch up. The kinds of impact that such innovations have in the world has often not been apparent until many years later.\\', \\'As part of the efforts in Social and Ethical Responsibilities of Computing (SERC) within the MIT Stephen A. Schwarzman College of Computing, a new case studies series examines social, ethical, and policy challenges of present-day efforts in computing with the aim of facilitating the development of responsible “habits of mind and action” for those who create and deploy computing technologies.\\', \\'“Advances in computing have undeniably changed much of how we live and work. Understanding and incorporating broader social context is becoming ever more critical,” says Daniel Huttenlocher, dean of the MIT Schwarzman College of Computing. “This case study series is designed to be a basis for discussions in the classroom and beyond, regarding social, ethical, economic, and other implications so that students and researchers can pursue the development of technology across domains in a holistic manner that addresses these important issues.”\\', \\'A modular system\\', \\'By design, the case studies are brief and modular to allow users to mix and match the content to fit a variety of pedagogical needs. Series editors David Kaiser and Julie Shah, who are the associate deans for SERC, structured the cases primarily to be appropriate for undergraduate instruction across a range of classes and fields of study.\\', \\'“Our goal was to provide a seamless way for instructors to integrate cases into an existing course or cluster several cases together to support a broader module within a course. They might also use the cases as a starting point to design new courses that focus squarely on themes of social and ethical responsibilities of computing,” says Kaiser, the Germeshausen Professor of the History of Science and professor of physics.\\', \\'Shah, an associate professor of aeronautics and astronautics and a roboticist who designs systems in which humans and machines operate side by side, expects that the cases will also be of interest to those outside of academia, including computing professionals, policy specialists, and general readers. In curating the series, Shah says that “we interpret ‘social and ethical responsibilities of computing’ broadly to focus on perspectives of people who are affected by various technologies, as well as focus on perspectives of designers and engineers.”\\', \\'The cases are not limited to a particular format and can take shape in various forms — from a magazine-like feature article or Socratic dialogues to choose-your-own-adventure stories or role-playing games grounded in empirical research. Each case study is brief, but includes accompanying notes and references to facilitate more in-depth exploration of a given topic. Multimedia projects will also be considered. “The main goal is to present important material — based on original research — in engaging ways to broad audiences of non-specialists,” says Kaiser.\\', \\'The SERC case studies are specially commissioned and written by scholars who conduct research centrally on the subject of the piece. Kaiser and Shah approached researchers from within MIT as well as from other academic institutions to bring in a mix of diverse voices on a spectrum of topics. Some cases focus on a particular technology or on trends across platforms, while others assess social, historical, philosophical, legal, and cultural facets that are relevant for thinking critically about current efforts in computing and data sciences.\\', \\'The cases published in the inaugural issue place readers in various settings that challenge them to consider the social and ethical implications of computing technologies, such as how social media services and surveillance tools are built; the racial disparities that can arise from deploying facial recognition technology in unregulated, real-world settings; the biases of risk prediction algorithms in the criminal justice system; and the politicization of data collection.\\', \\'\"Most of us agree that we want computing to work for social good, but which good? Whose good? Whose needs and values and worldviews are prioritized and whose are overlooked?” says Catherine D’Ignazio, an assistant professor of urban science and planning and director of the Data + Feminism Lab at MIT.\\', \\'D’Ignazio’s case for the series, co-authored with Lauren Klein, an associate professor in the English and Quantitative Theory and Methods departments at Emory University, introduces readers to the idea that while data are useful, they are not always neutral. “These case studies help us understand the unequal histories that shape our technological systems as well as study their disparate outcomes and effects. They are an exciting step towards holistic, sociotechnical thinking and making.\"\\', \\'Rigorously reviewed\\', \\'Kaiser and Shah formed an editorial board composed of 55 faculty members and senior researchers associated with 19 departments, labs, and centers at MIT, and instituted a rigorous peer-review policy model commonly adopted by specialized journals. Members of the editorial board will also help commission topics for new cases and help identify authors for a given topic.\\', \\'For each submission, the series editors collect four to six peer reviews, with reviewers mostly drawn from the editorial board. For each case, half the reviewers come from fields in computing and data sciences and half from fields in the humanities, arts, and social sciences, to ensure balance of topics and presentation within a given case study and across the series.\\', \\'“Over the past two decades I’ve become a\\\\xa0bit jaded when it comes to the academic review process, and so I was\\\\xa0particularly heartened to see such care and thought put into all of the reviews,\" says Hany Farid, a professor at the University of California at Berkeley with a joint appointment in the Department of Electrical Engineering and Computer Sciences and the School of Information. “The constructive review process made our case study significantly stronger.”\\', \\'Farid’s case, “The Dangers of Risk Prediction in the Criminal Justice System,” which he penned with Julia Dressel, recently a student of computer science at Dartmouth College, is one of the four commissioned pieces featured in the inaugural issue.\\', \\'Cases are additionally reviewed by undergraduate volunteers, who help the series editors gauge each submission for balance, accessibility for students in multiple fields of study, and possibilities for adoption in specific courses. The students also work with them to create original homework problems and active learning projects to accompany each case study, to further facilitate adoption of the original materials across a range of existing undergraduate subjects.\\', \"“I volunteered to work with this group because I believe that it\\'s incredibly important for those working in computer science to include thinking about ethics not as an afterthought, but integrated into every step and decision that is made, says Annie Snyder, a mathematical economics sophomore and a member of the MIT Schwarzman College of Computing’s Undergraduate Advisory Group. “While this is a massive issue to take on, this project is an amazing opportunity to start building an ethical culture amongst the incredibly talented students at MIT who will hopefully carry it forward into their own projects and workplace.”\", \\'New sets of case studies, produced with support from\\\\xa0the MIT Press’ Open Publishing Services program, will be published twice a year via the Knowledge Futures Group’s\\\\xa0PubPub\\\\xa0platform. The SERC case studies are made available for free on an open-access basis, under Creative Commons licensing terms. Authors retain copyright, enabling them to reuse and republish their work in more specialized scholarly publications.\\', \\'“It was important to us to approach this project in an inclusive way and lower the barrier for people to be able to access this content. These are complex issues that we need to deal with, and we hope that by making the cases widely available, more people will engage in social and ethical considerations as they’re studying and developing computing technologies,” says Shah.\\']'}, {'Article Body': '[\\'“The Laughing Room,”\\\\xa0an interactive art installation by author, illustrator, and MIT graduate student Jonathan \"Jonny\" Sun, looks like a typical living room: couches, armchairs, coffee table, soft lighting. This cozy scene, however, sits in a glass-enclosed space, flanked by bright lights and a microphone, with a bank of laptops and a video camera positioned across the room. People wander in, take a seat, begin chatting. After a pause in the conversation, a riot of canned laughter rings out, prompting genuine giggles from the group.\\', \\'Presented at the Cambridge Public Library in Cambridge, Massachusetts, Nov. 16-18, \"The Laughing Room\" was an artificially intelligent room programmed to play an audio laugh track whenever participants said something that its algorithm deemed funny. Sun, who is currently on leave from his PhD program within the MIT Department of Urban Studies and Planning, is an affiliate at the Berkman Klein Center for Internet and Society at Harvard University, and creative researcher at the metaLAB at Harvard, created the project to explore the increasingly social and cultural roles of technology in public and private spaces, users’ agency within and dependence on such technology, and the issues of privacy raised by these systems. The installations were presented as part of ARTificial Intelligence, an ongoing program led by MIT associate professor of literature Stephanie Frampton that fosters public dialogue about the emerging ethical and social implications of artificial intelligence (AI) through art and design.\\']'}]], 'distances': [[1.3810615539550781, 1.4073359966278076, 1.4421770572662354, 1.486783742904663, 1.4912267923355103, 1.4972448348999023, 1.4984560012817383, 1.4989073276519775, 1.5020194053649902, 1.505950689315796]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(query_texts=[\"laptop\"], n_results=10 )\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92a93c1",
   "metadata": {},
   "source": [
    "## Vector MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c01b3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3756b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "getado = collection.get(ids=\"id141\", \n",
    "                       include=[\"documents\", \"embeddings\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e833db1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.73729430e-02, -1.82920452e-02,  1.48837995e-02,\n",
       "        -6.51341975e-02,  2.60852501e-02, -3.37915793e-02,\n",
       "        -8.85920227e-02, -2.32951622e-02,  3.66715305e-02,\n",
       "        -5.46598248e-02, -4.64128628e-02, -8.24273191e-03,\n",
       "         1.19425273e-02,  4.93168868e-02,  3.10725272e-02,\n",
       "        -3.82354259e-02,  2.31101066e-02,  1.22701988e-01,\n",
       "        -6.19672947e-02,  5.81677221e-02,  1.47276628e-03,\n",
       "        -7.65489414e-02,  3.21513750e-02, -8.23823139e-02,\n",
       "         6.63918778e-02,  4.48568985e-02,  4.21497077e-02,\n",
       "        -8.53938982e-02, -6.01443984e-02, -6.61355108e-02,\n",
       "        -2.66331788e-02,  6.44640326e-02, -4.93826345e-02,\n",
       "         4.94234189e-02, -1.08341053e-02,  5.14731668e-02,\n",
       "        -3.51315998e-02,  6.42413571e-02, -9.27531570e-02,\n",
       "        -3.59810181e-02, -8.51940811e-02,  6.69028834e-02,\n",
       "        -2.12268773e-02, -9.54463799e-03,  7.42490366e-02,\n",
       "         7.68522471e-02,  8.58911593e-03, -2.83467062e-02,\n",
       "         3.36568616e-02,  1.72231649e-03, -8.33549798e-02,\n",
       "        -5.49885258e-02, -2.26667766e-02,  2.02457476e-02,\n",
       "        -4.22268026e-02,  8.29240456e-02,  5.33289649e-02,\n",
       "        -9.79389064e-03, -2.85167377e-02,  7.09009841e-02,\n",
       "         7.97037259e-02,  4.17227931e-02, -1.70167368e-02,\n",
       "         3.81954364e-03,  5.04044183e-02,  1.54670291e-02,\n",
       "        -4.92304303e-02, -9.20306817e-02, -3.04638669e-02,\n",
       "        -1.18113887e-02, -4.06130254e-02,  2.30968390e-02,\n",
       "        -3.77157144e-02, -6.83670118e-02,  4.81336750e-02,\n",
       "        -3.41931954e-02, -4.40390259e-02, -5.85925318e-02,\n",
       "         1.64189339e-02, -5.24764359e-02,  8.13472047e-02,\n",
       "         2.94023454e-02,  1.79613917e-03,  1.03220783e-01,\n",
       "        -8.25282633e-02, -2.25818679e-02, -1.10813655e-01,\n",
       "         2.54925359e-02, -6.96044639e-02, -1.87111851e-02,\n",
       "        -2.32404284e-02, -9.19320434e-02, -7.96021596e-02,\n",
       "        -3.08665144e-03,  6.44251779e-02, -1.16472438e-01,\n",
       "         6.06766343e-02, -7.57522061e-02,  8.29777792e-02,\n",
       "         1.35594858e-02,  1.04740765e-02,  2.27763923e-03,\n",
       "         5.63155264e-02, -3.26687954e-02,  7.61456266e-02,\n",
       "         5.02772592e-02,  6.50320724e-02, -1.63914133e-02,\n",
       "         1.22323157e-02, -4.72140238e-02, -6.26809429e-03,\n",
       "        -8.66247341e-02, -3.01937815e-02, -2.70902980e-02,\n",
       "        -4.67833085e-03, -1.80513822e-02,  2.33877767e-02,\n",
       "         1.39709450e-02,  8.65843322e-04, -4.40095216e-02,\n",
       "        -5.70356995e-02, -5.11068627e-02,  1.56870764e-02,\n",
       "        -1.64536503e-03, -9.16432403e-03, -4.90796342e-02,\n",
       "         2.37428825e-02,  5.49692860e-33, -4.64475639e-02,\n",
       "         7.54008815e-02,  3.28044556e-02,  9.57196131e-02,\n",
       "         5.39640598e-02, -6.04378199e-03, -8.17613862e-03,\n",
       "        -3.65759544e-02,  1.41120134e-02, -1.23663945e-02,\n",
       "         4.73293150e-03,  1.03953891e-02,  5.21081686e-02,\n",
       "         1.23025946e-01, -1.96301229e-02, -8.30520876e-03,\n",
       "        -3.29059176e-02,  8.11921060e-02, -5.27103851e-03,\n",
       "         6.61904216e-02,  4.02407385e-02, -2.21358240e-02,\n",
       "         7.23727420e-02, -1.05681457e-02, -2.35703103e-02,\n",
       "        -9.33776144e-03,  1.43310858e-03,  3.24568488e-02,\n",
       "         2.87720058e-02, -3.53213213e-02, -1.20750554e-02,\n",
       "         8.80075619e-02,  1.38349719e-02, -8.18203911e-02,\n",
       "         2.17739260e-03,  5.90585545e-02,  6.62508756e-02,\n",
       "        -8.41436535e-03,  4.86696232e-03,  1.98719539e-02,\n",
       "         1.73457004e-02,  7.86906555e-02,  4.66076017e-04,\n",
       "        -7.63906911e-02, -5.98657802e-02,  7.64949992e-02,\n",
       "         1.47764981e-02,  3.64876166e-02, -1.36675164e-01,\n",
       "         4.49585728e-02,  4.07074206e-02, -2.45160400e-03,\n",
       "        -5.19066565e-02, -6.56486899e-02, -5.19786291e-02,\n",
       "        -5.71896099e-02,  2.73935478e-02, -4.65551578e-02,\n",
       "         2.66802777e-02,  3.02752145e-02,  5.75919822e-02,\n",
       "         5.92703931e-02, -2.57594213e-02,  1.29700713e-02,\n",
       "        -2.78219096e-02, -1.16384747e-02,  5.05717322e-02,\n",
       "        -5.31274080e-02,  1.51599096e-02,  5.84016107e-02,\n",
       "        -4.46436331e-02, -5.49304532e-03, -5.16262352e-02,\n",
       "        -1.01295590e-01,  4.70363622e-04, -3.13107781e-02,\n",
       "        -4.69197705e-02, -2.96169594e-02,  3.53327915e-02,\n",
       "         3.51940729e-02, -9.89155397e-02, -9.81751457e-03,\n",
       "         3.34000774e-02, -4.95204069e-02, -4.52375151e-02,\n",
       "        -3.80358733e-02,  5.52948937e-03, -8.08642209e-02,\n",
       "        -1.21838087e-02,  2.59452574e-02, -4.83420938e-02,\n",
       "        -3.37627158e-02,  2.26924140e-02, -3.04410905e-02,\n",
       "         3.10631301e-02, -5.20923628e-33, -1.21634835e-02,\n",
       "         4.71201688e-02, -4.49178629e-02,  1.57166962e-02,\n",
       "        -2.54337806e-02, -7.46235549e-02,  6.79395860e-03,\n",
       "         9.57199000e-03, -9.20371339e-03, -1.80449579e-02,\n",
       "        -1.73550621e-02,  3.42719606e-04,  3.44769545e-02,\n",
       "        -2.42378823e-02, -2.16381326e-02, -6.57601142e-03,\n",
       "         5.16994782e-02, -7.30758831e-02, -1.03641853e-01,\n",
       "        -8.44994746e-03,  4.20945324e-02, -2.49091648e-02,\n",
       "        -4.44757789e-02, -1.09489551e-02, -6.83219805e-02,\n",
       "         1.26909196e-01,  1.08403908e-02, -7.17008412e-02,\n",
       "        -3.44387861e-03,  3.16013619e-02,  9.72609967e-03,\n",
       "        -7.80914724e-02,  3.16402167e-02, -4.12862077e-02,\n",
       "         3.08486726e-02,  2.26313360e-02, -8.90398864e-03,\n",
       "         1.50029743e-02, -1.96759403e-02, -6.01917654e-02,\n",
       "        -2.95444708e-02,  6.06330335e-02, -5.52885905e-02,\n",
       "        -9.64678638e-03, -4.35432345e-02, -2.70254854e-02,\n",
       "        -3.04582883e-02,  3.11558153e-02,  4.88401204e-02,\n",
       "         9.90955606e-02, -2.28372682e-02,  4.98579741e-02,\n",
       "        -2.00985046e-03, -6.86700568e-02,  2.37269644e-02,\n",
       "        -5.43654151e-03, -8.05532709e-02,  2.62471922e-02,\n",
       "         9.33694839e-02,  7.49709383e-02, -2.10245810e-02,\n",
       "        -7.17199370e-02, -8.66340697e-02,  6.19059475e-03,\n",
       "         4.43572663e-02,  4.95354570e-02, -6.26671761e-02,\n",
       "         5.88614680e-03, -1.52820274e-02,  5.41663915e-02,\n",
       "         3.21633518e-02,  3.44639309e-02,  2.87035722e-02,\n",
       "         2.74726506e-02,  5.85457422e-02, -5.82365803e-02,\n",
       "         1.87122691e-02,  1.33299723e-01,  1.67611297e-02,\n",
       "         4.22332324e-02,  3.17729115e-02, -5.54753616e-02,\n",
       "         6.02095053e-02,  1.39755085e-01,  2.90010795e-02,\n",
       "         5.48472628e-02, -4.37598825e-02, -5.63274212e-02,\n",
       "         2.87509579e-02, -4.56584767e-02, -2.94223726e-02,\n",
       "        -4.42487895e-02,  9.98843648e-03,  1.89003104e-03,\n",
       "         2.55665672e-03, -4.48331541e-08, -5.11927456e-02,\n",
       "        -5.13805784e-02,  5.16729690e-02, -1.67850349e-02,\n",
       "        -4.11430933e-02, -2.33594831e-02,  2.42624935e-02,\n",
       "         1.84631884e-01, -5.81001937e-02, -1.37662701e-02,\n",
       "        -2.78350860e-02, -1.22602750e-02, -1.84276756e-02,\n",
       "        -9.66010732e-04,  1.10957973e-01,  2.61355545e-02,\n",
       "         3.09017524e-02,  3.20747681e-02, -1.03529291e-02,\n",
       "         7.47529268e-02, -3.08721662e-02,  8.37991072e-04,\n",
       "        -3.11900303e-02,  2.35163346e-02,  3.53328623e-02,\n",
       "         6.37331828e-02, -5.26830480e-02, -1.52765969e-02,\n",
       "         2.53567304e-02,  7.71286956e-04,  4.45574373e-02,\n",
       "         3.76866162e-02,  4.34288681e-02,  3.38883176e-02,\n",
       "         4.19909619e-02, -9.37653407e-02, -5.85979782e-03,\n",
       "        -1.13630749e-03, -5.76280989e-02, -9.27795544e-02,\n",
       "        -5.83038479e-02,  1.11328112e-03,  6.59075975e-02,\n",
       "         5.11282161e-02,  1.19797565e-01,  2.43364740e-02,\n",
       "         1.28266484e-01, -1.10065555e-02, -5.55888042e-02,\n",
       "         1.20822955e-02, -7.41595402e-02,  6.38885051e-02,\n",
       "         3.92470248e-02,  1.19049579e-01,  3.19032036e-02,\n",
       "        -3.73786576e-02,  4.18322980e-02,  1.43809346e-02,\n",
       "         2.45870259e-02,  8.92644450e-02, -1.02572441e-02,\n",
       "         3.09635419e-02, -7.81133249e-02, -1.82880871e-02]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_vectors = getado[\"embeddings\"]\n",
    "word_list = getado[\"documents\"]\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351808e8",
   "metadata": {},
   "source": [
    "Once we have our information inside the Database we can query It, and ask for data that matches our needs. The search is done inside the content of the document, and it dosn't look for the exact word, or phrase. The results will be based on the similarity between the search terms and the content of documents. \n",
    "\n",
    "The metadata is not used in the search, but they can be utilized for filtering or refining the results after the initial search. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d0870",
   "metadata": {},
   "source": [
    "## Loading the model and creating the prompt\n",
    "TRANSFORMERS!!\n",
    "Time to use the library **transformers**, the most famous library from [hugging face](https://huggingface.co/) for working with language models. \n",
    "\n",
    "We are importing: \n",
    "* **Autotokenizer**: It is a utility class for tokenizing text inputs that are compatible with various pre-trained language models.\n",
    "* **AutoModelForCasualLLM**: it provides an interface to pre-trained language models specifically designed for language generation tasks using causal language modeling (e.g., GPT models), or the model used in this notebook ***databricks/dolly-v2-3b***.\n",
    "* **pipeline**: provides a simple interface for performing various natural language processing (NLP) tasks, such as text generation (our case) or text classification. \n",
    "\n",
    "The model selected is [dolly-v2-3b](https://huggingface.co/databricks/dolly-v2-3b), the smallest Dolly model. It have 3billion paramaters, more than enough for our sample, and works much better than GPT2. \n",
    "\n",
    "Please, feel free to test [different Models](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending), you need to search for NLP models trained for text-generation. My recomendation is choose \"small\" models, or we will run out of memory in kaggle.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1605e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d1ac90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 15:57:53.912504: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-25 15:57:53.912575: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-25 15:57:53.913744: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-25 15:57:53.920359: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-25 15:57:54.872850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e25e1b66f24cbdb286b6307b92356f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/450 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9d2a244d2347f7adf6dbef4b726eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4607766d83dc44c280d0bf5c174adfd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/228 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cc5cb698b74d4dbf7d5eadbddaca1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4364c71c514ef5a169ee175a2478b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"databricks/dolly-v2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d507f9f",
   "metadata": {},
   "source": [
    "The next step is to initialize the pipeline using the objects created above. \n",
    "\n",
    "The model's response is limited to 256 tokens, for this project I'm not interested in a longer response, but it can easily be extended to whatever length you want.\n",
    "\n",
    "Setting ***device_map*** to ***auto*** we are instructing the model to automaticaly select the most appropiate device: CPU or GPU for processing the text generation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66f535",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55af484b",
   "metadata": {},
   "source": [
    "## Creating the extended prompt\n",
    "To create the prompt we use the result from query the Vector Database  and the sentence introduced by the user. \n",
    "\n",
    "The prompt have two parts, the **relevant context** that is the information recovered from the database and the **user's question**. \n",
    "\n",
    "We only need to join the two parts together to create the prompt that we are going to send to the model. \n",
    "\n",
    "You can limit the lenght of the context passed to the model, because we can get some Memory problems with one of the datasets that contains a realy large text in the document part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef2d967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Relevant context: #[\\'When humans look at a scene, they see objects and the relationships between them. On top of your desk, there might be a laptop that is sitting to the left of a phone, which is in front of a computer monitor.\\', \\'\\', \\'Many deep learning models struggle to see the world this way because they don’t understand the entangled relationships between individual objects. Without knowledge of these relationships, a robot designed to help someone in a kitchen would have difficulty following a command like “pick up the spatula that is to the left of the stove and place it on top of the cutting board.”\\', \\'\\', \\'In an effort to solve this problem, MIT researchers have developed a model that understands the underlying relationships between objects in a scene. Their model represents individual relationships one at a time, then combines these representations to describe the overall scene. This enables the model to generate more accurate images from text descriptions, even when the scene includes several objects that are arranged in different relationships with one another.\\', \\'\\', \\'This work could be applied in situations where industrial robots must perform intricate, multistep manipulation tasks, like stacking items in a warehouse or assembling appliances. It also moves the field one step closer to enabling machines that can learn from and interact with their environments more like humans do.\\', \\'\\', \\'“When I look at a table, I can’t say that there is an object at XYZ location. Our minds don’t work like that. In our minds, when we understand a scene, we really understand it based on the relationships between the objects. We think that by building a system that can understand the relationships between objects, we could use that system to more effectively manipulate and change our environments,” says Yilun Du, a PhD student in the Computer Science and Artificial Intelligence Laboratory (CSAIL) and co-lead author of the paper.\\', \\'\\', \\'Du wrote the paper with co-lead authors Shuang Li, a CSAIL PhD student, and Nan Liu, a graduate student at the University of Illinois at Urbana-Champaign; as well as Joshua B. Tenenbaum, a professor of computational cognitive science in the Department of Brain and Cognitive Sciences and a member of CSAIL; and senior author Antonio Torralba, the Delta Electronics Professor of Electrical Engineering and Computer Science and a member of CSAIL. The research will be presented at the Conference on Neural Information Processing Systems in December.\\', \\'\\', \\'One relationship at a time\\', \\'\\', \\'The framework the researchers developed can generate an image of a scene based on a text description of objects and their relationships, like “A wood table to the left of a blue stool. A red couch to the right of a blue stool.”\\', \\'\\', \\'Their system would break these sentences down into two smaller pieces that describe each individual relationship (“a wood table to the left of a blue stool” and “a red couch to the right of a blue stool”), and then model each part separately. Those pieces are then combined through an optimization process that generates an image of the scene.\\', \\'\\', \\'The researchers used a machine-learning technique called energy-based models to represent the individual object relationships in a scene description. This technique enables them to use one energy-based model to encode each relational description, and then compose them together in a way that infers all objects and relationships.\\', \\'\\', \\'By breaking the sentences down into shorter pieces for each relationship, the system can recombine them in a variety of ways, so it is better able to adapt to scene descriptions it hasn’t seen before, Li explains.\\', \\'\\', \\'“Other systems would take all the relations holistically and generate the image one-shot from the description. However, such approaches fail when we have out-of-distribution descriptions, such as descriptions with more relations, since these model can’t really adapt one shot to generate images containing more relationships. However, as we are composing these separate, smaller models together, we can model a larger number of relationships and adapt to novel combinations,” Du says.\\', \\'\\', \\'The system also works in reverse — given an image, it can find text descriptions that match the relationships between objects in the scene. In addition, their model can be used to edit an image by rearranging the objects in the scene so they match a new description.\\', \\'\\', \\'Understanding complex scenes\\', \\'\\', \\'The researchers compared their model to other deep learning methods that were given text descriptions and tasked with generating images that displayed the corresponding objects and their relationships. In each instance, their model outperformed the baselines.\\', \\'\\', \\'They also asked humans to evaluate whether the generated images matched the original scene description. In the most complex examples, where descriptions contained three relationships, 91 percent of participants concluded that the new model performed better.\\', \\'\\', \\'“One interesting thing we found is that for our model, we can increase our sentence from having one relation description to having two, or three, or even four descriptions, and our approach continues to be able to generate images that are correctly described by those descriptions, while other methods fail,” Du says.\\', \\'\\', \\'The researchers also showed the model images of scenes it hadn’t seen before, as well as several different text descriptions of each image, and it was able to successfully identify the description that best matched the object relationships in the image.\\', \\'\\', \\'And when the researchers gave the system two relational scene descriptions that described the same image but in different ways, the model was able to understand that the descriptions were equivalent.\\', \\'\\', \\'The researchers were impressed by the robustness of their model, especially when working with descriptions it hadn’t encountered before.\\', \\'\\', \\'“This is very promising because that is closer to how humans work. Humans may only see several examples, but we can extract useful information from just those few examples and combine them together to create infinite combinations. And our model has such a property that allows it to learn from fewer data but generalize to more complex scenes or image generations,” Li says.\\', \\'\\', \\'While these early results are encouraging, the researchers would like to see how their model performs on real-world images that are more complex, with noisy backgrounds and objects that are blocking one another.\\', \\'\\', \\'They are also interested in eventually incorporating their model into robotics systems, enabling a robot to infer object relationships from videos and then apply this knowledge to manipulate objects in the world.\\', \\'\\', \\'“Developing visual representations that can deal with the compositional nature of the world around us is one of the key open problems in computer vision. This paper makes significant progress on this problem by proposing an energy-based model that explicitly models multiple relations among the objects depicted in the image. The results are really impressive,” says Josef Sivic, a distinguished researcher at the Czech Institute of Informatics, Robotics, and Cybernetics at Czech Technical University, who was not involved with this research.\\', \\'\\', \\'This research is supported, in part, by Raytheon BBN Technologies Corp., Mitsubishi Electric Research Laboratory, the National Science Foundation, the Office of Naval Research, and the IBM Thomas J. Watson Research Center.\\'] #[\\'Computers have become so pervasive in today’s world that preparing students to work and assume leadership roles in this shifting landscape requires giving them a better understanding of how computers work, how to use them, and how they affect every aspect of society. That’s the reasoning behind the creation of the new MIT Stephen A. Schwarzman College of Computing, and it was the theme of many of the presentations and panel discussions in this week’s three-day celebration of the new college.\\', \\'“We’re in the midst of a global transformation that’s catalyzed by the rapid acceleration of digital technologies, including unprecedented access to computation and data,” said Farnam Jahanian, president of Carnegie Mellon University, in a keynote address on Wednesday. “The scale and scope and pace of these advances are truly unprecedented in human history.”\\', \\'“The impact of these technologies is ubiquitous,” he said, “with a wide range of applications from health care to transportation, finance, energy, manufacturing, and far beyond. … The pace of innovation is accelerating dramatically.”\\', \\'These changes require a profound rethinking of the role of education in this rapidly changing environment, Jahanian said. “Imagine a day when by integrating emerging technologies, such as AI-enabled learning techniques and inverted classrooms, we can achieve personalized, outcome-based education,” he said.\\', \\'MIT Provost Martin Schmidt, in a discussion with reporters, said that in creating the MIT Schwarzman College of Computing, “one of the things that’s really critical to us is that not only should this advance computation, but it should really link to all the disciplines across the campus.” The college will “strengthen those disciplines in their use of these new tools,” he said, “but also when we learn things about how we apply those tools to the disciplines, that knowledge flows back … and informs the next generation” of computing research.\\', \\'Schmidt added that in planning the new college, a key question was how MIT will deliver on its promise of making sure that the college “has in its DNA” an awareness of the societal impact of current and future advances in computing. This appreciation “should inform our educational agenda, what our undergraduates and graduates learn in the classroom, and it should inform our research agenda,” he said. “It should shape how the research is performed, and the kind of content we produce that informs policies and informs governments on how they should respond to the deployment of these technologies.”\\', \\'The new college was founded partly in response to the fact that “there really was a transformation occurring across the campus,” with computation increasingly forming a key part of the work in amost all disciplines, Schmidt said. While about 40 percent of MIT students major in computer science, there was a clear need for an even greater integration of computation and data science early and deeply into every aspect of education.\\', \\'Melissa Nobles, dean of MIT’s School of Humanities, Arts and Social Sciences, who also participated in the discussion, told reporters that students in that school were very excited to take part in this increased integration of their disciplines with computation. She cited examples of classes where mixed groups of computer science students and those majoring in arts, economics, or literature worked on problems that combined their different kinds of expertise. In one class, for example, the students studied in exhaustive detail the way writers of 19th century novels used male and female pronouns and how that related to the genders of the author and the main characters. The project required both computer expertise to analyze thousands of texts, and a knowledge of the literature in order to provide context for their findings.\\', \\'Also during the discussion, Maria Klawe, president of Harvey Mudd College in California and another keynote speaker, pointed out that a deep understanding of computers and their impact is increasingly needed in a rapidly changing world where it is estimated that many of the jobs people perform today “are just going to disappear” within the next few decades. That makes interdisciplinary education more important than ever, she said.\\', \\'Regarding the creation of the new college, she said, “I see this as an incredibly important step for MIT, and I think it’s going to influence other institutions to do similar things.”\\', \\'The goals of the college reach far beyond just helping people in other disciplines to use computers more effectively, Nobles and others emphasized. It’s also important, they said, to make sure that the skills and knowledge from other fields flow back into computer science, influencing the ethical, political, and social implications of the work in that field — not just as an afterthought but as a fundamental part of thinking and planning.\\', \\'For example, while it is tempting to make use of massive sets of data collected by social media, the use of such datasets can raise serious concerns about privacy and informed consent. Such issues may be relatively new territory for computer scientists, but they are longstanding issues that have been dealt with extensively by social scientists and philosophers whose expertise can help inform the data collection and analysis procedures.\\', \\'The speakers at Wednesday’s symposium, representing many different fields and institutions, shared a sense of excitement about the potential for the MIT Schwarzman College of Computing to bring about significant innovations. “MIT continues to be a world-class institution that offers a distinctive education and research, of course,” Jahanian said in his keynote, “and this latest development will certainly increase its impact in this changing world.”\\'] #[\\'“Who is Bram Stoker?” Those three words demonstrated the amazing potential of artificial intelligence. It was the answer to a final question in a particularly memorable 2011 episode of\\\\xa0Jeopardy!. The three competitors were former champions Brad Rutter and Ken Jennings, and Watson, a super computer developed by IBM. By answering the final question correctly, Watson became the first computer to beat a human on the famous quiz show.\\', \\'“In a way, Watson winning\\\\xa0Jeopardy!\\\\xa0seemed unfair to people,” says Jeehwan Kim, the Class ‘47 Career Development Professor and a faculty member of the MIT departments of Mechanical Engineering and Materials Science and Engineering. “At the time, Watson was connected to a super computer the size of a room while the human brain is just a few pounds. But the ability to replicate a human brain’s ability to learn is incredibly difficult.”\\', \\'Kim specializes in machine learning, which relies on algorithms to teach computers how to learn like a human brain. “Machine learning is cognitive computing,” he explains. “Your computer recognizes things without you telling the computer what it’s looking at.”\\', \\'Machine learning is one example of artificial intelligence in practice. While the phrase “machine learning” often conjures up science fiction typified in shows like \"Westworld\" or \"Battlestar Galactica,\" smart systems and devices are already pervasive in the fabric of our daily lives. Computers and phones use face recognition to unlock. Systems sense and adjust the temperature in our homes. Devices answer questions or play our favorite music on demand. Nearly every major car company has entered the race to develop a safe self-driving car.\\', \\'For any of these products to work, the software and hardware both have to work in perfect synchrony. Cameras, tactile sensors, radar, and light detection all need to function properly to feed information back to computers. Algorithms need to be designed so these machines can process these sensory data and make decisions based on the highest probability of success.\\', \\'Kim and the much of the faculty at MIT’s Department of Mechanical Engineering are creating new software that connects with hardware to create intelligent devices. Rather than building the sentient robots romanticized in popular culture, these researchers are working on projects that improve everyday life and make humans safer, more efficient, and better informed.\\', \\'Making portable devices smarter\\', \\'Jeehwan Kim holds up sheet of paper. If he and his team are successful, one day the power of a super computer like IBM’s Watson will be shrunk down to the size of one sheet of paper. “We are trying to build an actual physical neural network on a letter paper size,” explains Kim.\\', \\'To date, most neural networks have been software-based and made using the conventional method known as the Von Neumann computing method. Kim however has been using neuromorphic computing methods.\\', \\'“Neuromorphic computer means portable AI,” says Kim. “So, you build artificial neurons and synapses on a small-scale wafer.” The result is a so-called ‘brain-on-a-chip.’\\\\nRather than compute information from binary signaling, Kim’s neural network processes information like an analog device. Signals act like artificial neurons and move across thousands of arrays to particular cross points, which function like synapses. With thousands of arrays connected, vast amounts of information could be processed at once. For the first time, a portable piece of equipment could mimic the processing power of the brain.\\', \\'“The key with this method is you really need to control the artificial synapses well. When you’re talking about thousands of cross points, this poses challenges,” says Kim.\\', \\'According to Kim, the design and materials that have been used to make these artificial synapses thus far have been less than ideal. The amorphous materials used in neuromorphic chips make it incredibly difficult to control the ions once voltage is applied.\\', \\'In a\\\\xa0Nature Materials\\\\xa0study published earlier this year, Kim found that when his team made a chip out of silicon germanium they were able to control the current flowing out of the synapse and reduce variability to 1 percent. With control over how the synapses react to stimuli, it was time to put their chip to the test.\\', \\'“We envision that if we build up the actual neural network with material we can actually do handwriting recognition,” says Kim. In a computer simulation of their new artificial neural network design, they provided thousands of handwriting samples. Their neural network was able to accurately recognize 95 percent of the samples.\\', \\'“If you have a camera and an algorithm for the handwriting data set connected to our neural network, you can achieve handwriting recognition,” explains Kim.\\', \\'While building the physical neural network for handwriting recognition is the next step for Kim’s team, the potential of this new technology goes beyond handwriting recognition. “Shrinking the power of a super computer down to a portable size could revolutionize the products we use,” says Kim. “The potential is limitless – we can integrate this technology in our phones, computers, and robots to make them substantially smarter.”\\', \\'Making homes smarter\\', \\'While Kim is working on making our portable products more intelligent, Professor Sanjay Sarma and Research Scientist Josh Siegel hope to integrate smart devices within the biggest product we own: our homes.\\', \\'One evening, Sarma was in his home when one of his circuit breakers kept going off. This circuit breaker — known as an arc-fault circuit interrupter (AFCI) — was designed to shut off power when an electric arc is detected to prevent fires. While AFCIs are great at preventing fires, in Sarma’s case there didn’t seem to be an issue. “There was no discernible reason for it to keep going off,” recalls Sarma. “It was incredibly distracting.”\\', \"AFCIs are notorious for such ‘nuisance trips,’ which disconnect safe objects unnecessarily. Sarma, who also serves as MIT\\'s vice president for open learning, turned his frustration into opportunity. If he could embed the AFCI with smart technologies and connect it to the ‘internet of things,’ he could teach the circuit breaker to learn when a product is safe or when a product actually poses a fire risk.\\\\n“Think of it like a virus scanner,” explains Siegel. “Virus scanners are connected to a system that updates them with new virus definitions over time.” If Sarma and Siegel could embed similar technology into AFCIs, the circuit breakers could detect exactly what product is being plugged in and learn new object definitions over time.\", \\'If, for example, a new vacuum cleaner is plugged into the circuit breaker and the power shuts off without reason, the smart AFCI can learn that it’s safe and add it to a list of known safe objects. The AFCI learns these definitions with the aid of a neural network. But, unlike Jeewhan Kim’s physical neural network, this network is software-based.\\', \\'The neural network is built by gathering thousands of data points during simulations of arcing. Algorithms are then written to help the network assess its environment, recognize patterns, and make decisions based on the probability of achieving the desired outcome. With the help of a $35 microcomputer and a sound card, the team can cheaply integrate this technology into circuit breakers.\\', \"As the smart AFCI learns about the devices it encounters, it can simultaneously distribute its knowledge and definitions to every other home using the internet of things.\\\\n“Internet of things could just as well be called \\'intelligence of things,” says Sarma. “Smart, local technologies with the aid of the cloud can make our environments adaptive and the user experience seamless.”\", \\'Circuit breakers are just one of many ways neural networks can be used to make homes smarter. This kind of technology can control the temperature of your house, detect when there’s an anomaly such as an intrusion or burst pipe, and run diagnostics to see when things are in need of repair.\\', \\'“We’re developing software for monitoring mechanical systems that’s self-learned,” explains Siegel. “You don’t teach these devices all the rules, you teach them how to learn the rules.”\\', \\'Making manufacturing and design smarter\\\\nArtificial intelligence can not only help improve how users interact with products, devices, and environments. It can also improve the efficiency with which objects are made by optimizing the manufacturing and design process.\\', \\'“Growth in automation along with complementary technologies including 3-D printing, AI, and machine learning compels us to, in the long run, rethink how we design factories and supply chains,” says Associate Professor A. John Hart.\\', \\'Hart, who has done extensive research in 3-D printing, sees AI as a way to improve quality assurance in manufacturing. 3-D printers incorporating high-performance sensors, that are capable of analyzing data on the fly, will help accelerate the adoption of 3-D printing for mass production.\\', \\'“Having 3-D printers that learn how to create parts with fewer defects and inspect parts as they make them will be a really big deal — especially when the products you’re making have critical properties such as medical devices or parts for aircraft engines,” Hart explains.\\', \\'The very process of designing the structure of these parts can also benefit from intelligent software. Associate Professor Maria Yang has been looking at how designers can use automation tools to design more efficiently. “We call it hybrid intelligence for design,” says Yang. “The goal is to enable effective collaboration between intelligent tools and human designers.”\\', \\'In a recent study, Yang and graduate student Edward Burnell tested a design tool with varying levels of automation. Participants used the software to pick nodes for a 2-D truss of either a stop sign or a bridge. The tool would then automatically come up with optimized solutions based on intelligent algorithms for where to connect nodes and the width of each part.\\', \\'“We’re trying to design smart algorithms that fit with the ways designers already think,” says Burnell.\\', \\'Making robots smarter\\', \\'If there is anything on MIT’s campus that most closely resembles the futuristic robots of science fiction, it would be Professor Sangbae Kim’s robotic cheetah. The four-legged creature senses its surrounding environment using LIDAR technologies and moves in response to this information. Much like its namesake, it can run and leap over obstacles.\\', \\'Kim’s primary focus is on navigation. “We are building a very unique system specially designed for dynamic movement of the robot,” explains Kim. “I believe it is going to reshape the interactive robots in the world. You can think of all kinds of applications — medical, health care, factories.”\\', \\'Kim sees opportunity to eventually connect his research with the physical neural network his colleague Jeewhan Kim is working on. “If you want the cheetah to recognize people, voice, or gestures, you need a lot of learning and processing,” he says. “Jeewhan’s neural network hardware could possibly enable that someday.”\\', \\'Combining the power of a portable neural network with a robot capable of skillfully navigating its surroundings could open up a new world of possibilities for human and AI interaction. This is just one example of how researchers in mechanical engineering can one-day collaborate to bring AI research to next level.\\', \\'While we may be decades away from interacting with intelligent robots, artificial intelligence and machine learning has already found its way into our routines. Whether it’s using face and handwriting recognition to protect our information, tapping into the internet of things to keep our homes safe, or helping engineers build and design more efficiently, the benefits of AI technologies are pervasive.\\', \\'The science fiction fantasy of a world overtaken by robots is far from the truth. “There’s this romantic notion that everything is going to be automatic,” adds Maria Yang. “But I think the reality is you’re going to have tools that will work with people and help make their daily life a bit easier.”\\'] #[\\'To demystify artificial intelligence (AI) and unlock its benefits, the MIT Quest for Intelligence created the Quest Bridge to bring new intelligence tools and ideas into classrooms, labs, and homes. This spring, more than a dozen\\\\xa0Undergraduate Research Opportunities Program\\\\xa0(UROP) students joined the project in its mission to make AI accessible to all. Undergraduates worked on applications designed to teach kids about AI, improve access to AI programs and infrastructure, and harness AI to improve literacy and mental health. Six projects are highlighted here.\\', \\'Project Athena for cloud computing\\', \\'Training an AI model often requires remote servers to handle the heavy number-crunching, but getting projects to the cloud and back is no trivial matter. To simplify the process, an undergraduate club called the\\\\xa0MIT Machine Intelligence Community\\\\xa0(MIC) is building an interface modeled after MIT’s\\\\xa0Project Athena, which brought desktop computing to campus in the 1980s.\\', \\'Amanda Li stumbled on the MIC during orientation last fall. She was looking for computer power to train an AI language model she had built to identify the nationality of non-native English speakers. The club had a bank of cloud credits, she learned, but no practical system for giving them away. A plan to build such a system, tentatively named “Monkey,” quickly took shape.\\', \\'The system would have to send a student’s training data and AI model to the cloud, put the project in a queue, train the model, and send the finished project back to MIT. It would also have to track individual usage to make sure cloud credits were evenly distributed.\\', \\'This spring, Monkey became a UROP project, and Li and sophomore Sebastian Rodriguez continued to work on it under the guidance of the Quest Bridge. So far, the students have created four modules in GitHub that will eventually become the foundation for a distributed system.\\', \\'“The coding isn’t the difficult part,” says Li. “It’s the exploring the server side of machine learning — Docker, Google Cloud, and the API. The most important thing I’ve learned is how to efficiently design and pipeline a project as big as this.”\\', \\'A launch is expected\\\\xa0sometime next year.\\\\xa0“This is a huge project, with some timely problems that industry is also trying to address,” says Quest Bridge AI engineer Steven Shriver, who is supervising the project. “I have no doubt the students will figure it out: I’m here to help when they need it.”\\', \\'An easy-to-use AI program for segmenting images\\', \\'The ability to divide an image into its component parts underlies more complicated AI tasks like picking out proteins in pictures of microscopic cells, or stress fractures in shattered materials. Although fundamental, image segmentation programs are still hard for non-engineers to navigate. In a project with the Quest Bridge, first-year Marco Fleming helped to build a Jupyter notebook for image segmentation, part of the Quest Bridge’s broader mission to develop a set of AI building blocks that researchers can tailor for specific applications.\\', \\'Fleming came to the project with self-taught coding skills, but no experience with machine learning, GitHub, or using a command-line interface. Working with Katherine Gallagher, an AI engineer with the Quest Bridge, and a more experienced classmate, Sule Kahraman, Fleming became fluent in convolutional neural networks, the workhorse for many machine vision tasks. “It’s kind of weird,” he explains. “You take a picture and do a lot of math to it, and the machine learns where the edges are.” Bound for a summer internship at Allstate this summer, Fleming says the project gave him a confidence boost.\\', \\'His participation also benefitted the Quest Bridge, says Gallagher. “We’re developing these notebooks for people like Marco, a freshman with no machine learning experience. Seeing where Marco got tripped up was really valuable.”\\', \\'An automated image classifier: no coding required\\', \\'Anyone can build apps that impact the world. That’s the motto of the\\\\xa0MIT AppInventor, a programming environment founded by\\\\xa0Hal Abelson, the Class of 1922 Professor in MIT’s\\\\xa0Department of Electrical Engineering and Computer Science. Working in Abelson’s lab over Independent Activity Period, sophomore Yuria Utsumi developed a web interface that lets anyone build a deep learning classifier to sort pictures of, say, happy faces and sad faces, or apples and oranges.\\', \\'In four steps, the\\\\xa0Image Classification Explorer\\\\xa0lets users label and upload their images to the web, select a customizable model, add testing data, and see the results. Utsumi built the app with a pre-trained classifier that she restructured to learn from a set of new and unfamiliar images. Once users retrain the classifier on the new images, they can upload the model to AppInventor to view it on their smartphones.\\', \\'In a recent test run of the Explorer app, students at Boston Latin Academy uploaded selfies shot on their laptop webcams and classified their facial expressions. For Utsumi, who picked the project hoping to gain practical web development and programming skills, it was a moment of triumph. “This is the first time I’m solving an algorithms problem in real life!” she says.\\\\xa0“It was fun to see the students become more comfortable with machine learning,” she adds. “I’m excited to help expand the platform to teach more concepts.”\\', \\'Introducing kids to machine-generated art\\', \\'One of the hottest trends in AI is a new method for creating computer-generated art using generative adversarial networks, or GANs. A pair of neural networks work together to create a photorealistic image while letting the artist add their unique twist. One AI program called\\\\xa0GANpaint, developed in the lab of MIT Quest for Intelligence Director\\\\xa0Antonio Torralba, lets users add trees, clouds, and doors, among other features, to a set of pre-drawn images.\\', \\'In a project with the Quest Bridge, sophomore Maya Nigrin is helping to adapt GANpaint to the popular coding platform for kids,\\\\xa0Scratch. The work involves training a new GAN on pictures of castles and developing custom Scratch extensions to integrate GANpaint with Scratch. The students are also developing Jupyter notebooks to teach others how to think critically about GANs as the technology makes it easier to make and share doctored images.\\', \\'A former babysitter and piano teacher who now tutors middle and high school students in computer science, Nigrin says she picked the project for its emphasis on K-12 education.\\\\xa0Asked for the most important takeaway, she says: “If you can’t solve the problem, go around it.”\\', \\'Learning to problem-solve is a key skill for any software engineer, says Gallagher, who supervised the project. “It can be challenging,” she says, “but that’s part of the fun. The students\\\\xa0will hopefully\\\\xa0come away with a realistic sense of what software development entails.”\\', \\'A robot that lifts you up when you’re feeling blue\\', \\'Anxiety and depression are on the rise as more of our time is spent staring at screens. But if technology is the problem, it might also be the answer, according to\\\\xa0Cynthia Breazeal, an associate professor of media arts and sciences at the\\\\xa0MIT Media Lab.\\', \\'In a new project, Breazeal is rebooting her home robot Jibo as a personal wellness coach. (The MIT spinoff that commercialized Jibo closed last fall, but MIT has a license to use Jibo for applied research). MIT junior Kika Arias spent the last semester helped to design interactions for Jibo to read and respond to people’s moods with personalized bits of advice. If Jibo senses you’re down, for example, it might suggest a “wellness” chat and some positive psychology exercises, like writing down something you feel grateful for.\\', \\'Jibo the wellness coach will face its first test in a pilot study with MIT students this summer. To get it ready, Arias designed and assembled what she calls a “glorified robot chair,” a portable mount for Jibo and its suite of instruments: a camera, microphone, computer, and tablet. She has translated scripts written for Jibo by a human life coach into his playful but laid-back voice. And she has made a widely used scale for self-reported emotions, which study participants will use to rate their mood, more engaging.\\', \\'“I’m not a hardcore machine learning, cloud-computing type, but I’ve discovered I’m capable of a lot more than I thought,” she says. “I’ve always felt a strong desire to help people, so when I found this lab, I thought this is exactly where I’m supposed to be.”\\', \\'A storytelling robot that helps kids learn to read\\', \\'Kids who are read-to aloud tend to pick up reading easier, but not all parents themselves know how to read or have time to regularly read stories to their children. What if a home robot could fill in, or even promote higher-quality parent-child reading time?\\', \\'In the first phase of a larger project, researchers in Breazeal’s lab are recording parents as they read aloud to their children, and are analyzing video, audio, and physiological data from the reading sessions. “These interactions play a big\\\\xa0role in a child’s literacy later in life,” says first-year student Shreya Pandit, who worked on the project this semester.\\\\xa0“There’s a sharing of emotion, and exchange of questions and answers during the telling of the story.”\\', \\'These sidebar conversations are critical for learning, says Breazeal. Ideally, the robot is there to strengthen the parent-child bond and provide helpful prompts for both parent and child.\\', \\'To understand how a robot can augment learning,\\\\xa0Pandit has helped to develop parent surveys, run behavioral experiments, analyze data, and integrate multiple data streams. One surprise, she says, has been learning how much work is self-directed: She looks for a problem, researches solutions, and runs them by others in the lab before picking one — for example, an algorithm for splitting audio files based on who’s speaking, or a way of scoring the complexity of the stories being read aloud.\\', \\'“I try to set goals for myself and report something back after each session,” she says. “It’s cool to look at this data and try to figure out what it can tell us about improving literacy.”\\', \\'These Quest for Intelligence UROP projects were funded by\\\\xa0Eric Schmidt, technical adviser to Alphabet Inc., and his wife, Wendy.\\'] #[\\'When Armando Solar-Lezama was a third grader in Mexico City, his science class did a unit on electrical circuits. The students were divided into teams of three, and each team member had to bring in a light bulb, a battery, or a switch.\\', \\'Solar-Lezama, whose father worked for an electronics company, volunteered to provide the switch. Using electrical components his father had brought home from work, Solar-Lezama built a “flip-flop” circuit and attached it to a touch-sensitive field effect transistor. When the circuit was off, touching the transistor turned it on, and when it was on, touching the transistor turned it off. “I was pretty proud of my circuit,” says Solar-Lezama, now an MIT professor of electrical engineering and computer science.\\', \\'By the time he got to school, however, one of his soldered connections had come loose, and the circuit’s performance was erratic. “They failed the whole group,” Solar-Lezama says. “And everybody was like, ‘Why couldn’t you just go to the store and get a switch like normal people do?’”\\', \\'The next year, in an introductory computer science class, Solar-Lezama was assigned to write a simple program that would send a few lines of text to a printer. Instead, he wrote a program that asked the user a series of questions, each question predicated on the response to the one before. The answer to the final question determined the text that would be sent to the printer.\\', \\'This time, the program worked perfectly. But “the teacher failed me because that’s not what the assignment was supposed to be,” Solar-Lezama says. “The educational system was not particularly flexible.”\\', \\'At that point, Solar-Lezama abandoned trying to import his extracurricular interests into the classroom. “I sort of brushed it off,” he recalls. “I was doing my own thing. As long as school didn’t take too much of my time, it was fine.”\\', \\'So, in 1997, when Solar-Lezama’s father moved the family to College Station, Texas — the Mexican economy was still in the throes of the three-year-old Mexican peso crisis — the 15-year-old Armando began to teach himself calculus and linear algebra.\\', \\'Accustomed to the autonomy of living in a huge city with a subway he could take anywhere, Solar-Lezama bridled at having to depend on rides from his parents to so much as go to the library. “For the first three years that I was in Texas, I was convinced that as soon as I turned 18, I was going to go back to Mexico,” he says. “Because what was I doing in this place in the middle of nowhere?” He began systematically educating himself in everything he would need to ace the Mexican college entrance exams.\\', \\'At his Texan high school, however, he was placed by default in the lowest of the school’s three academic tracks, which is where most immigrants with imperfect English found themselves. Against the recommendations of the school administrators, he insisted on taking physics; within two weeks, his physics teacher had moved him up to a higher-level class.\\', \\'By his junior year, Solar-Lezama was enrolled in the most demanding math and science classes the school offered, in most of which his classmates were seniors. But in the humanities, where he still struggled with the language — and, he admits, his own lack of interest — he remained on the lower track.\\', \\'“In the time I was there, I got to move from one track to the other,” Solar-Lezama says. “It was really shocking to realize how different these tracks were.”\\', \\'Outside the classroom, Solar-Lezama was a member of a team that finished second in the nation in the Department of Energy’s Science Bowl competition. He also won a regional science fair held at Texas A&M with a computer simulation he’d whipped up in an afternoon, when he and some friends realized that they wouldn’t be able to get a scrap-built hovercraft working by the fair deadline. And he started working for a local software startup, doing database coding.\\', \\'But inside the classroom, “my record was very bimodal,” he says. Though he excelled in math and science, he ended his senior year ranked only about 100th in a class of 400.\\', \\'Still, he decided to put his return to Mexico on hold. “By the time I was a senior in high school, I sort of found my place,” he says. “I was learning lots of things that I was interested in, and I decided that, ‘Okay, maybe I’ll stay here for college, and then I’ll go back.’”\\', \\'His spotty academic performance, however, was an obstacle. MIT was one of several universities that denied him undergraduate admission. But the father of one of his Science Bowl teammates taught nuclear engineering at Texas A&M and, recognizing Solar-Lezama’s talent, encouraged him to apply for a generous scholarship offered through the department.\\', \\'To ensure that international students could navigate the transition to a new educational system and, often, a new language, the university restricted the number of units they could carry as freshmen, and Solar-Lezama, his three years of American high school notwithstanding, counted as an international student. So to keep himself busy, he audited several courses outside the nuclear-engineering curriculum.\\', \\'One of these was Introduction to Algorithms. Although he wasn’t formally enrolled at the time, Solar-Lezama did all the homework and took all the exams, and he ended up with the highest grade in the class.\\', \\'“Before that point, I thought of programming as a useful skill,” Solar-Lezama says. “One of the things that really excited me about this class was that you could prove things about algorithms and get some guarantees about how something is going to work, and I found that extremely appealing. So I decided to switch majors to computer science.”\\', \\'Graduating in three years, Solar-Lezama decided to postpone his return to Mexico a little longer, applying to graduate programs at MIT, Carnegie Mellon University, and the University of California at Berkeley. “I thought, if I don’t get in to any of them, fine, I’ll go back to Mexico,” he says. Once again, MIT turned him down, as did CMU. But he got into Berkeley.\\', \\'Solar-Lezama arrived at Berkeley planning to continue his work on large parallel computing systems, but his conversations with his advisor, Ras Bodik, quickly took a different turn. Different types of simulations generally required different computational strategies. But implementing those strategies often required reshuffling the same low-level processes. Was it possible, Bodik and Solar-Lezama wondered, to devise a way to formulate the strategies broadly and automate the reshuffling?\\', \\'Solar-Lezama thus found himself part of a small community of researchers working on “program synthesis,” or the automatic generation of computer programs. His \\\\xa0thesis project was a language called Sketch, which lets programmers describe program functionality in general terms and automatically fills in the computational details.\\', \\'Sketch treats program synthesis as a search problem: The task is to search the space of all possible programs for one that can meet the requirements imposed by the general description. The chief innovation behind Sketch was a set of algorithms for rapidly paring down the search space, so that a satisfactory program could be found in real time.\\', \\'“There were three or four of us who were pushing this area and telling everybody who would listen that this was the right direction for programming systems research, and for a long time there was a lot of hostility toward these kinds of ideas,” Solar-Lezama says. “Little by little, we started converting a few more people, and all of a sudden they reached a critical mass, and now it’s an extremely active area of research.”\\', \\'After graduating from Berkeley, Solar-Lezama went on the job market, and MIT finally made him an offer. In his seven years at the Institute, where he recently earned tenure, Sketch has remained the foundation of his research, which has developed along three parallel tracks.\\', \\'The first track is the extension of Sketch, so that it can handle more diverse and complex computations. The second is the application of Sketch and its underlying machinery to particular problems — such as orienting new members of large programming teams toward the existing code base, automatically grading programming homework, and parallelizing code for faster execution on multicore chips.\\', \\'Recently, Solar-Lezama’s group has also begun investigating the application of program synthesis to machine learning. Machine learning involves teaching a computer system to perform some classification task by presenting it with training examples. But suppose that the training data consists of a row of three squares and a row of three circles. Which image belongs to the same class, a row of three stars or four circles arranged in a square?\\', \\'Existing machine-learning systems are good at learning to recognize circles from examples of circles, but they’re not as good at the kind of abstract pattern matching that humans do intuitively. A program synthesizer, however, is much more likely to converge on a program for producing three-object rows than one that sometimes produces rows and sometimes produces squares.\\', \\'Having finally made it to city with a good subway system, Solar-Lezama no longer has any plans to move back to Mexico. His wife has a Mexican father and spent much of her childhood in Mexico, but her mother is from Minnesota, and she had planned on settling in the U.S. when she and Solar-Lezama met in Berkeley. Their children, ages 6 and 3, might also find it hard to adjust to life in Mexico. Although they speak Spanish exclusively at home, they speak English at their school in Medford, Massachusetts, and, says Solar-Lezama, “they’re developing a Boston accent.”\\'] #[\\'How will advances in computing transform human society?\\', \\'MIT students contemplated this impending question as part of the Envisioning the Future of Computing Prize — an essay contest in which they were challenged to imagine ways that computing technologies could improve our lives, as well as the pitfalls and dangers associated with them.\\', \\'Offered for the first time this year, the Institute-wide competition invited MIT undergraduate and graduate students to share their ideas, aspirations, and vision for what they think a future propelled by advancements in computing holds. Nearly 60 students put pen to paper, including those majoring in mathematics, philosophy, electrical engineering and computer science, brain and cognitive sciences, chemical engineering, urban studies and planning, and management, and entered their submissions.\\', \\'Students dreamed up highly inventive scenarios for how the technologies of today and tomorrow could impact society, for better or worse. Some recurring themes emerged, such as tackling issues in climate change and health care. Others proposed ideas for particular technologies that ranged from digital twins as a tool for navigating the deluge of information online to a cutting-edge platform powered by artificial intelligence, machine learning, and biosensors to create personalized storytelling films that help individuals understand themselves and others.\\', \\'Conceived of by the Social and Ethical Responsibilities of Computing (SERC), a cross-cutting initiative of the MIT Schwarzman College of Computing in collaboration with the School of Humanities, Arts, and Social Sciences (SHASS), the intent of the competition was “to create a space for students to think in a creative, informed, and rigorous way about the societal benefits and costs of the technologies they are or will be developing,” says Caspar Hare, professor of philosophy, co-associate dean of SERC, and the lead organizer of the Envisioning the Future of Computing Prize. “We also wanted to convey that MIT values such thinking.”\\', \\'Prize winners\\', \\'The contest implemented a two-stage evaluation process wherein all essays were reviewed anonymously by a panel of MIT faculty members from the college and SHASS for the initial round. Three qualifiers were then invited to present their entries at an awards ceremony on May 8, followed by a Q&A with a judging panel and live in-person audience for the final round.\\', \"The winning entry was awarded to Robert Cunningham \\'23, a recent graduate in math and physics, for his paper on the implications of a personalized language model that is fine-tuned to predict an individual’s writing based on their past texts and emails. Told from the perspective of three fictional characters: Laura, founder of the tech startup ScribeAI, and Margaret and Vincent, a couple in college who are frequent users of the platform, readers gained insights into the societal shifts that take place and the unforeseen repercussions of the technology.\", \\'Cunningham, who took home the grand prize of $10,000, says he came up with the concept for his essay in late January while thinking about the upcoming release of GPT-4 and how it might be applied. Created by the developers of ChatGPT — an AI chatbot that has managed to capture popular imagination for its capacity to imitate human-like text, images, audio, and code — GPT-4, which was unveiled in March, is the newest version of OpenAI’s language model systems.\\', \"“GPT-4 is wild in reality, but some rumors before it launched were even wilder, and I had a few long\\\\xa0plane rides to\\\\xa0think about them! I enjoyed this opportunity to solidify a vague notion into a piece of writing, and since some of my favorite works of science fiction are short stories, I figured I\\'d take the chance to write one,” Cunningham says.\", \"The other two finalists, awarded $5,000 each, included Gabrielle Kaili-May Liu \\'23, a recent graduate in mathematics with computer science, and brain and cognitive sciences, for her entry on using the reinforcement learning with human feedback technique as a tool for transforming human interactions with AI; and Abigail Thwaites and Eliot Matthew Watkins, graduate students in the Department of Philosophy and Linguistics, for their joint submission on automatic fact checkers, an AI-driven software that they argue could potentially help mitigate the spread of misinformation and be a profound social good.\", \\'“We were so excited to see the amazing response to this contest. It made clear how much students at MIT, contrary to stereotype, really care about the wider implications of technology, says Daniel Jackson, professor of computer science and one of the final-round judges. “So many of the essays were incredibly thoughtful and creative. Robert’s story was a chilling, but entirely plausible take on our AI future; Abigail and Eliot’s analysis brought new clarity to what harms misinformation actually causes; and Gabrielle’s piece gave a lucid overview of a prominent new technology. I hope we’ll be able to run this contest every year, and that it will encourage all our students to broaden their perspectives even further.”\\', \\'Fellow judge Graham Jones, professor of anthropology, adds: “The winning entries reflected the incredible breadth of our students’ engagement with socially responsible computing. They challenge us to think differently about how to design computational technologies, conceptualize social impacts, and imagine future scenarios. Working with a cross-disciplinary panel of judges catalyzed lots of new conversations. As a sci-fi fan, I was thrilled that the top prize went to a such a stunning piece of speculative fiction!”\\', \\'Other judges on the panel for the final round included:\\', \\'Honorable mentions\\', \\'In addition to the grand prize winner and runners up, 12 students were recognized with honorable mentions for their entries, with each receiving $500.\\', \\'The honorees and the title of their essays include:\\', \\'The Envisioning the Future of Computing Prize was supported by MAC3 Impact Philanthropies.\\'] #[\\'With a box of popcorn in one hand, Hal Abelson, a renowned computer scientist, strolled through the first floor of the Ray and Maria Stata Center studying the machine learning exhibits that surrounded him on the afternoon of Feb. 26. Everywhere he looked he saw evidence of the remarkable things MIT students can do when given access to computing resources.\\', \\'“Computing tools and infrastructure have gotten to a place where students can outperform professional researchers. You are constrained mostly by your imagination. It’s just an amazing time,” said Abelson, the Class of 1922 Professor of Computer Science and Engineering.\\', \\'Abelson, and a crowd of hundreds, was witnessing the kickoff of a three-day celebration of the MIT Stephen A. Schwarzman College of Computing. The afternoon event was an exposition of projects that transformed the student street lobby area of the Stata Center into a computing fairground of sorts, replete with courtesy popcorn, bubble tea, lemon squares, brownies, celebratory stickers, and a host of student exhibits that crossed disciplines, broke barriers, and inspired new thinking.\\', \\'For Kadeem Khan, a graduate student in urban studies and planning and an expo participant, the day was special. “I wanted to do a project focused on machine learning and the developing world,” he said. Khan applied machine learning to generate useful insights on poverty in Nairobi by analyzing data from multiple sources, including census, satellite imagery, and data from a geographic information system.\\', \\'“The poverty exhibit is an example of what I was just saying,” said Abelson. “Somehow the resources are here now to allow students to bring things to the next level.” Abeslson and Nicholas Roy, a professor of aeronautics and astronautics, CSAIL researcher, and director of the Bridge in the Quest for Intelligence, helped judge the teams during the monthlong student computing challenges leading up to yesterday.\\', \"Like Khan, MIT electrical engineering and computer science graduate student Natalie Lao embarked on a winning project with the potential to make transformative change in the world. “My background is in AI — but I\\'m also very interested in ethics and fairness and the risks involved when applying AI to the real world,” she said. Her team’s project uses network propagation and analysis to automatically discover and potentially halt the spread of fake news across a variety of media platforms. “We’re talking to the Department of Defense and various companies and trying to see how we can get the solution out in the world,” she said.\", \\'The MIT Schwarzman College of Computing, which represents a $1 billion commitment to addressing the global opportunities and challenges presented by the prevalence of computing and the rise of artificial intelligence, will provide students with unprecedented computing resources, including access to large data sets and the tools to learn from them. Yesterday, top entrants spoke in excited tones about the data sets they accessed during the Machine Learning Across Disciplines Challenge, which, along with the Connect Arts, Community, and Computing Challenge, was funded by the MIT-IBM Watson AI Lab.\\', \\'Graduate students Agni Orfanoudaki and Antonin Dauvin, who are both studying operations research at the MIT Sloan School of Management, applied machine learning and techniques developed at MIT Operations Research Center to patient data from Boston Medical Center spanning two decades. They are developing an analytic approach to understanding the impact of different anti-hypertensive drugs.\\', \\'Senior Sarah Wooders, an undergraduate in math and computer science, has collected a dataset of over 4 million product images and descriptions scraped from online sources. She then trained models to collectively label over 90 important clothing attributes and is is now building a system that can automatically label new clothing products. “It’s really exciting to see all the applications of AI,” said Wooders, also a top entrant. “My project feels like such an obvious idea but this type of system hasn’t been created yet. It seems the same thing is true for a lot of things in AI right now. And so someone like me can come along and do it.”\\'] #[\\'Ask a smart home device for the weather forecast, and it takes several seconds for the device to respond. One reason this latency occurs is because connected devices don’t have enough memory or power to store and run the enormous machine-learning models needed for the device to understand what a user is asking of it. The model is stored in a data center that may be hundreds of miles away, where the answer is computed and sent to the device.\\', \\'\\', \\'MIT researchers have created a new method for computing directly on these devices, which drastically reduces this latency. Their technique shifts the memory-intensive steps of running a machine-learning model to a central server where components of the model are encoded onto light waves.\\', \\'\\', \\'The waves are transmitted to a connected device using fiber optics, which enables tons of data to be sent lightning-fast through a network. The receiver then employs a simple optical device that rapidly performs computations using the parts of a model carried by those light waves.\\', \\'\\', \\'This technique leads to more than a hundredfold improvement in energy efficiency when compared to other methods. It could also improve security, since a user’s data do not need to be transferred to a central location for computation.\\', \\'\\', \\'This method could enable a self-driving car to make decisions in real-time while using just a tiny percentage of the energy currently required by power-hungry computers. It could also allow a user to have a latency-free conversation with their smart home device, be used for live video processing over cellular networks, or even enable high-speed image classification on a spacecraft millions of miles from Earth.\\', \\'\\', \\'“Every time you want to run a neural network, you have to run the program, and how fast you can run the program depends on how fast you can pipe the program in from memory. Our pipe is massive — it corresponds to sending a full feature-length movie over the internet every millisecond or so. That is how fast data comes into our system. And it can compute as fast as that,” says senior author Dirk Englund, an associate professor in the Department of Electrical Engineering and Computer Science (EECS) and member of the MIT Research Laboratory of Electronics.\\', \\'\\', \\'Joining Englund on the paper is lead author and EECS grad student Alexander Sludds; EECS grad student Saumil Bandyopadhyay, Research Scientist Ryan Hamerly, as well as others from MIT, the MIT Lincoln Laboratory, and Nokia Corporation. The research is published today in Science.\\', \\'\\', \\'Lightening the load\\', \\'\\', \\'Neural networks are machine-learning models that use layers of connected nodes, or neurons, to recognize patterns in datasets and perform tasks, like classifying images or recognizing speech. But these models can contain billions of weight parameters, which are numeric values that transform input data as they are processed. These weights must be stored in memory. At the same time, the data transformation process involves billions of algebraic computations, which require a great deal of power to perform.\\', \\'\\', \\'The process of fetching data (the weights of the neural network, in this case) from memory and moving them to the parts of a computer that do the actual computation is one of the biggest limiting factors to speed and energy efficiency, says Sludds.\\', \\'\\', \\'“So our thought was, why don’t we take all that heavy lifting — the process of fetching billions of weights from memory — move it away from the edge device and put it someplace where we have abundant access to power and memory, which gives us the ability to fetch those weights quickly?” he says.\\', \\'\\', \\'The neural network architecture they developed, Netcast, involves storing weights in a central server that is connected to a novel piece of hardware called a smart transceiver. This smart transceiver, a thumb-sized chip that can receive and transmit data, uses technology known as silicon photonics to fetch trillions of weights from memory each second.\\', \\'\\', \\'It receives weights as electrical signals and imprints them onto light waves. Since the weight data are encoded as bits (1s and 0s) the transceiver converts them by switching lasers; a laser is turned on for a 1 and off for a 0. It combines these light waves and then periodically transfers them through a fiber optic network so a client device doesn’t need to query the server to receive them.\\', \\'\\', \\'“Optics is great because there are many ways to carry data within optics. For instance, you can put data on different colors of light, and that enables a much higher data throughput and greater bandwidth than with electronics,” explains Bandyopadhyay.\\', \\'\\', \\'Trillions per second\\', \\'\\', \\'Once the light waves arrive at the client device, a simple optical component known as a broadband “Mach-Zehnder” modulator uses them to perform super-fast, analog computation. This involves encoding input data from the device, such as sensor information, onto the weights. Then it sends each individual wavelength to a receiver that detects the light and measures the result of the computation.\\', \\'\\', \\'The researchers devised a way to use this modulator to do trillions of multiplications per second, which vastly increases the speed of computation on the device while using only a tiny amount of power.\\', \\'\\', \\'“In order to make something faster, you need to make it more energy efficient. But there is a trade-off. We’ve built a system that can operate with about a milliwatt of power but still do trillions of multiplications per second. In terms of both speed and energy efficiency, that is a gain of orders of magnitude,” Sludds says.\\', \\'\\', \\'They tested this architecture by sending weights over an 86-kilometer fiber that connects their lab to MIT Lincoln Laboratory. Netcast enabled machine-learning with high accuracy — 98.7 percent for image classification and 98.8 percent for digit recognition — at rapid speeds.\\', \\'\\', \\'“We had to do some calibration, but I was surprised by how little work we had to do to achieve such high accuracy out of the box. We were able to get commercially relevant accuracy,” adds Hamerly.\\', \\'\\', \\'Moving forward, the researchers want to iterate on the smart transceiver chip to achieve even better performance. They also want to miniaturize the receiver, which is currently the size of a shoe box, down to the size of a single chip so it could fit onto a smart device like a cell phone.\\', \\'\\', \\'“Using photonics and light as a platform for computing is a really exciting area of research with potentially huge implications on the speed and efficiency of our information technology landscape,” says Euan Allen, a Royal Academy of Engineering Research Fellow at the University of Bath, who was not involved with this work. “The work of Sludds et al. is an exciting step toward seeing real-world implementations of such devices, introducing a new and practical edge-computing scheme whilst also exploring some of the fundamental limitations of computation at very low (single-photon) light levels.”\\', \\'\\', \\'The research is funded, in part, by NTT Research, the National Science Foundation, the Air Force Office of Scientific Research, the Air Force Research Laboratory, and the Army Research Office.\\'] #[\\'Traditional computer scientists and engineers are trained to develop solutions for specific needs, but aren’t always trained to consider their broader implications. Each new technology generation, and particularly the rise of artificial intelligence, leads to new kinds of systems, new ways of creating tools, and new forms of data, for which norms, rules, and laws frequently have yet to catch up. The kinds of impact that such innovations have in the world has often not been apparent until many years later.\\', \\'As part of the efforts in Social and Ethical Responsibilities of Computing (SERC) within the MIT Stephen A. Schwarzman College of Computing, a new case studies series examines social, ethical, and policy challenges of present-day efforts in computing with the aim of facilitating the development of responsible “habits of mind and action” for those who create and deploy computing technologies.\\', \\'“Advances in computing have undeniably changed much of how we live and work. Understanding and incorporating broader social context is becoming ever more critical,” says Daniel Huttenlocher, dean of the MIT Schwarzman College of Computing. “This case study series is designed to be a basis for discussions in the classroom and beyond, regarding social, ethical, economic, and other implications so that students and researchers can pursue the development of technology across domains in a holistic manner that addresses these important issues.”\\', \\'A modular system\\', \\'By design, the case studies are brief and modular to allow users to mix and match the content to fit a variety of pedagogical needs. Series editors David Kaiser and Julie Shah, who are the associate deans for SERC, structured the cases primarily to be appropriate for undergraduate instruction across a range of classes and fields of study.\\', \\'“Our goal was to provide a seamless way for instructors to integrate cases into an existing course or cluster several cases together to support a broader module within a course. They might also use the cases as a starting point to design new courses that focus squarely on themes of social and ethical responsibilities of computing,” says Kaiser, the Germeshausen Professor of the History of Science and professor of physics.\\', \\'Shah, an associate professor of aeronautics and astronautics and a roboticist who designs systems in which humans and machines operate side by side, expects that the cases will also be of interest to those outside of academia, including computing professionals, policy specialists, and general readers. In curating the series, Shah says that “we interpret ‘social and ethical responsibilities of computing’ broadly to focus on perspectives of people who are affected by various technologies, as well as focus on perspectives of designers and engineers.”\\', \\'The cases are not limited to a particular format and can take shape in various forms — from a magazine-like feature article or Socratic dialogues to choose-your-own-adventure stories or role-playing games grounded in empirical research. Each case study is brief, but includes accompanying notes and references to facilitate more in-depth exploration of a given topic. Multimedia projects will also be considered. “The main goal is to present important material — based on original research — in engaging ways to broad audiences of non-specialists,” says Kaiser.\\', \\'The SERC case studies are specially commissioned and written by scholars who conduct research centrally on the subject of the piece. Kaiser and Shah approached researchers from within MIT as well as from other academic institutions to bring in a mix of diverse voices on a spectrum of topics. Some cases focus on a particular technology or on trends across platforms, while others assess social, historical, philosophical, legal, and cultural facets that are relevant for thinking critically about current efforts in computing and data sciences.\\', \\'The cases published in the inaugural issue place readers in various settings that challenge them to consider the social and ethical implications of computing technologies, such as how social media services and surveillance tools are built; the racial disparities that can arise from deploying facial recognition technology in unregulated, real-world settings; the biases of risk prediction algorithms in the criminal justice system; and the politicization of data collection.\\', \\'\"Most of us agree that we want computing to work for social good, but which good? Whose good? Whose needs and values and worldviews are prioritized and whose are overlooked?” says Catherine D’Ignazio, an assistant professor of urban science and planning and director of the Data + Feminism Lab at MIT.\\', \\'D’Ignazio’s case for the series, co-authored with Lauren Klein, an associate professor in the English and Quantitative Theory and Methods departments at Emory University, introduces readers to the idea that while data are useful, they are not always neutral. “These case studies help us understand the unequal histories that shape our technological systems as well as study their disparate outcomes and effects. They are an exciting step towards holistic, sociotechnical thinking and making.\"\\', \\'Rigorously reviewed\\', \\'Kaiser and Shah formed an editorial board composed of 55 faculty members and senior researchers associated with 19 departments, labs, and centers at MIT, and instituted a rigorous peer-review policy model commonly adopted by specialized journals. Members of the editorial board will also help commission topics for new cases and help identify authors for a given topic.\\', \\'For each submission, the series editors collect four to six peer reviews, with reviewers mostly drawn from the editorial board. For each case, half the reviewers come from fields in computing and data sciences and half from fields in the humanities, arts, and social sciences, to ensure balance of topics and presentation within a given case study and across the series.\\', \\'“Over the past two decades I’ve become a\\\\xa0bit jaded when it comes to the academic review process, and so I was\\\\xa0particularly heartened to see such care and thought put into all of the reviews,\" says Hany Farid, a professor at the University of California at Berkeley with a joint appointment in the Department of Electrical Engineering and Computer Sciences and the School of Information. “The constructive review process made our case study significantly stronger.”\\', \\'Farid’s case, “The Dangers of Risk Prediction in the Criminal Justice System,” which he penned with Julia Dressel, recently a student of computer science at Dartmouth College, is one of the four commissioned pieces featured in the inaugural issue.\\', \\'Cases are additionally reviewed by undergraduate volunteers, who help the series editors gauge each submission for balance, accessibility for students in multiple fields of study, and possibilities for adoption in specific courses. The students also work with them to create original homework problems and active learning projects to accompany each case study, to further facilitate adoption of the original materials across a range of existing undergraduate subjects.\\', \"“I volunteered to work with this group because I believe that it\\'s incredibly important for those working in computer science to include thinking about ethics not as an afterthought, but integrated into every step and decision that is made, says Annie Snyder, a mathematical economics sophomore and a member of the MIT Schwarzman College of Computing’s Undergraduate Advisory Group. “While this is a massive issue to take on, this project is an amazing opportunity to start building an ethical culture amongst the incredibly talented students at MIT who will hopefully carry it forward into their own projects and workplace.”\", \\'New sets of case studies, produced with support from\\\\xa0the MIT Press’ Open Publishing Services program, will be published twice a year via the Knowledge Futures Group’s\\\\xa0PubPub\\\\xa0platform. The SERC case studies are made available for free on an open-access basis, under Creative Commons licensing terms. Authors retain copyright, enabling them to reuse and republish their work in more specialized scholarly publications.\\', \\'“It was important to us to approach this project in an inclusive way and lower the barrier for people to be able to access this content. These are complex issues that we need to deal with, and we hope that by making the cases widely available, more people will engage in social and ethical considerations as they’re studying and developing computing technologies,” says Shah.\\'] #[\\'“The Laughing Room,”\\\\xa0an interactive art installation by author, illustrator, and MIT graduate student Jonathan \"Jonny\" Sun, looks like a typical living room: couches, armchairs, coffee table, soft lighting. This cozy scene, however, sits in a glass-enclosed space, flanked by bright lights and a microphone, with a bank of laptops and a video camera positioned across the room. People wander in, take a seat, begin chatting. After a pause in the conversation, a riot of canned laughter rings out, prompting genuine giggles from the group.\\', \\'Presented at the Cambridge Public Library in Cambridge, Massachusetts, Nov. 16-18, \"The Laughing Room\" was an artificially intelligent room programmed to play an audio laugh track whenever participants said something that its algorithm deemed funny. Sun, who is currently on leave from his PhD program within the MIT Department of Urban Studies and Planning, is an affiliate at the Berkman Klein Center for Internet and Society at Harvard University, and creative researcher at the metaLAB at Harvard, created the project to explore the increasingly social and cultural roles of technology in public and private spaces, users’ agency within and dependence on such technology, and the issues of privacy raised by these systems. The installations were presented as part of ARTificial Intelligence, an ongoing program led by MIT associate professor of literature Stephanie Frampton that fosters public dialogue about the emerging ethical and social implications of artificial intelligence (AI) through art and design.\\']\\n\\n The user\\'s question: Can I buy a Toshiba laptop?'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Can I buy a Toshiba laptop?\"\n",
    "context = \" \".join([f\"#{str(i)}\" for i in results[\"documents\"][0]])\n",
    "#context = context[0:5120]\n",
    "prompt_template = f\"Relevant context: {context}\\n\\n The user's question: {question}\"\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c16327",
   "metadata": {},
   "source": [
    "Now all that remains is to send the prompt to the model and wait for its response!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0c378",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "lm_response = pipe(prompt_template)\n",
    "print(lm_response[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 836401,
     "sourceId": 1428159,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3496946,
     "sourceId": 6104553,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1977878,
     "sourceId": 7598394,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30527,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 489.921972,
   "end_time": "2024-02-21T16:01:21.828095",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-21T15:53:11.906123",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0249072f0f4d4cf28fb7947cf183fc2f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "071dc48299b84b408748c32846f30a91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "08f67a076c464789be8d398558cd75e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "09c8e6a194ed4324b327e6a87684e665": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0e64d4ec99324323b3b31f7d45dfcc4f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0eae314cc5144156a113eb72a4eaf34c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0f145d5b0b4244459f6596c9c502f8be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "100f3123c54549529f8de368e02d4235": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1f7b107efd5143009e27f4f4685d3f73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "20f9dafca84449cf9a1665899463dc44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_78046bc935e741c59b0444927ded134c",
       "placeholder": "​",
       "style": "IPY_MODEL_982d8ef8be744f928b21c6437b9498da",
       "value": "special_tokens_map.json: 100%"
      }
     },
     "297bea8d91534269a0c4d867460a7422": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f0994ffb8a54fbbbe3a418dbc91971f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0e64d4ec99324323b3b31f7d45dfcc4f",
       "placeholder": "​",
       "style": "IPY_MODEL_8991be26abf54b5caf8731fca42a8089",
       "value": " 450/450 [00:00&lt;00:00, 23.2kB/s]"
      }
     },
     "2ff3db9b7c8e4371a9b903ce362061fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3ede74746904448ca228a8befe4a4b65",
       "placeholder": "​",
       "style": "IPY_MODEL_08f67a076c464789be8d398558cd75e6",
       "value": " 819/819 [00:00&lt;00:00, 50.6kB/s]"
      }
     },
     "3118fe61249c4f98ab8d4b552f361f5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c4e8037a97b8409089cc35ce37527074",
       "max": 228,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d8f20332768c4410acca9acdd4637c29",
       "value": 228
      }
     },
     "326f9b73a7014e2fb29cd9fe1f5c6f1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_297bea8d91534269a0c4d867460a7422",
       "max": 5684548185,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b8d559bb30e94d82b46f7b58a0c024b4",
       "value": 5684548185
      }
     },
     "339e4065be8b4c95a920fb48731746f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "33f0869f1a984e73adcaa0c7aa3c57ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0249072f0f4d4cf28fb7947cf183fc2f",
       "max": 819,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_921feca6cb0e4a40b8e6cd98966b10db",
       "value": 819
      }
     },
     "3405eeefcfbe43ecb6b64b350ff9f8d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_20f9dafca84449cf9a1665899463dc44",
        "IPY_MODEL_3118fe61249c4f98ab8d4b552f361f5e",
        "IPY_MODEL_d0cd93a6e91e4d66acaa8bf7a2cb8271"
       ],
       "layout": "IPY_MODEL_de1a013ad9be46dfbf0a53ee19a29eb4"
      }
     },
     "38e262fc922f4643887db999b462d341": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3ede74746904448ca228a8befe4a4b65": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "40d353ff3e23425baf4a46e383004641": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "49f2a0d9255d4a3caeec87e69323e11e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f59d36bae75647b2a0a92f444ff760cb",
       "placeholder": "​",
       "style": "IPY_MODEL_718a693672a84637a77a8ad97a30801a",
       "value": "tokenizer.json: 100%"
      }
     },
     "5dd55f4a68ea442bb55ca3ec74c13a4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6825870d2df64e4a99f90d7975d6eb71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "718a693672a84637a77a8ad97a30801a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "78046bc935e741c59b0444927ded134c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "78a21c021d134602b778dc53bfb172ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_071dc48299b84b408748c32846f30a91",
       "placeholder": "​",
       "style": "IPY_MODEL_09c8e6a194ed4324b327e6a87684e665",
       "value": "pytorch_model.bin: 100%"
      }
     },
     "8136d0dafa85458f9c2035f20696b46a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "833bbebf57994a258c44e30179f001f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1f7b107efd5143009e27f4f4685d3f73",
       "max": 2114274,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9e71ef2f2ceb4a938bd2f9316ed65ce8",
       "value": 2114274
      }
     },
     "8991be26abf54b5caf8731fca42a8089": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8f80875c8064427eb49ea59c78c2bdea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a9ba697537434994834ad62d4f83d3ca",
        "IPY_MODEL_d577109efa6a4113adb8557757e57fb8",
        "IPY_MODEL_2f0994ffb8a54fbbbe3a418dbc91971f"
       ],
       "layout": "IPY_MODEL_daa56ebc4084424cb67a62810fee7d3a"
      }
     },
     "921feca6cb0e4a40b8e6cd98966b10db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "982d8ef8be744f928b21c6437b9498da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9e71ef2f2ceb4a938bd2f9316ed65ce8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a261d20add5749288618865ccfad3cd8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a49b72f0dce841dfa283b6627b331637": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e612cd0070d54d54a55644fdd4ed80ea",
        "IPY_MODEL_33f0869f1a984e73adcaa0c7aa3c57ec",
        "IPY_MODEL_2ff3db9b7c8e4371a9b903ce362061fa"
       ],
       "layout": "IPY_MODEL_e667e61c3b9547da98d28e8f894ad051"
      }
     },
     "a9ba697537434994834ad62d4f83d3ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_339e4065be8b4c95a920fb48731746f0",
       "placeholder": "​",
       "style": "IPY_MODEL_bcf374daf9d04159b576e522aa3e034a",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "b08805b629a54d939c18e69e11d51a30": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b8d559bb30e94d82b46f7b58a0c024b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bcf374daf9d04159b576e522aa3e034a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c4e8037a97b8409089cc35ce37527074": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c6f4a753acb14444b135012774856e3d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_49f2a0d9255d4a3caeec87e69323e11e",
        "IPY_MODEL_833bbebf57994a258c44e30179f001f0",
        "IPY_MODEL_d3f505b52a7342cfa7044d08e0f7916d"
       ],
       "layout": "IPY_MODEL_100f3123c54549529f8de368e02d4235"
      }
     },
     "d0cd93a6e91e4d66acaa8bf7a2cb8271": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0eae314cc5144156a113eb72a4eaf34c",
       "placeholder": "​",
       "style": "IPY_MODEL_38e262fc922f4643887db999b462d341",
       "value": " 228/228 [00:00&lt;00:00, 13.8kB/s]"
      }
     },
     "d3f505b52a7342cfa7044d08e0f7916d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0f145d5b0b4244459f6596c9c502f8be",
       "placeholder": "​",
       "style": "IPY_MODEL_5dd55f4a68ea442bb55ca3ec74c13a4e",
       "value": " 2.11M/2.11M [00:00&lt;00:00, 11.5MB/s]"
      }
     },
     "d577109efa6a4113adb8557757e57fb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dfcf5d456d3e4c01b7d66318e251806a",
       "max": 450,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8136d0dafa85458f9c2035f20696b46a",
       "value": 450
      }
     },
     "d8f20332768c4410acca9acdd4637c29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "daa56ebc4084424cb67a62810fee7d3a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "de1a013ad9be46dfbf0a53ee19a29eb4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dfcf5d456d3e4c01b7d66318e251806a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e612cd0070d54d54a55644fdd4ed80ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a261d20add5749288618865ccfad3cd8",
       "placeholder": "​",
       "style": "IPY_MODEL_40d353ff3e23425baf4a46e383004641",
       "value": "config.json: 100%"
      }
     },
     "e667e61c3b9547da98d28e8f894ad051": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f2a121711a034097b6947e1963653022": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_78a21c021d134602b778dc53bfb172ea",
        "IPY_MODEL_326f9b73a7014e2fb29cd9fe1f5c6f1a",
        "IPY_MODEL_fad71b44151e45ed9f9b941320395266"
       ],
       "layout": "IPY_MODEL_6825870d2df64e4a99f90d7975d6eb71"
      }
     },
     "f59d36bae75647b2a0a92f444ff760cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fad71b44151e45ed9f9b941320395266": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_feac2783956045e48549532f2050dac4",
       "placeholder": "​",
       "style": "IPY_MODEL_b08805b629a54d939c18e69e11d51a30",
       "value": " 5.68G/5.68G [00:27&lt;00:00, 207MB/s]"
      }
     },
     "feac2783956045e48549532f2050dac4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
