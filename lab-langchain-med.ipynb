{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40261e9b-c536-41bc-8a0a-b7d7c6aa0baa",
   "metadata": {},
   "source": [
    "# Lab | LangChain Med\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- continue on with lesson 2' example, use different datasets to test what we did in class. Some datasets are suggested in the notebook but feel free to scout other datasets on HuggingFace or Kaggle.\n",
    "- Find another model on Hugging Face and compare it.\n",
    "- Modify the prompt to fit your selected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ce0d3f",
   "metadata": {
    "papermill": {
     "duration": 0.028165,
     "end_time": "2024-02-21T15:58:20.016814",
     "exception": false,
     "start_time": "2024-02-21T15:58:19.988649",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dad70f",
   "metadata": {
    "papermill": {
     "duration": 0.016026,
     "end_time": "2024-02-21T15:58:20.049214",
     "exception": false,
     "start_time": "2024-02-21T15:58:20.033188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load the Dataset\n",
    "As you can see the notebook is ready to work with three different Datasets. Just uncomment the lines of the Dataset you want to use. \n",
    "\n",
    "I selected Datasets with News. Two of them have just a brief decription of the news, but the other contains the full text. \n",
    "\n",
    "As we are working in a free and limited space, I limited the number of news to use with the variable MAX_NEWS. Feel free to pull more if you have memory available. \n",
    "\n",
    "The name of the field containing the text of the new is stored in the variable *DOCUMENT* and the metadata in *TOPIC*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d5b28bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers\n",
    "!pip install -q xformers\n",
    "!pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5401d320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\katha\\anaconda3\\envs\\langchain\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\katha\\.cache\\kagglehub\\datasets\\deepanshudalal09\\mit-ai-news-published-till-2023\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"deepanshudalal09/mit-ai-news-published-till-2023\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e70a47e0",
   "metadata": {
    "papermill": {
     "duration": 1.170763,
     "end_time": "2024-02-21T15:58:21.235862",
     "exception": false,
     "start_time": "2024-02-21T15:58:20.065099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# news = pd.read_csv('/kaggle/input/topic-labeled-news-dataset/labelled_newscatcher_dataset.csv', sep=';')\n",
    "# MAX_NEWS = 1000\n",
    "# DOCUMENT=\"title\"\n",
    "# TOPIC=\"topic\"\n",
    "\n",
    "#news = pd.read_csv('/kaggle/input/bbc-news/bbc_news.csv')\n",
    "#MAX_NEWS = 1000\n",
    "#DOCUMENT=\"description\"\n",
    "#TOPIC=\"title\"\n",
    "\n",
    "news = pd.read_csv('articles.csv')\n",
    "MAX_NEWS = 100\n",
    "DOCUMENT=\"Article Body\"\n",
    "TOPIC=\"Article Header\"\n",
    "\n",
    "#news = \"PICK A DATASET\" #Ideally pick one from the commented ones above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb276f4",
   "metadata": {
    "papermill": {
     "duration": 0.016461,
     "end_time": "2024-02-21T15:58:21.268282",
     "exception": false,
     "start_time": "2024-02-21T15:58:21.251821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "ChromaDB requires that the data has a unique identifier. We can make it with this statement, which will create a new column called **Id**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c151df",
   "metadata": {
    "papermill": {
     "duration": 0.054847,
     "end_time": "2024-02-21T15:58:21.339906",
     "exception": false,
     "start_time": "2024-02-21T15:58:21.285059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Source</th>\n",
       "      <th>Article Header</th>\n",
       "      <th>Sub_Headings</th>\n",
       "      <th>Article Body</th>\n",
       "      <th>Url</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>July 7, 2023</td>\n",
       "      <td>Adam Zewe</td>\n",
       "      <td>MIT News Office</td>\n",
       "      <td>Learning the language of molecules to predict ...</td>\n",
       "      <td>This AI system only needs a small amount of da...</td>\n",
       "      <td>['Discovering new materials and drugs typicall...</td>\n",
       "      <td>https://news.mit.edu/2023/learning-language-mo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>July 6, 2023</td>\n",
       "      <td>Alex Ouyang</td>\n",
       "      <td>Abdul Latif Jameel Clinic for Machine Learning...</td>\n",
       "      <td>MIT scientists build a system that can generat...</td>\n",
       "      <td>BioAutoMATED, an open-source, automated machin...</td>\n",
       "      <td>['Is it possible to build machine-learning mod...</td>\n",
       "      <td>https://news.mit.edu/2023/bioautomated-open-so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>June 30, 2023</td>\n",
       "      <td>Jennifer Michalowski</td>\n",
       "      <td>McGovern Institute for Brain Research</td>\n",
       "      <td>When computer vision works more like a brain, ...</td>\n",
       "      <td>Training artificial neural networks with data ...</td>\n",
       "      <td>['From cameras to self-driving cars, many of t...</td>\n",
       "      <td>https://news.mit.edu/2023/when-computer-vision...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>June 30, 2023</td>\n",
       "      <td>Mary Beth Gallagher</td>\n",
       "      <td>School of Engineering</td>\n",
       "      <td>Educating national security leaders on artific...</td>\n",
       "      <td>Experts from MIT’s School of Engineering, Schw...</td>\n",
       "      <td>['Understanding artificial intelligence and ho...</td>\n",
       "      <td>https://news.mit.edu/2023/educating-national-s...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>June 30, 2023</td>\n",
       "      <td>Adam Zewe</td>\n",
       "      <td>MIT News Office</td>\n",
       "      <td>Researchers teach an AI to write better chart ...</td>\n",
       "      <td>A new dataset can help scientists develop auto...</td>\n",
       "      <td>['Chart captions that explain complex trends a...</td>\n",
       "      <td>https://news.mit.edu/2023/researchers-chart-ca...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Published Date                Author  \\\n",
       "0           0   July 7, 2023             Adam Zewe   \n",
       "1           1   July 6, 2023           Alex Ouyang   \n",
       "2           2  June 30, 2023  Jennifer Michalowski   \n",
       "3           3  June 30, 2023   Mary Beth Gallagher   \n",
       "4           4  June 30, 2023             Adam Zewe   \n",
       "\n",
       "                                              Source  \\\n",
       "0                                    MIT News Office   \n",
       "1  Abdul Latif Jameel Clinic for Machine Learning...   \n",
       "2              McGovern Institute for Brain Research   \n",
       "3                              School of Engineering   \n",
       "4                                    MIT News Office   \n",
       "\n",
       "                                      Article Header  \\\n",
       "0  Learning the language of molecules to predict ...   \n",
       "1  MIT scientists build a system that can generat...   \n",
       "2  When computer vision works more like a brain, ...   \n",
       "3  Educating national security leaders on artific...   \n",
       "4  Researchers teach an AI to write better chart ...   \n",
       "\n",
       "                                        Sub_Headings  \\\n",
       "0  This AI system only needs a small amount of da...   \n",
       "1  BioAutoMATED, an open-source, automated machin...   \n",
       "2  Training artificial neural networks with data ...   \n",
       "3  Experts from MIT’s School of Engineering, Schw...   \n",
       "4  A new dataset can help scientists develop auto...   \n",
       "\n",
       "                                        Article Body  \\\n",
       "0  ['Discovering new materials and drugs typicall...   \n",
       "1  ['Is it possible to build machine-learning mod...   \n",
       "2  ['From cameras to self-driving cars, many of t...   \n",
       "3  ['Understanding artificial intelligence and ho...   \n",
       "4  ['Chart captions that explain complex trends a...   \n",
       "\n",
       "                                                 Url  id  \n",
       "0  https://news.mit.edu/2023/learning-language-mo...   0  \n",
       "1  https://news.mit.edu/2023/bioautomated-open-so...   1  \n",
       "2  https://news.mit.edu/2023/when-computer-vision...   2  \n",
       "3  https://news.mit.edu/2023/educating-national-s...   3  \n",
       "4  https://news.mit.edu/2023/researchers-chart-ca...   4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news[\"id\"] = news.index\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1849922b",
   "metadata": {
    "papermill": {
     "duration": 0.027701,
     "end_time": "2024-02-21T15:58:21.383814",
     "exception": false,
     "start_time": "2024-02-21T15:58:21.356113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Because it is just a course we select a small portion of News.\n",
    "subset_news = news.head(MAX_NEWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893babc1",
   "metadata": {
    "papermill": {
     "duration": 0.015939,
     "end_time": "2024-02-21T15:58:21.416088",
     "exception": false,
     "start_time": "2024-02-21T15:58:21.400149",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import and configure the Vector Database\n",
    "I'm going to use ChromaDB, the most popular OpenSource embedding Database. \n",
    "\n",
    "First we need to import ChromaDB, and after that import the **Settings** class from **chromadb.config** module. This class allows us to change the setting for the ChromaDB system, and customize its behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82db86aa",
   "metadata": {
    "papermill": {
     "duration": 1.095805,
     "end_time": "2024-02-21T15:58:22.528102",
     "exception": false,
     "start_time": "2024-02-21T15:58:21.432297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051bc3a1",
   "metadata": {
    "papermill": {
     "duration": 0.015938,
     "end_time": "2024-02-21T15:58:22.560953",
     "exception": false,
     "start_time": "2024-02-21T15:58:22.545015",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we need to create the seetings object calling the **Settings** function imported previously. We store the object in the variable **settings_chroma**.\n",
    "\n",
    "Is necessary to inform two parameters \n",
    "* chroma_db_impl. Here we specify the database implementation and the format how store the data. I choose ***duckdb***, because his high-performace. It operate primarly in memory. And is fully compatible with SQL. The store format ***parquet*** is good for tabular data. With good compression rates and performance. \n",
    "\n",
    "* persist_directory: It just contains the directory where the data will be stored. Is possible work without a directory and the data will be stored in memory without persistece, but Kaggle dosn't support that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b21a77ff",
   "metadata": {
    "papermill": {
     "duration": 0.640745,
     "end_time": "2024-02-21T15:58:23.217828",
     "exception": false,
     "start_time": "2024-02-21T15:58:22.577083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"/path/to/persist/directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5176774",
   "metadata": {
    "papermill": {
     "duration": 0.016853,
     "end_time": "2024-02-21T15:58:23.252446",
     "exception": false,
     "start_time": "2024-02-21T15:58:23.235593",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Filling and Querying the ChromaDB Database\n",
    "The Data in ChromaDB is stored in collections. If the collection exist we need to delete it. \n",
    "\n",
    "In the next lines, we are creating the collection by calling the ***create_collection*** function in the ***chroma_client*** created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0cc5748",
   "metadata": {
    "papermill": {
     "duration": 0.090234,
     "end_time": "2024-02-21T15:58:23.358887",
     "exception": false,
     "start_time": "2024-02-21T15:58:23.268653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "collection_name = \"news_collection\"\n",
    "if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n",
    "        chroma_client.delete_collection(name=collection_name)\n",
    "\n",
    "collection = chroma_client.create_collection(name=collection_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688831d1",
   "metadata": {
    "papermill": {
     "duration": 0.01831,
     "end_time": "2024-02-21T15:58:23.394771",
     "exception": false,
     "start_time": "2024-02-21T15:58:23.376461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It's time to add the data to the collection. Using the function ***add*** we need to inform, at least ***documents***, ***metadatas*** and ***ids***. \n",
    "* In the **document** we store the big text, it's a different column in each Dataset. \n",
    "* In **metadatas**, we can informa a list of topics. \n",
    "* In **id** we need to inform an unique identificator for each row. It MUST be unique! I'm creating the ID using the range of MAX_NEWS. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb1a28a",
   "metadata": {
    "papermill": {
     "duration": 89.680388,
     "end_time": "2024-02-21T15:59:53.091437",
     "exception": false,
     "start_time": "2024-02-21T15:58:23.411049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "collection.add(\n",
    "    documents=subset_news[DOCUMENT].tolist(),\n",
    "    metadatas=[{TOPIC: topic} for topic in subset_news[TOPIC].tolist()],\n",
    "    ids=[f\"id{x}\" for x in range(MAX_NEWS)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff6a4fcf",
   "metadata": {
    "papermill": {
     "duration": 0.121938,
     "end_time": "2024-02-21T15:59:53.236515",
     "exception": false,
     "start_time": "2024-02-21T15:59:53.114577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['id9', 'id85', 'id1', 'id52', 'id73', 'id93', 'id6', 'id16', 'id38', 'id18']], 'embeddings': None, 'documents': [['[\\'How will advances in computing transform human society?\\', \\'MIT students contemplated this impending question as part of the Envisioning the Future of Computing Prize — an essay contest in which they were challenged to imagine ways that computing technologies could improve our lives, as well as the pitfalls and dangers associated with them.\\', \\'Offered for the first time this year, the Institute-wide competition invited MIT undergraduate and graduate students to share their ideas, aspirations, and vision for what they think a future propelled by advancements in computing holds. Nearly 60 students put pen to paper, including those majoring in mathematics, philosophy, electrical engineering and computer science, brain and cognitive sciences, chemical engineering, urban studies and planning, and management, and entered their submissions.\\', \\'Students dreamed up highly inventive scenarios for how the technologies of today and tomorrow could impact society, for better or worse. Some recurring themes emerged, such as tackling issues in climate change and health care. Others proposed ideas for particular technologies that ranged from digital twins as a tool for navigating the deluge of information online to a cutting-edge platform powered by artificial intelligence, machine learning, and biosensors to create personalized storytelling films that help individuals understand themselves and others.\\', \\'Conceived of by the Social and Ethical Responsibilities of Computing (SERC), a cross-cutting initiative of the MIT Schwarzman College of Computing in collaboration with the School of Humanities, Arts, and Social Sciences (SHASS), the intent of the competition was “to create a space for students to think in a creative, informed, and rigorous way about the societal benefits and costs of the technologies they are or will be developing,” says Caspar Hare, professor of philosophy, co-associate dean of SERC, and the lead organizer of the Envisioning the Future of Computing Prize. “We also wanted to convey that MIT values such thinking.”\\', \\'Prize winners\\', \\'The contest implemented a two-stage evaluation process wherein all essays were reviewed anonymously by a panel of MIT faculty members from the college and SHASS for the initial round. Three qualifiers were then invited to present their entries at an awards ceremony on May 8, followed by a Q&A with a judging panel and live in-person audience for the final round.\\', \"The winning entry was awarded to Robert Cunningham \\'23, a recent graduate in math and physics, for his paper on the implications of a personalized language model that is fine-tuned to predict an individual’s writing based on their past texts and emails. Told from the perspective of three fictional characters: Laura, founder of the tech startup ScribeAI, and Margaret and Vincent, a couple in college who are frequent users of the platform, readers gained insights into the societal shifts that take place and the unforeseen repercussions of the technology.\", \\'Cunningham, who took home the grand prize of $10,000, says he came up with the concept for his essay in late January while thinking about the upcoming release of GPT-4 and how it might be applied. Created by the developers of ChatGPT — an AI chatbot that has managed to capture popular imagination for its capacity to imitate human-like text, images, audio, and code — GPT-4, which was unveiled in March, is the newest version of OpenAI’s language model systems.\\', \"“GPT-4 is wild in reality, but some rumors before it launched were even wilder, and I had a few long\\\\xa0plane rides to\\\\xa0think about them! I enjoyed this opportunity to solidify a vague notion into a piece of writing, and since some of my favorite works of science fiction are short stories, I figured I\\'d take the chance to write one,” Cunningham says.\", \"The other two finalists, awarded $5,000 each, included Gabrielle Kaili-May Liu \\'23, a recent graduate in mathematics with computer science, and brain and cognitive sciences, for her entry on using the reinforcement learning with human feedback technique as a tool for transforming human interactions with AI; and Abigail Thwaites and Eliot Matthew Watkins, graduate students in the Department of Philosophy and Linguistics, for their joint submission on automatic fact checkers, an AI-driven software that they argue could potentially help mitigate the spread of misinformation and be a profound social good.\", \\'“We were so excited to see the amazing response to this contest. It made clear how much students at MIT, contrary to stereotype, really care about the wider implications of technology, says Daniel Jackson, professor of computer science and one of the final-round judges. “So many of the essays were incredibly thoughtful and creative. Robert’s story was a chilling, but entirely plausible take on our AI future; Abigail and Eliot’s analysis brought new clarity to what harms misinformation actually causes; and Gabrielle’s piece gave a lucid overview of a prominent new technology. I hope we’ll be able to run this contest every year, and that it will encourage all our students to broaden their perspectives even further.”\\', \\'Fellow judge Graham Jones, professor of anthropology, adds: “The winning entries reflected the incredible breadth of our students’ engagement with socially responsible computing. They challenge us to think differently about how to design computational technologies, conceptualize social impacts, and imagine future scenarios. Working with a cross-disciplinary panel of judges catalyzed lots of new conversations. As a sci-fi fan, I was thrilled that the top prize went to a such a stunning piece of speculative fiction!”\\', \\'Other judges on the panel for the final round included:\\', \\'Honorable mentions\\', \\'In addition to the grand prize winner and runners up, 12 students were recognized with honorable mentions for their entries, with each receiving $500.\\', \\'The honorees and the title of their essays include:\\', \\'The Envisioning the Future of Computing Prize was supported by MAC3 Impact Philanthropies.\\']', \"['As a graduate student doing his master’s thesis on speech recognition at the MIT AI Lab (now the MIT Computer Science and Artificial Intelligence Laboratory), Dan Huttenlocher worked closely with Professor Victor Zue. Well known for pioneering the development of systems that enable an user to interact with computers using spoken language, Zue traveled frequently to Asia — where much of the early research in speech recognition happened during the 1980s. Huttenlocher occasionally accompanied his professor on these trips, many of which involved interactions with members of MIT Industrial Liaison Program, as he recalls. “It was a tremendous opportunity,” according to Huttenlocher, “and it was a large part of what built my interest in engaging with companies and industry in addition to the academic side of research.”', 'Huttenlocher went on to earn his PhD in computer vision at the Institute and has since embarked on a career that encompasses academia, industry, and the philanthropic sector. In addition to solidifying his status as an esteemed researcher in the academic realm, he spent 12 years as a scientist at Xerox’s Palo Alto Research Center before leaving to co-found a financial technology company. He served on the board of the John D. and Catherine T. MacArthur Foundation from 2010-22 (including as chair starting in 2018), and serves on the boards of directors at Amazon.com and Corning, Inc. He also helped found Cornell Tech, the technology, business, law, and design campus in New York City built by Cornell University. There, he was the school’s first dean and vice provost, guiding its efforts to tie together industry and computing to enhance New York’s tech ecosystem.', 'Today, Huttenlocher serves as the inaugural dean at MIT Schwarzman College of Computing. To highlight the significance of this moment in time, and the need for an interdisciplinary computing hub like the college of computing, he references the oft-cited prediction that software would gobble up and disrupt traditional industry structures. Huttenlocher believes that while this insight was right, what we’re experiencing now is something different, greater, with vast implications for humanity. Computing on the whole — not only software but also hardware, algorithms, and machine learning — has evolved to the point where it is redefining our approach to problem-solving in nearly every industry sector, discipline, and area of research. This, he suggests, is also redefining reality as we experience it.']\", '[\\'Is it possible to build machine-learning models without machine-learning expertise?\\', \\'Jim Collins, the Termeer Professor of Medical Engineering and Science in the Department of Biological Engineering at MIT and the life sciences faculty lead at the Abdul Latif Jameel Clinic for Machine Learning in Health (Jameel Clinic), along with a number of colleagues decided to tackle this problem when facing a similar conundrum. An open-access paper on their proposed solution, called BioAutoMATED, was published on June 21 in Cell Systems.\\', \\'Recruiting machine-learning researchers can be a time-consuming and financially costly process for science and engineering labs. Even with a machine-learning expert, selecting the appropriate model, formatting the dataset for the model, then fine-tuning it can dramatically change how the model performs, and takes a lot of work.\\', \\'“In your machine-learning project, how much time will you typically spend on data preparation and transformation?” asks a 2022 Google course on the Foundations of Machine Learning (ML). The two choices offered are either “Less than half the project time” or “More than half the project time.” If you guessed the latter, you would be correct; Google states that it takes over 80 percent of project time to format the data, and that’s not even taking into account the time needed to frame the problem in machine-learning terms.\\', \\'“It would take many weeks of effort to figure out the appropriate model for our dataset, and this is a really prohibitive step for a lot of folks that want to use machine learning or biology,” says Jacqueline Valeri, a fifth-year PhD student of biological engineering in Collins’s lab who is first co-author of the paper.\\', \"BioAutoMATED is an automated machine-learning system that can select and build an appropriate model for a given dataset and even take care of the laborious task of data preprocessing, whittling down a months-long process to just a few hours. Automated machine-learning (AutoML) systems are still in a relatively nascent stage of development, with current usage primarily focused on image and text recognition, but largely unused in subfields of biology, points out first co-author and Jameel Clinic postdoc Luis Soenksen PhD \\'20.\", \\'“The fundamental language of biology is based on sequences,” explains Soenksen, who earned his doctorate in the MIT Department of Mechanical Engineering. “Biological sequences such as DNA, RNA, proteins, and glycans have the amazing informational property of being intrinsically standardized, like an alphabet. A lot of AutoML tools are developed for text, so it made sense to extend it to [biological] sequences.”\\', \\'Moreover, most AutoML tools can only explore and build reduced types of models. “But you can’t really know from the start of a project which model will be best for your dataset,” Valeri says. “By incorporating multiple tools under one umbrella tool, we really allow a much larger search space than any individual AutoML tool could achieve on its own.”\\', \\'BioAutoMATED’s repertoire of supervised ML models includes three types: binary classification models (dividing data into two classes), multi-class classification models (dividing data into multiple classes), and regression models (fitting continuous numerical values or measuring the strength of key relationships between variables). BioAutoMATED is even able to help determine how much data is required to appropriately train the chosen model.\\', \\'\"Our tool explores models that are better-suited for smaller, sparser biological datasets as well as more complex neural networks,” Valeri says. This is an advantage for research groups with new data that may or may not be suited for a machine learning problem.\\', \\'\"Conducting novel and successful experiments at the intersection of biology and machine learning can cost a lot of money,” Soenksen explains. \"Currently, biology-centric labs need to invest in significant digital infrastructure and AI-ML trained human resources before they can even see if their ideas are poised to pan out. We want to lower these barriers for domain experts in biology.” With BioAutoMATED, researchers have the freedom to run initial experiments to assess if it’s worthwhile to hire a machine-learning expert to build a different model for further experimentation.\\', \\'The open-source code is publicly available and, researchers emphasize, it is easy to run. “What we would love to see is for people to take our code, improve it, and collaborate with larger communities to make it a tool for all,” Soenksen says. “We want to prime the biological research community and generate awareness related to AutoML techniques, as a seriously useful pathway that could merge rigorous biological practice with fast-paced AI-ML practice better than it is achieved today.”\\', \"Collins, the senior author on the paper, is also affiliated with the MIT Institute for Medical Engineering and Science, the Harvard-MIT Program in Health Sciences and Technology, the Broad Institute of MIT and Harvard, and the Wyss Institute. Additional MIT contributors to the paper include Katherine M. Collins \\'21; Nicolaas M. Angenent-Mari PhD \\'21; Felix Wong, a former postdoc in the Department of Biological Engineering, IMES, and the Broad Institute; and Timothy K. Lu, a professor of biological engineering and of electrical engineering and computer science.\", \\'This work was supported, in part, by a Defense Threat Reduction Agency grant, the Defense Advance Research Projects Agency SD2 program, the Paul G. Allen Frontiers Group, the Wyss Institute for Biologically Inspired Engineering of Harvard University; an MIT-Takeda Fellowship, a Siebel Foundation Scholarship, a CONACyT grant, an MIT-TATA Center fellowship, a Johnson & Johnson Undergraduate Research Scholarship, a Barry Goldwater Scholarship, a Marshall Scholarship, Cambridge Trust, and the National Institute of Allergy and Infectious Diseases of the National Institutes of Health. This work is part of the Antibiotics-AI Project, which is supported by the Audacious Project, Flu Lab, LLC, the Sea Grape Foundation, Rosamund Zander and Hansjorg Wyss for the Wyss Foundation, and an anonymous donor.\\']', \"['It’s no secret that OpenAI’s ChatGPT has some incredible capabilities — for instance, the chatbot can write poetry that resembles Shakespearean sonnets or debug code for a computer program. These abilities are made possible by the massive machine-learning model that ChatGPT is built upon. Researchers have found that when these types of models become large enough, extraordinary capabilities emerge.', '', 'But bigger models also require more time and money to train. The training process involves showing hundreds of billions of examples to a model. Gathering so much data is an involved process in itself. Then come the monetary and environmental costs of running many powerful computers for days or weeks to train a model that may have billions of parameters.', '', '“It’s been estimated that training models at the scale of what ChatGPT is hypothesized to run on could take millions of dollars, just for a single training run. Can we improve the efficiency of these training methods, so we can still get good models in less time and for less money? We propose to do this by leveraging smaller language models that have previously been trained,” says Yoon Kim, an assistant professor in MIT’s Department of Electrical Engineering and Computer Science and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL).', '', 'Rather than discarding a previous version of a model, Kim and his collaborators use it as the building blocks for a new model. Using machine learning, their method learns to “grow” a larger model from a smaller model in a way that encodes knowledge the smaller model has already gained. This enables faster training of the larger model.', '', 'Their technique saves about 50 percent of the computational cost required to train a large model, compared to methods that train a new model from scratch. Plus, the models trained using the MIT method performed as well as, or better than, models trained with other techniques that also use smaller models to enable faster training of larger models.', '', 'Reducing the time it takes to train huge models could help researchers make advancements faster with less expense, while also reducing the carbon emissions generated during the training process. It could also enable smaller research groups to work with these massive models, potentially opening the door to many new advances.', '', '“As we look to democratize these types of technologies, making training faster and less expensive will become more important,” says Kim, senior author of a paper on this technique.', '', 'Kim and his graduate student Lucas Torroba Hennigen wrote the paper with lead author Peihao Wang, a graduate student at the University of Texas at Austin, as well as others at the MIT-IBM Watson AI Lab and Columbia University. The research will be presented at the International Conference on Learning Representations.', '', 'The bigger the better', '', 'Large language models like GPT-3, which is at the core of ChatGPT, are built using a neural network architecture called a transformer. A neural network, loosely based on the human brain, is composed of layers of interconnected nodes, or “neurons.” Each neuron contains parameters, which are variables learned during the training process that the neuron uses to process data.', '', 'Transformer architectures are unique because, as these types of neural network models get bigger, they achieve much better results.', '', '“This has led to an arms race of companies trying to train larger and larger transformers on larger and larger datasets. More so than other architectures, it seems that transformer networks get much better with scaling. We’re just not exactly sure why this is the case,” Kim says.', '', 'These models often have hundreds of millions or billions of learnable parameters. Training all these parameters from scratch is expensive, so researchers seek to accelerate the process.', '', 'One effective technique is known as model growth. Using the model growth method, researchers can increase the size of a transformer by copying neurons, or even entire layers of a previous version of the network, then stacking them on top. They can make a network wider by adding new neurons to a layer or make it deeper by adding additional layers of neurons.', '', 'In contrast to previous approaches for model growth, parameters associated with the new neurons in the expanded transformer are not just copies of the smaller network’s parameters, Kim explains. Rather, they are learned combinations of the parameters of the smaller model.', '', 'Learning to grow', '', 'Kim and his collaborators use machine learning to learn a linear mapping of the parameters of the smaller model. This linear map is a mathematical operation that transforms a set of input values, in this case the smaller model’s parameters, to a set of output values, in this case the parameters of the larger model.', '', 'Their method, which they call a learned Linear Growth Operator (LiGO), learns to expand the width and depth of larger network from the parameters of a smaller network in a data-driven way.', '', 'But the smaller model may actually be quite large — perhaps it has a hundred million parameters — and researchers might want to make a model with a billion parameters. So the LiGO technique breaks the linear map into smaller pieces that a machine-learning algorithm can handle.', '', 'LiGO also expands width and depth simultaneously, which makes it more efficient than other methods. A user can tune how wide and deep they want the larger model to be when they input the smaller model and its parameters, Kim explains.', '', 'When they compared their technique to the process of training a new model from scratch, as well as to model-growth methods, it was faster than all the baselines. Their method saves about 50 percent of the computational costs required to train both vision and language models, while often improving performance.', '', 'The researchers also found they could use LiGO to accelerate transformer training even when they didn’t have access to a smaller, pretrained model.', '', '“I was surprised by how much better all the methods, including ours, did compared to the random initialization, train-from-scratch baselines.” Kim says.', '', 'In the future, Kim and his collaborators are looking forward to applying LiGO to even larger models.', '', 'The work was funded, in part, by the MIT-IBM Watson AI Lab, Amazon, the IBM Research AI Hardware Center, Center for Computational Innovation at Rensselaer Polytechnic Institute, and the U.S. Army Research Office.']\", \"['The MIT-Takeda Program, a collaboration between MIT’s School of Engineering and Takeda Pharmaceuticals Company, fuels the development and application of artificial intelligence capabilities to benefit human health and drug development. Part of the Abdul Latif Jameel Clinic for Machine Learning in Health, the program coalesces disparate disciplines, merges theory and practical implementation, combines algorithm and hardware innovations, and creates multidimensional collaborations between academia and industry.', 'With the aim of building a community dedicated to the next generation of AI and system-level breakthroughs, the MIT-Takeda Program is also creating educational opportunities. Every year Takeda funds fellowships to support graduate students pursuing research related to health and AI. This year’s Takeda Fellows, described below, are working on projects ranging from electronic health record systems and robotic control to pandemic preparedness and traumatic brain injuries.', 'Camille C. Farruggio', 'Farruggio is a PhD candidate in the Department of Materials Science and Engineering whose research leverages AI and machine learning, including regression modeling, to help realize the promise of cells-as-medicine applications. As a Takeda Fellow, she seeks to develop a holistic understanding of the culture conditions and cell attributes that modulate and predict cell efficacy as therapeutic treatments and solve existing technology bottlenecks in the production of cell therapies.', 'Wenhao Gao', 'Gao is a PhD candidate in the Department of Chemical Engineering who aims to accelerate biological and chemical discovery processes. His work specifically focuses on AI for health sciences and cutting-edge applications of machine learning for molecular discovery and drug development. Gao’s research, supported by a Takeda Fellowship, seeks to create a more efficient process, using AI algorithms to advance de novo design methods and organic synthesis for accelerated drug development.', 'Samuel Goldman', 'Goldman is a PhD candidate in the Computational and Systems Biology Program whose research interests lie at the intersection of biology, analytical chemistry, and machine learning. Specifically, Goldman uses mass spectrometry data and generative deep learning to elucidate the structures of unknown molecules in biological samples, with important implications for drug discovery. As a Takeda Fellow, he will build new computational tools to characterize and measure unknown small molecule metabolites in a cellular mixture.', 'Sarah Gurev', 'Gurev is a PhD candidate in the Department of Electrical Engineering and Computer Science. Her research seeks to address the challenges of pandemic preparedness and the prediction of viral immune evasion. As a Takeda Fellow, Gurev will advance her work at the intersection of computational approaches and experimental screening to develop new models of antibody escape.', 'R’mani Haulcy\\\\nHaulcy is a PhD candidate in the Department of Electrical Engineering and Computer Science whose work bridges the fields of AI and health to create cutting-edge AI-based assessments of cognitive impairment in speech and language disorders. Supported by a Takeda Fellowship, Haulcy will develop new tools for speech processing focused on the measurement of health-related speech biomarkers, specifically examining the speech of subjects with frontotemporal dementia and primary progressive aphasia.Velina Kozareva\\\\nKozareva is a PhD candidate in the Computational and Systems Biology Program whose research focuses on developing machine learning methods to integrate multi-omic data in heterogeneous diseases. As a Takeda Fellow, Kozareva aims to develop computational methods to simultaneously identify subtypes of heterogeneous diseases and the causal mechanisms that drive each subtype, with an initial focus on amyotrophic lateral sclerosis.Yang Liu\\\\nLiu is a PhD candidate in the Department of Electrical Engineering and Computer Science whose current work focuses on AI for health records and computational imaging/photography, which lies at the confluence of computer science, optics, biomedical/neuroscience, hardware design, and software design. Liu’s Takeda Fellowship will support his current research, a collaborative project that aims to address the connected challenges of delivering health care and maintaining health-care records in resource-constrained settings.Luke Murray\\\\nMurray is a PhD candidate in the Department of Electrical Engineering and Computer Science whose work is focused on electronic health record (EHR) systems, which have revolutionized health care and hold tremendous potential for clinical diagnosis, operations, and research, but also suffer from serious shortcomings. Through his Takeda Fellowship, Murray will tackle a primary EHR limitation: disparate interfaces that fragment the clinical workflow into time-consuming, error-prone processes that require clinicians to spend more time interacting with EHRs than with patients.Mark Olchanyi\\\\nOlchanyi is a PhD candidate in the Harvard-MIT Program in Health Sciences and Technology whose research seeks to advance our knowledge of traumatic brain injuries (TBIs). Olchanyi’s research, supported by a Takeda Fellowship, will apply deep learning to study in vivo imaging-based TBI biomarkers, with a particular focus on subcortical white matter lesions in acute TBIs resulting in disorders of consciousness.Krista Pullen\\\\nPullen is a PhD candidate in the Department of Biological Engineering whose research is situated at the intersection of vaccine immunology and machine learning. With the support of a Takeda Fellowship, Pullen will develop and validate the application of cross-species modeling in the context of vaccine immunology to enable the prediction of human efficacy from preclinical data.Georgia Thomas\\\\nThomas is a PhD candidate in the Harvard-MIT Program in Health Sciences and Technology whose research explores the underlying physics of optical imaging, with the goal of expanding its capacity to address important medical challenges. As a Takeda Fellow, Thomas will advance her work to create innovative tools to better understand and treat coronary atherosclerosis, a disease affecting over 18 million people in the United States alone.A. Michael West Jr.\\\\nWest is a PhD candidate in the Department of Mechanical Engineering whose research integrates robotics, AI, and health care to improve robotic rehabilitation and advance human-robot interactions. Specifically, his work explores the human neuromotor control of movement, with the goal of enhancing robot control and performance. As a Takeda Fellow, West will study the functionality of the human hand and its ability to manipulate objects and tools.']\", '[\\'Like millions of others during the global Covid-19 lockdowns, Emmanuel Kasigazi, an entrepreneur from Uganda, turned to YouTube to pass the time. But he wasn’t following an influencer or watching music videos. A lifelong learner, Kasigazi was scouring the video-sharing platform for educational resources. Since 2013, when he got his first smartphone, Kasigazi has been charting his own learning journey through YouTube, educating himself on subjects as diverse as psychology and artificial intelligence. And it was while searching for the answer to an AI-related question that Kasigazi first discovered MIT OpenCourseWare (OCW).\\', \"“The search results showed MIT lectures, and I thought, \\'Which MIT is this?’” recalls Kasigazi, who admits he was initially skeptical as he opened the OCW YouTube channel. To his amazement, he found hundreds of courses there — not only clips, but complete lectures that he could follow alongside the students in MIT classrooms. He searched for more information on OCW and tried the channel on different browsers to triple-check its credibility. “Here they were, all these courses by one of the best — if not the best — schools in tech in the world, and they were free. For a long time I couldn’t believe it. I told everyone I knew,” he remembers.\", \"For Kasigazi, the channel became a gateway to other open education resources, including the OpenCourseWare website and MITx courses, both part of MIT Open Learning. “I always had the questions — I grew up on science cartoons like \\'Dexter’s Laboratory\\' and \\'Pinky and the Brain\\' — so I would go on YouTube to try to find answers to these questions, and I found this whole other world,” he says.\", \\'OCW launched its YouTube channel in 2008, and this August passed 4 million subscribers. While introductory computer science, math, and physics are the most-visited courses on the OCW website, the most popular YouTube videos reflect a more diverse range of interests, including a lecture about piloting a fighter jet aircraft, an introduction to the human brain, and an introduction to financial terms and concepts.\\', \\'Through this extensive collection, Kasigazi explains that he’s been able to explore “the things I love,” while also studying cloud computing, data science, and AI — fields that he plans to pursue in graduate studies. He says, “This is what OpenCourseWare has enabled me to do: I get the chance to not only watch the future happen, but I can actually be a part of it and create it.”\\', \\'Understanding humanity through the liberal arts\\', \\'When Kasigazi was young, a beloved aunt recognized his natural curiosity and steered him toward the best schools. “I owe her everything,” he says, “everything I am is because of her.” Thanks to his excellent grades he received an academic scholarship from the Ugandan government to attend Makerere University, one of the top universities in sub-Saharan Africa, where he earned a degree in information systems. Having pursued IT for its practical applications, Kasigazi admits that he was initially more interested in the science and theory behind computers than “the coding bits of it.”\\', \\'“I love the concept of it — how we are trying to make these machines,” he says, explaining that he’s long been drawn to the social sciences and humanities, particularly psychology and philosophy.\\', \\'“I’m interested in how we work as human beings, because everything we do is for, with, and around human beings,” says Kasigazi, who considers psychology to be foundational to almost every field. “Whatever it is you’re teaching these kids, they’re going to be dealing with people. So first teach them what people think, how they act — that was my drive to love psychology.”\\', \\'Kasigazi has also turned to OCW to brush up on his coding skills, watching 6.0001 (Introduction to Computer Science and Programming Using Python) lectures with Professor Ana Bell and reviewing the instructor-paced version with Professor Eric Grimson now on MITx. “I am proud to say MIT OCW has made me fall in love with coding … it makes sense like it never has before,” he says.\\', \\'Nurturing a worldview\\', \\'In 2014 Kasigazi moved to South Sudan, which had only recently emerged from a civil war as an independent nation. Fresh out of university, he was there to teach computer skills and graphic design — some of his students included members of the new country’s government — but his time in South Sudan quickly became a learning experience for him, too. “When you grow up in your community, you have this bubble. We all experience it — it’s a human thing,” he reflects. “For the first time, I realized that everything I knew is not a given. Everything I grew up knowing is not universal.”\\', \\'With his worldview newly broadened, he began to nurture his interest in psychology, philosophy, and the sciences, watching crash courses, explainer videos, and other content on the subject. “It’s entertainment, to me, at the same time that it’s a passion,” he says. Today Kasigazi runs his own company, which he started in 2012 with friends and resumed when he returned to Uganda seven years ago.\\', \\'Since coming across the OCW YouTube channel, Kasigazi has worked through all of the freely available MIT psychology courses. Professor John Gabrieli’s 9.00SC (Introduction to Psychology) have particularly resonated with him, even prompting him to reach out to Gabrieli. “As much as I’d been getting some knowledge on psychology over the years online, it wasn’t as deep and as interesting or captivating as your classes were,” he wrote. “From your teaching style, to the explanations, to the topics, to how you make people understand a topic, to the experiments mentioned and referenced, to how you approach questions and later make one think deeper about them.”\\', \\'“The message from Emmanuel is deeply touching about the joy of learning,” says Gabrieli, who is also an investigator at the McGovern Institute. “I am so grateful to OCW for making this course on psychology open to the world, and to Emmanuel for so delightfully sharing what this course meant to him.”\\', \\'New courses are added regularly to both the OCW website and YouTube channel. Kasigazi, who’s currently enjoying 9.13 (Introduction to the Human Brain) from professor and McGovern Institute investigator Nancy Kanwisher, looks forward to discovering what new worlds of knowledge they’ll open.\\']', \"['When Erik Duhaime PhD ’19 was working on his thesis in MIT’s Center for Collective Intelligence, he noticed his wife, then a medical student, spending hours studying on apps that offered flash cards and quizzes. His research had shown that, as a group, medical students could classify skin lesions more accurately than professional dermatologists; the trick was to continually measure each student’s performance on cases with known answers, throw out the opinions of people who were bad at the task, and intelligently pool the opinions of people that were good.', 'Combining his wife’s studying habits with his research, Duhaime founded Centaur Labs, a company that created a mobile app called DiagnosUs to gather the opinions of medical experts on real-world scientific and biomedical data. Through the app, users review anything from images of potentially cancerous skin lesions or audio clips of heart and lung sounds that could indicate a problem. If the users are accurate, Centaur uses their opinions and awards them small cash prizes. Those opinions, in turn, help medical AI companies train and improve their algorithms.', 'The approach combines the desire of medical experts to hone their skills with the desperate need for well-labeled medical data by companies using AI for biotech, developing pharmaceuticals, or commercializing medical devices.', '“I realized my wife’s studying could be productive work for AI developers,” Duhaime recalls. “Today we have tens of thousands of people using our app, and about half are medical students who are blown away that they win money in the process of studying. So, we have this gamified platform where people are competing with each other to train data and winning money if they’re good and improving their skills at the same time — and by doing that they are labeling data for teams building life saving AI.”', 'Gamifying medical labeling', 'Duhaime completed his PhD under Thomas Malone, the Patrick J. McGovern Professor of Management and founding director of the Center for Collective Intelligence.', '“What interested me was the wisdom of crowds phenomenon,” Duhaime says. “Ask a bunch of people how many jelly beans are in a jar, and the average of everybody’s answer is pretty close. I was interested in how you navigate that problem in a task that requires skill or expertise. Obviously you don’t just want to ask a bunch of random people if you have cancer, but at the same time, we know that second opinions in health care can be extremely valuable. You can think of our platform as a supercharged way of getting a second opinion.”', 'Duhaime began exploring ways to leverage collective intelligence to improve medical diagnoses. In one experiment, he trained groups of lay people and medical school students that he describes as “semiexperts” to classify skin conditions, finding that by combining the opinions of the highest performers he could outperform professional dermatologists. He also found that by combining algorithms trained to detect skin cancer with the opinions of experts, he could outperform either method on its own.', '“The core insight was you do two things,” Duhaime explains. “The first thing is to measure people’s performance — which sounds obvious, but even in the medical domain it isn’t done much. If you ask a dermatologist if they’re good, they say, ‘Yeah of course, I’m a dermatologist.’ They don’t necessarily know how good they are at specific tasks. The second thing is that when you get multiple opinions, you need to identify complementarities between the different people. You need to recognize that expertise is multidimensional, so it’s a little more like putting together the optimal trivia team than it is getting the five people who are all the best at the same thing. For example, one dermatologist might be better at identifying melanoma, whereas another might be better at classifying the severity of psoriasis.”', 'While still pursuing his PhD, Duhaime founded Centaur and began using MIT’s entrepreneurial ecosystem to further develop the idea. He received funding from MIT’s Sandbox Innovation Fund in 2017 and participated in the delta v startup accelerator run by the Martin Trust Center for MIT Entrepreneurship over the summer of 2018. The experience helped him get into the prestigious Y Combinator accelerator later that year.', 'The DiagnosUs app, which Duhaime developed with Centaur co-founders Zach Rausnitz and Tom Gellatly, is designed to help users test and improve their skills. Duhaime says about half of users are medical school students and the other half are mostly doctors, nurses, and other medical professionals.', '“It’s better than studying for exams, where you might have multiple choice questions,” Duhaime says. “They get to see actual cases and practice.”', 'Centaur gathers millions of opinions every week from tens of thousands of people around the world. Duhaime says most people earn coffee money, although the person who’s earned the most from the platform is a doctor in eastern Europe who’s made around $10,000.', '“People can do it on the couch, they can do it on the T,” Duhaime says. “It doesn’t feel like work — it’s fun.”', 'The approach stands in sharp contrast to traditional data labeling and AI content moderation, which are typically outsourced to low-resource countries.', 'Centaur’s approach produces accurate results, too. In a paper with researchers from Brigham and Women’s Hospital, Massachusetts General Hospital (MGH), and Eindhoven University of Technology, Centaur showed its crowdsourced opinions labeled lung ultrasounds as reliably as experts did. Another study with researchers at Memorial Sloan Kettering showed crowdsourced labeling of dermoscopic images was more accurate than that of highly experienced dermatologists. Beyond images, Centaur’s platform also works with video, audio, text from sources like research papers or anonymized conversations between doctors and patients, and waves from electroencephalograms (EEGs) and electrocardiographys (ECGs).', 'Finding the experts', 'Centaur has found that the best performers come from surprising places. In 2021, to collect expert opinions on EEG patterns, researchers held a contest through the DiagnosUs app at a conference featuring about 50 epileptologists, each with more than 10 years of experience. The organizers made a custom shirt to give to the contest’s winner, who they assumed would be in attendance at the conference.', 'But when the results came in, a pair of medical students in Ghana, Jeffery Danquah and Andrews Gyabaah, had beaten everyone in attendance. The highest-ranked conference attendee had come in ninth.', '“I started by doing it for the money, but I realized it actually started helping me a lot,” Gyabaah told Centaur’s team later. “There were times in the clinic where I realized that I was doing better than others because of what I learned on the DiagnosUs app.”', 'As AI continues to change the nature of work, Duhaime believes Centaur Labs will be used as an ongoing check on AI models.', '“Right now, we’re helping people train algorithms primarily, but increasingly I think we’ll be used for monitoring algorithms and in conjunction with algorithms, basically serving as the humans in the loop for a range of tasks,” Duhaime says. “You might think of us less as a way to train AI and more as a part of the full life cycle, where we’re providing feedback on models’ outputs or monitoring the model.”', 'Duhaime sees the work of humans and AI algorithms becoming increasingly integrated and believes Centaur Labs has an important role to play in that future.', '“It’s not just train algorithm, deploy algorithm,” Duhaime says. “Instead, there will be these digital assembly lines all throughout the economy, and you need on-demand expert human judgment infused in different places along the value chain.”']\", '[\\'There has been a remarkable surge in the use of algorithms and artificial intelligence to address a wide range of problems and challenges. While their adoption, particularly with the rise of AI, is reshaping nearly every industry sector, discipline, and area of research, such innovations often expose unexpected consequences that involve new norms, new expectations, and new rules and laws.\\', \\'To facilitate deeper understanding, the Social and Ethical Responsibilities of Computing (SERC), a cross-cutting initiative in the MIT Schwarzman College of Computing, recently brought together social scientists and humanists with computer scientists, engineers, and other computing faculty for an exploration of the ways in which the broad applicability of algorithms and AI has presented both opportunities and challenges in many aspects of society.\\', \\'“The very nature of our reality is changing. AI has the ability to do things that until recently were solely the realm of human intelligence — things that can challenge our understanding of what it means to be human,” remarked Daniel Huttenlocher, dean of the MIT Schwarzman College of Computing, in his opening address at the inaugural SERC Symposium. “This poses philosophical, conceptual, and practical questions on a scale not experienced since the start of the Enlightenment. In the face of such profound change, we need new conceptual maps for navigating the change.”\\', \\'The symposium offered a glimpse into the vision and activities of SERC in both research and education. “We believe our responsibility with SERC is to educate and equip our students and enable our faculty to contribute to responsible technology development and deployment,” said Georgia Perakis, the William F. Pounds Professor of Management in the MIT Sloan School of Management, co-associate dean of SERC, and the lead organizer of the symposium. “We’re drawing from the many strengths and diversity of disciplines across MIT and beyond and bringing them together to gain multiple viewpoints.”\\', \\'Through a succession of panels and sessions, the symposium delved into a variety of topics related to the societal and ethical dimensions of computing. In addition, 37 undergraduate and graduate students from a range of majors, including urban studies and planning, political science, mathematics, biology, electrical engineering and computer science, and brain and cognitive sciences, participated in a poster session to exhibit their research in this space, covering such topics as quantum ethics, AI collusion in storage markets, computing waste, and empowering users on social platforms for better content credibility.\\', \\'Showcasing a diversity of work\\', \\'In three sessions devoted to themes of beneficent and fair computing, equitable and personalized health, and algorithms and humans, the SERC Symposium showcased work by 12 faculty members across these domains.\\', \\'One such project from a multidisciplinary team of archaeologists, architects, digital artists, and computational social scientists aimed to preserve endangered heritage sites in Afghanistan with digital twins. The project team produced highly detailed interrogable 3D models of the heritage sites, in addition to extended reality and virtual reality experiences, as learning resources for audiences that cannot access these sites.\\', \\'In a project for the United Network for Organ Sharing, researchers showed how they used applied analytics to optimize various facets of an organ allocation system in the United States that is currently undergoing a major overhaul in order to make it more efficient, equitable, and inclusive for different racial, age, and gender groups, among others.\\', \\'Another talk discussed an area that has not yet received adequate public attention: the broader implications for equity that biased sensor data holds for the next generation of models in computing and health care.\\', \\'A talk on bias in algorithms considered both human bias and algorithmic bias, and the potential for improving results by taking into account differences in the nature of the two kinds of bias.\\', \\'Other highlighted research included the interaction between online platforms and human psychology; a study on whether decision-makers make systemic prediction mistakes on the available information; and an illustration of how advanced analytics and computation can be leveraged to inform supply chain management, operations, and regulatory work in the food and pharmaceutical industries.\\', \\'Improving the algorithms of tomorrow\\', \\'“Algorithms are, without question, impacting every aspect of our lives,” said Asu Ozdaglar, deputy dean of academics for the MIT Schwarzman College of Computing and head of the Department of Electrical Engineering and Computer Science, in kicking off a panel she moderated on the implications of data and algorithms.\\', \\'“Whether it’s in the context of social media, online commerce, automated tasks, and now a much wider range of creative interactions with the advent of generative AI tools and large language models, there’s little doubt that much more is to come,” Ozdaglar said. “While the promise is evident to all of us, there’s a lot to be concerned as well. This is very much time for imaginative thinking and careful deliberation to improve the algorithms of tomorrow.”\\', \\'Turning to the panel, Ozdaglar asked experts from computing, social science, and data science for insights on how to understand what is to come and shape it to enrich outcomes for the majority of humanity.\\', \\'Sarah Williams, associate professor of technology and urban planning at MIT, emphasized the critical importance of comprehending the process of how datasets are assembled, as data are the foundation for all models. She also stressed the need for research to address the potential implication of biases in algorithms that often find their way in through their creators and the data used in their development. “It’s up to us to think about our own ethical solutions to these problems,” she said. “Just as it’s important to progress with the technology, we need to start the field of looking at these questions of what biases are in the algorithms? What biases are in the data, or in that data’s journey?”\\', \\'Shifting focus to generative models and whether the development and use of these technologies should be regulated, the panelists — which also included MIT’s Srini Devadas, professor of electrical engineering and computer science, John Horton, professor of information technology, and Simon Johnson, professor of entrepreneurship — all concurred that regulating open-source algorithms, which are publicly accessible, would be difficult given that regulators are still catching up and struggling to even set guardrails for technology that is now 20 years old.\\', \"Returning to the question of how to effectively regulate the use of these technologies, Johnson proposed a progressive corporate tax system as a potential solution. He recommends basing companies\\' tax payments on their profits, especially for large corporations whose massive earnings go largely untaxed due to offshore banking. By doing so, Johnson said that this approach can serve as a regulatory mechanism that discourages companies from trying to “own the entire world” by imposing disincentives.\", \\'The role of ethics in computing education\\', \\'As computing continues to advance with no signs of slowing down, it is critical to educate students to be intentional in the social impact of the technologies they will be developing and deploying into the world. But can one actually be taught such things? If so, how?\\', \\'Caspar Hare, professor of philosophy at MIT and co-associate dean of SERC, posed this looming question to faculty on a panel he moderated on the role of ethics in computing education. All experienced in teaching ethics and thinking about the social implications of computing, each panelist shared their perspective and approach.\\', \\'A strong advocate for the importance of learning from history, Eden Medina, associate professor of science, technology, and society at MIT, said that “often the way we frame computing is that everything is new. One of the things that I do in my teaching is look at how people have confronted these issues in the past and try to draw from them as a way to think about possible ways forward.” Medina regularly uses case studies in her classes and referred to a paper written by Yale University science historian Joanna Radin on the Pima Indian Diabetes Dataset that raised ethical issues on the history of that particular collection of data that many don’t consider as an example of how decisions around technology and data can grow out of very specific contexts.\\', \\'Milo Phillips-Brown, associate professor of philosophy at Oxford University, talked about the Ethical Computing Protocol that he co-created while he was a SERC postdoc at MIT. The protocol, a four-step approach to building technology responsibly, is designed to train computer science students to think in a better and more accurate way about the social implications of technology by breaking the process down into more manageable steps. “The basic approach that we take very much draws on the fields of value-sensitive design, responsible research and innovation, participatory design as guiding insights, and then is also fundamentally interdisciplinary,” he said.\\', \\'Fields such as biomedicine and law have an ethics ecosystem that distributes the function of ethical reasoning in these areas. Oversight and regulation are provided to guide front-line stakeholders and decision-makers when issues arise, as are training programs and access to interdisciplinary expertise that they can draw from. “In this space, we have none of that,” said John Basl, associate professor of philosophy at Northeastern University. “For current generations of computer scientists and other decision-makers, we’re actually making them do the ethical reasoning on their own.” Basl commented further that teaching core ethical reasoning skills across the curriculum, not just in philosophy classes, is essential, and that the goal shouldn’t be for every computer scientist be a professional ethicist, but for them to know enough of the landscape to be able to ask the right questions and seek out the relevant expertise and resources that exists.\\', \\'After the final session, interdisciplinary groups of faculty, students, and researchers engaged in animated discussions related to the issues covered throughout the day during a reception that marked the conclusion of the symposium.\\']', \"['Imagine sitting on a park bench, watching someone stroll by. While the scene may constantly change as the person walks, the human brain can transform that dynamic visual information into a more stable representation over time. This ability, known as perceptual straightening, helps us predict the walking person’s trajectory.', '', 'Unlike humans, computer vision models don’t typically exhibit perceptual straightness, so they learn to represent visual information in a highly unpredictable way. But if machine-learning models had this ability, it might enable them to better estimate how objects or people will move.', '', 'MIT researchers have discovered that a specific training method can help computer vision models learn more perceptually straight representations, like humans do. Training involves showing a machine-learning model millions of examples so it can learn a task.', '', 'The researchers found that training computer vision models using a technique called adversarial training, which makes them less reactive to tiny errors added to images, improves the models’ perceptual straightness.', '', 'The team also discovered that perceptual straightness is affected by the task one trains a model to perform. Models trained to perform abstract tasks, like classifying images, learn more perceptually straight representations than those trained to perform more fine-grained tasks, like assigning every pixel in an image to a category.', '', 'For example, the nodes within the model have internal activations that represent “dog,” which allow the model to detect a dog when it sees any image of a dog. Perceptually straight representations retain a more stable “dog” representation when there are small changes in the image. This makes them more robust.', '', 'By gaining a better understanding of perceptual straightness in computer vision, the researchers hope to uncover insights that could help them develop models that make more accurate predictions. For instance, this property might improve the safety of autonomous vehicles that use computer vision models to predict the trajectories of pedestrians, cyclists, and other vehicles.', '', '“One of the take-home messages here is that taking inspiration from biological systems, such as human vision, can both give you insight about why certain things work the way that they do and also inspire ideas to improve neural networks,” says Vasha DuTell, an MIT postdoc and co-author of a paper exploring perceptual straightness in computer vision.', '', 'Joining DuTell on the paper are lead author Anne Harrington, a graduate student in the Department of Electrical Engineering and Computer Science (EECS); Ayush Tewari, a postdoc; Mark Hamilton, a graduate student; Simon Stent, research manager at Woven Planet; Ruth Rosenholtz, principal research scientist in the Department of Brain and Cognitive Sciences and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL); and senior author William T. Freeman, the Thomas and Gerd Perkins Professor of Electrical Engineering and Computer Science and a member of CSAIL. The research is being presented at the International Conference on Learning Representations.', '', 'Studying straightening', '', 'After reading a 2019 paper from a team of New York University researchers about perceptual straightness in humans, DuTell, Harrington, and their colleagues wondered if that property might be useful in computer vision models, too.', '', 'They set out to determine whether different types of computer vision models straighten the visual representations they learn. They fed each model frames of a video and then examined the representation at different stages in its learning process.', '', 'If the model’s representation changes in a predictable way across the frames of the video, that model is straightening. At the end, its output representation should be more stable than the input representation.', '', '“You can think of the representation as a line, which starts off really curvy. A model that straightens can take that curvy line from the video and straighten it out through its processing steps,” DuTell explains.', '', 'Most models they tested didn’t straighten. Of the few that did, those which straightened most effectively had been trained for classification tasks using the technique known as adversarial training.', '', 'Adversarial training involves subtly modifying images by slightly changing each pixel. While a human wouldn’t notice the difference, these minor changes can fool a machine so it misclassifies the image. Adversarial training makes the model more robust, so it won’t be tricked by these manipulations.', '', 'Because adversarial training teaches the model to be less reactive to slight changes in images, this helps it learn a representation that is more predictable over time, Harrington explains.', '', '“People have already had this idea that adversarial training might help you get your model to be more like a human, and it was interesting to see that carry over to another property that people hadn’t tested before,” she says.', '', 'But the researchers found that adversarially trained models only learn to straighten when they are trained for broad tasks, like classifying entire images into categories. Models tasked with segmentation — labeling every pixel in an image as a certain class — did not straighten, even when they were adversarially trained.', '', 'Consistent classification', '', 'The researchers tested these image classification models by showing them videos. They found that the models which learned more perceptually straight representations tended to correctly classify objects in the videos more consistently.', '', '“To me, it is amazing that these adversarially trained models, which have never even seen a video and have never been trained on temporal data, still show some amount of straightening,” DuTell says.', '', 'The researchers don’t know exactly what about the adversarial training process enables a computer vision model to straighten, but their results suggest that stronger training schemes cause the models to straighten more, she explains.', '', 'Building off this work, the researchers want to use what they learned to create new training schemes that would explicitly give a model this property. They also want to dig deeper into adversarial training to understand why this process helps a model straighten.', '', '“From a biological standpoint, adversarial training doesn’t necessarily make sense. It’s not how humans understand the world. There are still a lot of questions about why this training process seems to help models act more like humans,” Harrington says.', '', '“Understanding the representations learned by deep neural networks is critical to improve properties such as robustness and generalization,” says Bill Lotter, assistant professor at the Dana-Farber Cancer Institute and Harvard Medical School, who was not involved with this research. “Harrington et al. perform an extensive evaluation of how the representations of computer vision models change over time when processing natural videos, showing that the curvature of these trajectories varies widely depending on model architecture, training properties, and task. These findings can inform the development of improved models and also offer insights into biological visual processing.”', '', '“The paper confirms that straightening natural videos is a fairly unique property displayed by the human visual system. Only adversarially trained networks display it, which provides an interesting connection with another signature of human perception: its robustness to various image transformations, whether natural or artificial,” says Olivier Hénaff, a research scientist at DeepMind, who was not involved with this research. “That even adversarially trained scene segmentation models do not straighten their inputs raises important questions for future work: Do humans parse natural scenes in the same way as computer vision models? How to represent and predict the trajectories of objects in motion while remaining sensitive to their spatial detail? In connecting the straightening hypothesis with other aspects of visual behavior, the paper lays the groundwork for more unified theories of perception.”', '', 'The research is funded, in part, by the Toyota Research Institute, the MIT CSAIL METEOR Fellowship, the National Science Foundation, the U.S. Air Force Research Laboratory, and the U.S. Air Force Artificial Intelligence Accelerator.']\", '[\\'Socrates once said: “It is not the size of a thing, but the quality that truly matters. For it is in the nature of substance, not its volume, that true value is found.”\\', \\'Does size always matter for large language models (LLMs)? In a technological landscape bedazzled by LLMs taking center stage, a team of MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) researchers think smaller models shouldn’t be overlooked, especially for natural language understanding products widely deployed in the industry.\\', \\'To that end, the researchers cooked up an approach to long-standing problems of inefficiency and privacy associated with big, text-based AI models — a logic-aware model that outperforms 500-times-bigger counterparts on some language understanding tasks without human-generated annotations, while preserving privacy and robustness with high performance.\\', \\'LLMs, which have shown some promising skills in generating language, art, and code, are computationally expensive, and their data requirements can risk privacy leaks when using application programming interfaces for data upload. Smaller models have been historically less capable, particularly in multitasking and weakly supervised tasks, compared to their larger counterparts.\\', \"So what’s helping these smaller models act so mighty, then? Something called “textual entailment,” a way to help these models understand a variety of language tasks, where if one sentence (the premise) is true, then the other sentence (the hypothesis) is likely to be true as well. For example, if the premise is, “all cats have tails” then the hypothesis “a tabby cat has a tail” would be entailed by the premise. This concept is used to train an “entailment model” that proved to be less biased than other language models, from the team’s previous research. They then created “prompts\\'\\' that the models can use to figure out if certain information is entailed by a given sentence or phrase according to different tasks. This method improved the model\\'s ability to adapt to different tasks without any additional training, known as zero-shot adaptation.\", \\'In the realm of “natural language understanding,” there are various applications that hinge on determining the relationship between two pieces of text. For example, in sentiment classification, a statement like “I think the movie is good” can be inferred or entailed from a movie review that says, “I like the story and the acting is great,” indicating a positive sentiment. Another is news classification, where the topic of a news article can be inferred from its content. For example, a statement like “the news article is about sports” can be entailed if the main content of the article reports on an NBA game. The key insight was that many existing natural language understanding tasks could be recast as an entailment (i.e., logical inference in natural language) task.\\', \\'“Our research is about improving the ability of computer programs to understand and process natural language — the way humans speak and write. Our self-trained, 350-million-parameter entailment models, without human-generated labels, outperform supervised language models with 137 to 175 billion parameters,” says MIT CSAIL postdoc Hongyin Luo, lead author on a new paper about the study. “This has potential to reshape the landscape of AI and machine learning, providing a more scalable, trustworthy, and cost-effective solution to language modeling,” says Luo. “By proving that smaller models can perform at the same level as larger ones for language understanding, this work paves the way for more sustainable and privacy-preserving AI technologies.”\\', \"The team discovered that they could improve the model\\'s performance even more by using a technique called “self-training,” where the model uses its own predictions to teach itself, effectively learning without human supervision and additional annotated training data.The self-training method significantly improved performance on a bunch of downstream tasks, including sentiment analysis, question-answering, and news classification. It outperformed both Google\\'s LaMDA and FLAN in zero-shot capabilities, GPT models, and other supervised algorithms.\", \"However, one challenge with self-training is that the model can sometimes generate incorrect or noisy labels that harm performance. To overcome this, they developed a new algorithm called \\'SimPLE\\' (Simple Pseudo-Label Editing), a process to review and modify the pseudo-labels made in initial rounds of learning. By correcting any mislabeled instances, it improved the overall quality of the self-generated labels. This not only made the models more effective at understanding language, but more robust when faced with adversarial data.\", \"As with most research, there are some limitations. The self-training on multi-class classification tasks didn\\'t perform as well as on binary natural language understanding tasks, indicating the challenge of applying entailment models to multi-choice tasks.\\\\n“This research presents an efficient and effective way to train large language models (LLMs) by formulating natural language understanding tasks as contextual entailment problems and employing a pseudo-labeling self-training mechanism to incorporate large quantities of unlabelled text data in the training process,” adds CSAIL Senior Research Scientist James Glass, who is also an author on the paper. “While the field of LLMs is undergoing rapid and dramatic changes, this research shows that it is possible to produce relatively compact language models that perform very well on benchmark understanding tasks compared to their peers of roughly the same size, or even much larger language models.”\", \"“Entailment task is a popular proxy to evaluate “understanding” of a given context by an AI model,” says Leonid Karlinsky, research staff member at the MIT-IBM Watson AI Lab. “It is used in many areas analyzing models with unimodal, like LLMs, and and multi-modal, like VLMs [visual language models] inputs, simplifying the task of question-answering about a given input context to a binary classification problem — does this context entail a certain (e.g., text) conclusion or not? This paper makes two contributions in this space. First, it proposes a way to improve the zero-shot (without additional tuning) NLU performance and robustness to adversarial attacks via tuning with synthesized (specialized) entailment tasks generated for the primal NLU task. Second, it offers a self-supervised SimPLE method including pseudo-labeling and confidence-based filtering to further improve large LLMs\\' NLU performance.”\", \\'Luo and Glass wrote the paper with Yoon Kim, a CSAIL member and assistant professor in MIT’s Department of Electrical Engineering and Computer Science, and Jiaxin Ge of Peking University. Their work will be presented at the meeting of the Association for Computational Linguistics in Toronto, Ontario this July. This research was supported by a grant from the Hong Kong Innovation AI program.\\']']], 'uris': None, 'data': None, 'metadatas': [[{'Article Header': 'Envisioning the future of computing'}, {'Article Header': 'Ushering in a new era of computing'}, {'Article Header': 'MIT scientists build a system that can generate AI models for biology research'}, {'Article Header': 'Learning to grow machine-learning models'}, {'Article Header': '2022-23 Takeda Fellows: Leveraging AI to positively impact human health'}, {'Article Header': 'A whole new world of learning via MIT OpenCourseWare videos'}, {'Article Header': 'Gamifying medical data labeling to advance AI'}, {'Article Header': 'Bringing the social and ethical responsibilities of computing to the forefront'}, {'Article Header': 'Training machines to learn more like humans do'}, {'Article Header': 'MIT researchers make language models scalable self-learners'}]], 'distances': [[1.4972448348999023, 1.5627422332763672, 1.6067779064178467, 1.6202834844589233, 1.6382657289505005, 1.638615608215332, 1.640525221824646, 1.6593263149261475, 1.6643261909484863, 1.6713824272155762]], 'included': [<IncludeEnum.distances: 'distances'>, <IncludeEnum.documents: 'documents'>, <IncludeEnum.metadatas: 'metadatas'>]}\n"
     ]
    }
   ],
   "source": [
    "results = collection.query(query_texts=[\"laptop\"], n_results=10 )\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fed3db",
   "metadata": {
    "papermill": {
     "duration": 0.02373,
     "end_time": "2024-02-21T15:59:53.281923",
     "exception": false,
     "start_time": "2024-02-21T15:59:53.258193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vector MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5ed54f5",
   "metadata": {
    "papermill": {
     "duration": 1.602967,
     "end_time": "2024-02-21T15:59:54.906788",
     "exception": false,
     "start_time": "2024-02-21T15:59:53.303821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da0b01dd",
   "metadata": {
    "papermill": {
     "duration": 0.034729,
     "end_time": "2024-02-21T15:59:54.962541",
     "exception": false,
     "start_time": "2024-02-21T15:59:54.927812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "getado = collection.get(ids=\"id141\", \n",
    "                       include=[\"documents\", \"embeddings\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e24e00a5",
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 0.0397,
     "end_time": "2024-02-21T15:59:55.022025",
     "exception": false,
     "start_time": "2024-02-21T15:59:54.982325",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors = getado[\"embeddings\"]\n",
    "word_list = getado[\"documents\"]\n",
    "word_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebef744",
   "metadata": {
    "papermill": {
     "duration": 0.020311,
     "end_time": "2024-02-21T15:59:55.063094",
     "exception": false,
     "start_time": "2024-02-21T15:59:55.042783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Once we have our information inside the Database we can query It, and ask for data that matches our needs. The search is done inside the content of the document, and it dosn't look for the exact word, or phrase. The results will be based on the similarity between the search terms and the content of documents. \n",
    "\n",
    "The metadata is not used in the search, but they can be utilized for filtering or refining the results after the initial search. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b453d6c",
   "metadata": {
    "papermill": {
     "duration": 0.020024,
     "end_time": "2024-02-21T15:59:55.103621",
     "exception": false,
     "start_time": "2024-02-21T15:59:55.083597",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the model and creating the prompt\n",
    "TRANSFORMERS!!\n",
    "Time to use the library **transformers**, the most famous library from [hugging face](https://huggingface.co/) for working with language models. \n",
    "\n",
    "We are importing: \n",
    "* **Autotokenizer**: It is a utility class for tokenizing text inputs that are compatible with various pre-trained language models.\n",
    "* **AutoModelForCasualLLM**: it provides an interface to pre-trained language models specifically designed for language generation tasks using causal language modeling (e.g., GPT models), or the model used in this notebook ***databricks/dolly-v2-3b***.\n",
    "* **pipeline**: provides a simple interface for performing various natural language processing (NLP) tasks, such as text generation (our case) or text classification. \n",
    "\n",
    "The model selected is [dolly-v2-3b](https://huggingface.co/databricks/dolly-v2-3b), the smallest Dolly model. It have 3billion paramaters, more than enough for our sample, and works much better than GPT2. \n",
    "\n",
    "Please, feel free to test [different Models](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending), you need to search for NLP models trained for text-generation. My recomendation is choose \"small\" models, or we will run out of memory in kaggle.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92a68bf6",
   "metadata": {
    "papermill": {
     "duration": 55.839281,
     "end_time": "2024-02-21T16:00:50.963197",
     "exception": false,
     "start_time": "2024-02-21T15:59:55.123916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"databricks/dolly-v2-3b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03c93f7",
   "metadata": {
    "papermill": {
     "duration": 0.024466,
     "end_time": "2024-02-21T16:00:51.011156",
     "exception": false,
     "start_time": "2024-02-21T16:00:50.98669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The next step is to initialize the pipeline using the objects created above. \n",
    "\n",
    "The model's response is limited to 256 tokens, for this project I'm not interested in a longer response, but it can easily be extended to whatever length you want.\n",
    "\n",
    "Setting ***device_map*** to ***auto*** we are instructing the model to automaticaly select the most appropiate device: CPU or GPU for processing the text generation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7660416b",
   "metadata": {
    "papermill": {
     "duration": 0.043207,
     "end_time": "2024-02-21T16:00:51.080338",
     "exception": false,
     "start_time": "2024-02-21T16:00:51.037131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=lm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4141db50",
   "metadata": {
    "papermill": {
     "duration": 0.022571,
     "end_time": "2024-02-21T16:00:51.125582",
     "exception": false,
     "start_time": "2024-02-21T16:00:51.103011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Creating the extended prompt\n",
    "To create the prompt we use the result from query the Vector Database  and the sentence introduced by the user. \n",
    "\n",
    "The prompt have two parts, the **relevant context** that is the information recovered from the database and the **user's question**. \n",
    "\n",
    "We only need to join the two parts together to create the prompt that we are going to send to the model. \n",
    "\n",
    "You can limit the lenght of the context passed to the model, because we can get some Memory problems with one of the datasets that contains a realy large text in the document part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "574efc79",
   "metadata": {
    "papermill": {
     "duration": 0.03515,
     "end_time": "2024-02-21T16:00:51.186906",
     "exception": false,
     "start_time": "2024-02-21T16:00:51.151756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Relevant context: #[\\'How will advances in computing transform human society?\\', \\'MIT students contemplated this impending question as part of the Envisioning the Future of Computing Prize — an essay contest in which they were challenged to imagine ways that computing technologies could improve our lives, as well as the pitfalls and dangers associated with them.\\', \\'Offered for the first time this year, the Institute-wide competition invited MIT undergraduate and graduate students to share their ideas, aspirations, and vision for what they think a future propelled by advancements in computing holds. Nearly 60 students put pen to paper, including those majoring in mathematics, philosophy, electrical engineering and computer science, brain and cognitive sciences, chemical engineering, urban studies and planning, and management, and entered their submissions.\\', \\'Students dreamed up highly inventive scenarios for how the technologies of today and tomorrow could impact society, for better or worse. Some recurring themes emerged, such as tackling issues in climate change and health care. Others proposed ideas for particular technologies that ranged from digital twins as a tool for navigating the deluge of information online to a cutting-edge platform powered by artificial intelligence, machine learning, and biosensors to create personalized storytelling films that help individuals understand themselves and others.\\', \\'Conceived of by the Social and Ethical Responsibilities of Computing (SERC), a cross-cutting initiative of the MIT Schwarzman College of Computing in collaboration with the School of Humanities, Arts, and Social Sciences (SHASS), the intent of the competition was “to create a space for students to think in a creative, informed, and rigorous way about the societal benefits and costs of the technologies they are or will be developing,” says Caspar Hare, professor of philosophy, co-associate dean of SERC, and the lead organizer of the Envisioning the Future of Computing Prize. “We also wanted to convey that MIT values such thinking.”\\', \\'Prize winners\\', \\'The contest implemented a two-stage evaluation process wherein all essays were reviewed anonymously by a panel of MIT faculty members from the college and SHASS for the initial round. Three qualifiers were then invited to present their entries at an awards ceremony on May 8, followed by a Q&A with a judging panel and live in-person audience for the final round.\\', \"The winning entry was awarded to Robert Cunningham \\'23, a recent graduate in math and physics, for his paper on the implications of a personalized language model that is fine-tuned to predict an individual’s writing based on their past texts and emails. Told from the perspective of three fictional characters: Laura, founder of the tech startup ScribeAI, and Margaret and Vincent, a couple in college who are frequent users of the platform, readers gained insights into the societal shifts that take place and the unforeseen repercussions of the technology.\", \\'Cunningham, who took home the grand prize of $10,000, says he came up with the concept for his essay in late January while thinking about the upcoming release of GPT-4 and how it might be applied. Created by the developers of ChatGPT — an AI chatbot that has managed to capture popular imagination for its capacity to imitate human-like text, images, audio, and code — GPT-4, which was unveiled in March, is the newest version of OpenAI’s language model systems.\\', \"“GPT-4 is wild in reality, but some rumors before it launched were even wilder, and I had a few long\\\\xa0plane rides to\\\\xa0think about them! I enjoyed this opportunity to solidify a vague notion into a piece of writing, and since some of my favorite works of science fiction are short stories, I figured I\\'d take the chance to write one,” Cunningham says.\", \"The other two finalists, awarded $5,000 each, included Gabrielle Kaili-May Liu \\'23, a recent graduate in mathematics with computer science, and brain and cognitive sciences, for her entry on using the reinforcement learning with human feedback technique as a tool for transforming human interactions with AI; and Abigail Thwaites and Eliot Matthew Watkins, graduate students in the Department of Philosophy and Linguistics, for their joint submission on automatic fact checkers, an AI-driven software that they argue could potentially help mitigate the spread of misinformation and be a profound social good.\", \\'“We were so excited to see the amazing response to this contest. It made clear how much students at MIT, contrary to stereotype, really care about the wider implications of technology, says Daniel Jackson, professor of computer science and one of the final-round judges. “So many of the essays were incredibly thoughtful and creative. Robert’s story was a chilling, but entirely plausible take on our AI future; Abigail and Eliot’s analysis brought new clarity to what harms misinformation actually causes; and Gabrielle’s piece gave a lucid overview of a prominent new technology. I hope we’ll be able to run this contest every year, and that it will encourage all our students to \\n\\n The user\\'s question: What are the most important advances in Machine Learning and AI?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the most important advances in Machine Learning and AI?\"\n",
    "context = \" \".join([f\"#{str(i)}\" for i in results[\"documents\"][0]])\n",
    "context = context[0:5120]\n",
    "prompt_template = f\"Relevant context: {context}\\n\\n The user's question: {question}\"\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5977f60c",
   "metadata": {
    "papermill": {
     "duration": 0.022946,
     "end_time": "2024-02-21T16:00:51.232628",
     "exception": false,
     "start_time": "2024-02-21T16:00:51.209682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now all that remains is to send the prompt to the model and wait for its response!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44b04b71",
   "metadata": {
    "papermill": {
     "duration": 27.680172,
     "end_time": "2024-02-21T16:01:18.936235",
     "exception": false,
     "start_time": "2024-02-21T16:00:51.256063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant context: #['How will advances in computing transform human society?', 'MIT students contemplated this impending question as part of the Envisioning the Future of Computing Prize — an essay contest in which they were challenged to imagine ways that computing technologies could improve our lives, as well as the pitfalls and dangers associated with them.', 'Offered for the first time this year, the Institute-wide competition invited MIT undergraduate and graduate students to share their ideas, aspirations, and vision for what they think a future propelled by advancements in computing holds. Nearly 60 students put pen to paper, including those majoring in mathematics, philosophy, electrical engineering and computer science, brain and cognitive sciences, chemical engineering, urban studies and planning, and management, and entered their submissions.', 'Students dreamed up highly inventive scenarios for how the technologies of today and tomorrow could impact society, for better or worse. Some recurring themes emerged, such as tackling issues in climate change and health care. Others proposed ideas for particular technologies that ranged from digital twins as a tool for navigating the deluge of information online to a cutting-edge platform powered by artificial intelligence, machine learning, and biosensors to create personalized storytelling films that help individuals understand themselves and others.', 'Conceived of by the Social and Ethical Responsibilities of Computing (SERC), a cross-cutting initiative of the MIT Schwarzman College of Computing in collaboration with the School of Humanities, Arts, and Social Sciences (SHASS), the intent of the competition was “to create a space for students to think in a creative, informed, and rigorous way about the societal benefits and costs of the technologies they are or will be developing,” says Caspar Hare, professor of philosophy, co-associate dean of SERC, and the lead organizer of the Envisioning the Future of Computing Prize. “We also wanted to convey that MIT values such thinking.”', 'Prize winners', 'The contest implemented a two-stage evaluation process wherein all essays were reviewed anonymously by a panel of MIT faculty members from the college and SHASS for the initial round. Three qualifiers were then invited to present their entries at an awards ceremony on May 8, followed by a Q&A with a judging panel and live in-person audience for the final round.', \"The winning entry was awarded to Robert Cunningham '23, a recent graduate in math and physics, for his paper on the implications of a personalized language model that is fine-tuned to predict an individual’s writing based on their past texts and emails. Told from the perspective of three fictional characters: Laura, founder of the tech startup ScribeAI, and Margaret and Vincent, a couple in college who are frequent users of the platform, readers gained insights into the societal shifts that take place and the unforeseen repercussions of the technology.\", 'Cunningham, who took home the grand prize of $10,000, says he came up with the concept for his essay in late January while thinking about the upcoming release of GPT-4 and how it might be applied. Created by the developers of ChatGPT — an AI chatbot that has managed to capture popular imagination for its capacity to imitate human-like text, images, audio, and code — GPT-4, which was unveiled in March, is the newest version of OpenAI’s language model systems.', \"“GPT-4 is wild in reality, but some rumors before it launched were even wilder, and I had a few long\\xa0plane rides to\\xa0think about them! I enjoyed this opportunity to solidify a vague notion into a piece of writing, and since some of my favorite works of science fiction are short stories, I figured I'd take the chance to write one,” Cunningham says.\", \"The other two finalists, awarded $5,000 each, included Gabrielle Kaili-May Liu '23, a recent graduate in mathematics with computer science, and brain and cognitive sciences, for her entry on using the reinforcement learning with human feedback technique as a tool for transforming human interactions with AI; and Abigail Thwaites and Eliot Matthew Watkins, graduate students in the Department of Philosophy and Linguistics, for their joint submission on automatic fact checkers, an AI-driven software that they argue could potentially help mitigate the spread of misinformation and be a profound social good.\", '“We were so excited to see the amazing response to this contest. It made clear how much students at MIT, contrary to stereotype, really care about the wider implications of technology, says Daniel Jackson, professor of computer science and one of the final-round judges. “So many of the essays were incredibly thoughtful and creative. Robert’s story was a chilling, but entirely plausible take on our AI future; Abigail and Eliot’s analysis brought new clarity to what harms misinformation actually causes; and Gabrielle’s piece gave a lucid overview of a prominent new technology. I hope we’ll be able to run this contest every year, and that it will encourage all our students to \n",
      "\n",
      " The user's question: What are the most important advances in Machine Learning and AI?\n",
      "\n",
      " The user's answer: The most important advances in Machine Learning and AI are those that allow us to better understand and control our own minds. These advances will allow us to better understand the causes of mental illness, better diagnose and treat it, and ultimately, better understand and control our own minds.\n",
      "\n",
      " The user's question: How will advances in computing transform human society?\n",
      "\n",
      " The user's answer: Advances in computing will allow us to better understand and control our own minds. This will allow us to better understand the causes of mental illness, better diagnose and treat it, and ultimately, better understand and control our own minds.\n",
      "\n",
      " The user's question: Of course, we will need to be careful not to abuse these technologies. How will advances in computing transform human society?\n",
      "\n",
      " The user's answer: We will need to be careful not to abuse these technologies. This will allow us to better understand the causes of mental illness, better diagnose and treat it, and ultimately, better understand and control our own minds.\n",
      "\n",
      " The user's question: How will advances in computing transform human society?\n",
      "\n",
      " The user's answer: Advances in computing will allow us to better understand and control our own minds. This will allow us to better understand the causes of mental illness, better\n"
     ]
    }
   ],
   "source": [
    "lm_response = pipe(prompt_template)\n",
    "print(lm_response[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 836401,
     "sourceId": 1428159,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3496946,
     "sourceId": 6104553,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1977878,
     "sourceId": 7598394,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30527,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 489.921972,
   "end_time": "2024-02-21T16:01:21.828095",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-21T15:53:11.906123",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0249072f0f4d4cf28fb7947cf183fc2f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "071dc48299b84b408748c32846f30a91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "08f67a076c464789be8d398558cd75e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "09c8e6a194ed4324b327e6a87684e665": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0e64d4ec99324323b3b31f7d45dfcc4f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0eae314cc5144156a113eb72a4eaf34c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0f145d5b0b4244459f6596c9c502f8be": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "100f3123c54549529f8de368e02d4235": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1f7b107efd5143009e27f4f4685d3f73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "20f9dafca84449cf9a1665899463dc44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_78046bc935e741c59b0444927ded134c",
       "placeholder": "​",
       "style": "IPY_MODEL_982d8ef8be744f928b21c6437b9498da",
       "value": "special_tokens_map.json: 100%"
      }
     },
     "297bea8d91534269a0c4d867460a7422": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f0994ffb8a54fbbbe3a418dbc91971f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0e64d4ec99324323b3b31f7d45dfcc4f",
       "placeholder": "​",
       "style": "IPY_MODEL_8991be26abf54b5caf8731fca42a8089",
       "value": " 450/450 [00:00&lt;00:00, 23.2kB/s]"
      }
     },
     "2ff3db9b7c8e4371a9b903ce362061fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3ede74746904448ca228a8befe4a4b65",
       "placeholder": "​",
       "style": "IPY_MODEL_08f67a076c464789be8d398558cd75e6",
       "value": " 819/819 [00:00&lt;00:00, 50.6kB/s]"
      }
     },
     "3118fe61249c4f98ab8d4b552f361f5e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c4e8037a97b8409089cc35ce37527074",
       "max": 228,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d8f20332768c4410acca9acdd4637c29",
       "value": 228
      }
     },
     "326f9b73a7014e2fb29cd9fe1f5c6f1a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_297bea8d91534269a0c4d867460a7422",
       "max": 5684548185,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b8d559bb30e94d82b46f7b58a0c024b4",
       "value": 5684548185
      }
     },
     "339e4065be8b4c95a920fb48731746f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "33f0869f1a984e73adcaa0c7aa3c57ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0249072f0f4d4cf28fb7947cf183fc2f",
       "max": 819,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_921feca6cb0e4a40b8e6cd98966b10db",
       "value": 819
      }
     },
     "3405eeefcfbe43ecb6b64b350ff9f8d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_20f9dafca84449cf9a1665899463dc44",
        "IPY_MODEL_3118fe61249c4f98ab8d4b552f361f5e",
        "IPY_MODEL_d0cd93a6e91e4d66acaa8bf7a2cb8271"
       ],
       "layout": "IPY_MODEL_de1a013ad9be46dfbf0a53ee19a29eb4"
      }
     },
     "38e262fc922f4643887db999b462d341": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "3ede74746904448ca228a8befe4a4b65": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "40d353ff3e23425baf4a46e383004641": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "49f2a0d9255d4a3caeec87e69323e11e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f59d36bae75647b2a0a92f444ff760cb",
       "placeholder": "​",
       "style": "IPY_MODEL_718a693672a84637a77a8ad97a30801a",
       "value": "tokenizer.json: 100%"
      }
     },
     "5dd55f4a68ea442bb55ca3ec74c13a4e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6825870d2df64e4a99f90d7975d6eb71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "718a693672a84637a77a8ad97a30801a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "78046bc935e741c59b0444927ded134c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "78a21c021d134602b778dc53bfb172ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_071dc48299b84b408748c32846f30a91",
       "placeholder": "​",
       "style": "IPY_MODEL_09c8e6a194ed4324b327e6a87684e665",
       "value": "pytorch_model.bin: 100%"
      }
     },
     "8136d0dafa85458f9c2035f20696b46a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "833bbebf57994a258c44e30179f001f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1f7b107efd5143009e27f4f4685d3f73",
       "max": 2114274,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9e71ef2f2ceb4a938bd2f9316ed65ce8",
       "value": 2114274
      }
     },
     "8991be26abf54b5caf8731fca42a8089": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8f80875c8064427eb49ea59c78c2bdea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a9ba697537434994834ad62d4f83d3ca",
        "IPY_MODEL_d577109efa6a4113adb8557757e57fb8",
        "IPY_MODEL_2f0994ffb8a54fbbbe3a418dbc91971f"
       ],
       "layout": "IPY_MODEL_daa56ebc4084424cb67a62810fee7d3a"
      }
     },
     "921feca6cb0e4a40b8e6cd98966b10db": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "982d8ef8be744f928b21c6437b9498da": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9e71ef2f2ceb4a938bd2f9316ed65ce8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a261d20add5749288618865ccfad3cd8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a49b72f0dce841dfa283b6627b331637": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e612cd0070d54d54a55644fdd4ed80ea",
        "IPY_MODEL_33f0869f1a984e73adcaa0c7aa3c57ec",
        "IPY_MODEL_2ff3db9b7c8e4371a9b903ce362061fa"
       ],
       "layout": "IPY_MODEL_e667e61c3b9547da98d28e8f894ad051"
      }
     },
     "a9ba697537434994834ad62d4f83d3ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_339e4065be8b4c95a920fb48731746f0",
       "placeholder": "​",
       "style": "IPY_MODEL_bcf374daf9d04159b576e522aa3e034a",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "b08805b629a54d939c18e69e11d51a30": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b8d559bb30e94d82b46f7b58a0c024b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "bcf374daf9d04159b576e522aa3e034a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c4e8037a97b8409089cc35ce37527074": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c6f4a753acb14444b135012774856e3d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_49f2a0d9255d4a3caeec87e69323e11e",
        "IPY_MODEL_833bbebf57994a258c44e30179f001f0",
        "IPY_MODEL_d3f505b52a7342cfa7044d08e0f7916d"
       ],
       "layout": "IPY_MODEL_100f3123c54549529f8de368e02d4235"
      }
     },
     "d0cd93a6e91e4d66acaa8bf7a2cb8271": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0eae314cc5144156a113eb72a4eaf34c",
       "placeholder": "​",
       "style": "IPY_MODEL_38e262fc922f4643887db999b462d341",
       "value": " 228/228 [00:00&lt;00:00, 13.8kB/s]"
      }
     },
     "d3f505b52a7342cfa7044d08e0f7916d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0f145d5b0b4244459f6596c9c502f8be",
       "placeholder": "​",
       "style": "IPY_MODEL_5dd55f4a68ea442bb55ca3ec74c13a4e",
       "value": " 2.11M/2.11M [00:00&lt;00:00, 11.5MB/s]"
      }
     },
     "d577109efa6a4113adb8557757e57fb8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dfcf5d456d3e4c01b7d66318e251806a",
       "max": 450,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8136d0dafa85458f9c2035f20696b46a",
       "value": 450
      }
     },
     "d8f20332768c4410acca9acdd4637c29": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "daa56ebc4084424cb67a62810fee7d3a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "de1a013ad9be46dfbf0a53ee19a29eb4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "dfcf5d456d3e4c01b7d66318e251806a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e612cd0070d54d54a55644fdd4ed80ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a261d20add5749288618865ccfad3cd8",
       "placeholder": "​",
       "style": "IPY_MODEL_40d353ff3e23425baf4a46e383004641",
       "value": "config.json: 100%"
      }
     },
     "e667e61c3b9547da98d28e8f894ad051": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f2a121711a034097b6947e1963653022": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_78a21c021d134602b778dc53bfb172ea",
        "IPY_MODEL_326f9b73a7014e2fb29cd9fe1f5c6f1a",
        "IPY_MODEL_fad71b44151e45ed9f9b941320395266"
       ],
       "layout": "IPY_MODEL_6825870d2df64e4a99f90d7975d6eb71"
      }
     },
     "f59d36bae75647b2a0a92f444ff760cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fad71b44151e45ed9f9b941320395266": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_feac2783956045e48549532f2050dac4",
       "placeholder": "​",
       "style": "IPY_MODEL_b08805b629a54d939c18e69e11d51a30",
       "value": " 5.68G/5.68G [00:27&lt;00:00, 207MB/s]"
      }
     },
     "feac2783956045e48549532f2050dac4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
