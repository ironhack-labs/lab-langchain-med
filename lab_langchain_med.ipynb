{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "40261e9b-c536-41bc-8a0a-b7d7c6aa0baa",
      "metadata": {
        "id": "40261e9b-c536-41bc-8a0a-b7d7c6aa0baa"
      },
      "source": [
        "# Lab | LangChain Med\n",
        "\n",
        "## Objectives\n",
        "\n",
        "- continue on with lesson 2' example, use different datasets to test what we did in class. Some datasets are suggested in the notebook but feel free to scout other datasets on HuggingFace or Kaggle.\n",
        "- Find another model on Hugging Face and compare it.\n",
        "- Modify the prompt to fit your selected dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39ce0d3f",
      "metadata": {
        "papermill": {
          "duration": 0.028165,
          "end_time": "2024-02-21T15:58:20.016814",
          "exception": false,
          "start_time": "2024-02-21T15:58:19.988649",
          "status": "completed"
        },
        "tags": [],
        "id": "39ce0d3f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57dad70f",
      "metadata": {
        "papermill": {
          "duration": 0.016026,
          "end_time": "2024-02-21T15:58:20.049214",
          "exception": false,
          "start_time": "2024-02-21T15:58:20.033188",
          "status": "completed"
        },
        "tags": [],
        "id": "57dad70f"
      },
      "source": [
        "## Load the Dataset\n",
        "As you can see the notebook is ready to work with three different Datasets. Just uncomment the lines of the Dataset you want to use.\n",
        "\n",
        "I selected Datasets with News. Two of them have just a brief decription of the news, but the other contains the full text.\n",
        "\n",
        "As we are working in a free and limited space, I limited the number of news to use with the variable MAX_NEWS. Feel free to pull more if you have memory available.\n",
        "\n",
        "The name of the field containing the text of the new is stored in the variable *DOCUMENT* and the metadata in *TOPIC*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e70a47e0",
      "metadata": {
        "papermill": {
          "duration": 1.170763,
          "end_time": "2024-02-21T15:58:21.235862",
          "exception": false,
          "start_time": "2024-02-21T15:58:20.065099",
          "status": "completed"
        },
        "tags": [],
        "id": "e70a47e0"
      },
      "outputs": [],
      "source": [
        "# news = pd.read_csv('/kaggle/input/topic-labeled-news-dataset/labelled_newscatcher_dataset.csv', sep=';')\n",
        "# MAX_NEWS = 1000\n",
        "# DOCUMENT=\"title\"\n",
        "# TOPIC=\"topic\"\n",
        "\n",
        "#news = pd.read_csv('/kaggle/input/bbc-news/bbc_news.csv')\n",
        "#MAX_NEWS = 1000\n",
        "#DOCUMENT=\"description\"\n",
        "#TOPIC=\"title\"\n",
        "\n",
        "#news = pd.read_csv('/kaggle/input/mit-ai-news-published-till-2023/articles.csv')\n",
        "#MAX_NEWS = 100\n",
        "#DOCUMENT=\"Article Body\"\n",
        "#TOPIC=\"Article Header\"\n",
        "\n",
        "news = \"PICK A DATASET\" #Ideally pick one from the commented ones above"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeb276f4",
      "metadata": {
        "papermill": {
          "duration": 0.016461,
          "end_time": "2024-02-21T15:58:21.268282",
          "exception": false,
          "start_time": "2024-02-21T15:58:21.251821",
          "status": "completed"
        },
        "tags": [],
        "id": "aeb276f4"
      },
      "source": [
        "ChromaDB requires that the data has a unique identifier. We can make it with this statement, which will create a new column called **Id**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61c151df",
      "metadata": {
        "papermill": {
          "duration": 0.054847,
          "end_time": "2024-02-21T15:58:21.339906",
          "exception": false,
          "start_time": "2024-02-21T15:58:21.285059",
          "status": "completed"
        },
        "tags": [],
        "id": "61c151df"
      },
      "outputs": [],
      "source": [
        "news[\"id\"] = news.index\n",
        "news.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1849922b",
      "metadata": {
        "papermill": {
          "duration": 0.027701,
          "end_time": "2024-02-21T15:58:21.383814",
          "exception": false,
          "start_time": "2024-02-21T15:58:21.356113",
          "status": "completed"
        },
        "tags": [],
        "id": "1849922b"
      },
      "outputs": [],
      "source": [
        "#Because it is just a course we select a small portion of News.\n",
        "subset_news = news.head(MAX_NEWS)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "893babc1",
      "metadata": {
        "papermill": {
          "duration": 0.015939,
          "end_time": "2024-02-21T15:58:21.416088",
          "exception": false,
          "start_time": "2024-02-21T15:58:21.400149",
          "status": "completed"
        },
        "tags": [],
        "id": "893babc1"
      },
      "source": [
        "## Import and configure the Vector Database\n",
        "I'm going to use ChromaDB, the most popular OpenSource embedding Database.\n",
        "\n",
        "First we need to import ChromaDB, and after that import the **Settings** class from **chromadb.config** module. This class allows us to change the setting for the ChromaDB system, and customize its behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82db86aa",
      "metadata": {
        "papermill": {
          "duration": 1.095805,
          "end_time": "2024-02-21T15:58:22.528102",
          "exception": false,
          "start_time": "2024-02-21T15:58:21.432297",
          "status": "completed"
        },
        "tags": [],
        "id": "82db86aa"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "051bc3a1",
      "metadata": {
        "papermill": {
          "duration": 0.015938,
          "end_time": "2024-02-21T15:58:22.560953",
          "exception": false,
          "start_time": "2024-02-21T15:58:22.545015",
          "status": "completed"
        },
        "tags": [],
        "id": "051bc3a1"
      },
      "source": [
        "Now we need to create the seetings object calling the **Settings** function imported previously. We store the object in the variable **settings_chroma**.\n",
        "\n",
        "Is necessary to inform two parameters\n",
        "* chroma_db_impl. Here we specify the database implementation and the format how store the data. I choose ***duckdb***, because his high-performace. It operate primarly in memory. And is fully compatible with SQL. The store format ***parquet*** is good for tabular data. With good compression rates and performance.\n",
        "\n",
        "* persist_directory: It just contains the directory where the data will be stored. Is possible work without a directory and the data will be stored in memory without persistece, but Kaggle dosn't support that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b21a77ff",
      "metadata": {
        "papermill": {
          "duration": 0.640745,
          "end_time": "2024-02-21T15:58:23.217828",
          "exception": false,
          "start_time": "2024-02-21T15:58:22.577083",
          "status": "completed"
        },
        "tags": [],
        "id": "b21a77ff"
      },
      "outputs": [],
      "source": [
        "chroma_client = chromadb.PersistentClient(path=\"/path/to/persist/directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5176774",
      "metadata": {
        "papermill": {
          "duration": 0.016853,
          "end_time": "2024-02-21T15:58:23.252446",
          "exception": false,
          "start_time": "2024-02-21T15:58:23.235593",
          "status": "completed"
        },
        "tags": [],
        "id": "b5176774"
      },
      "source": [
        "## Filling and Querying the ChromaDB Database\n",
        "The Data in ChromaDB is stored in collections. If the collection exist we need to delete it.\n",
        "\n",
        "In the next lines, we are creating the collection by calling the ***create_collection*** function in the ***chroma_client*** created above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0cc5748",
      "metadata": {
        "papermill": {
          "duration": 0.090234,
          "end_time": "2024-02-21T15:58:23.358887",
          "exception": false,
          "start_time": "2024-02-21T15:58:23.268653",
          "status": "completed"
        },
        "tags": [],
        "id": "a0cc5748"
      },
      "outputs": [],
      "source": [
        "collection_name = \"news_collection\"\n",
        "if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n",
        "        chroma_client.delete_collection(name=collection_name)\n",
        "\n",
        "collection = chroma_client.create_collection(name=collection_name)\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "688831d1",
      "metadata": {
        "papermill": {
          "duration": 0.01831,
          "end_time": "2024-02-21T15:58:23.394771",
          "exception": false,
          "start_time": "2024-02-21T15:58:23.376461",
          "status": "completed"
        },
        "tags": [],
        "id": "688831d1"
      },
      "source": [
        "It's time to add the data to the collection. Using the function ***add*** we need to inform, at least ***documents***, ***metadatas*** and ***ids***.\n",
        "* In the **document** we store the big text, it's a different column in each Dataset.\n",
        "* In **metadatas**, we can informa a list of topics.\n",
        "* In **id** we need to inform an unique identificator for each row. It MUST be unique! I'm creating the ID using the range of MAX_NEWS.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fb1a28a",
      "metadata": {
        "papermill": {
          "duration": 89.680388,
          "end_time": "2024-02-21T15:59:53.091437",
          "exception": false,
          "start_time": "2024-02-21T15:58:23.411049",
          "status": "completed"
        },
        "tags": [],
        "id": "4fb1a28a"
      },
      "outputs": [],
      "source": [
        "\n",
        "collection.add(\n",
        "    documents=subset_news[DOCUMENT].tolist(),\n",
        "    metadatas=[{TOPIC: topic} for topic in subset_news[TOPIC].tolist()],\n",
        "    ids=[f\"id{x}\" for x in range(MAX_NEWS)],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff6a4fcf",
      "metadata": {
        "papermill": {
          "duration": 0.121938,
          "end_time": "2024-02-21T15:59:53.236515",
          "exception": false,
          "start_time": "2024-02-21T15:59:53.114577",
          "status": "completed"
        },
        "tags": [],
        "id": "ff6a4fcf"
      },
      "outputs": [],
      "source": [
        "results = collection.query(query_texts=[\"laptop\"], n_results=10 )\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31fed3db",
      "metadata": {
        "papermill": {
          "duration": 0.02373,
          "end_time": "2024-02-21T15:59:53.281923",
          "exception": false,
          "start_time": "2024-02-21T15:59:53.258193",
          "status": "completed"
        },
        "tags": [],
        "id": "31fed3db"
      },
      "source": [
        "## Vector MAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5ed54f5",
      "metadata": {
        "papermill": {
          "duration": 1.602967,
          "end_time": "2024-02-21T15:59:54.906788",
          "exception": false,
          "start_time": "2024-02-21T15:59:53.303821",
          "status": "completed"
        },
        "tags": [],
        "id": "f5ed54f5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da0b01dd",
      "metadata": {
        "papermill": {
          "duration": 0.034729,
          "end_time": "2024-02-21T15:59:54.962541",
          "exception": false,
          "start_time": "2024-02-21T15:59:54.927812",
          "status": "completed"
        },
        "tags": [],
        "id": "da0b01dd"
      },
      "outputs": [],
      "source": [
        "\n",
        "getado = collection.get(ids=\"id141\",\n",
        "                       include=[\"documents\", \"embeddings\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e24e00a5",
      "metadata": {
        "_kg_hide-output": true,
        "papermill": {
          "duration": 0.0397,
          "end_time": "2024-02-21T15:59:55.022025",
          "exception": false,
          "start_time": "2024-02-21T15:59:54.982325",
          "status": "completed"
        },
        "scrolled": true,
        "tags": [],
        "id": "e24e00a5"
      },
      "outputs": [],
      "source": [
        "word_vectors = getado[\"embeddings\"]\n",
        "word_list = getado[\"documents\"]\n",
        "word_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebef744",
      "metadata": {
        "papermill": {
          "duration": 0.020311,
          "end_time": "2024-02-21T15:59:55.063094",
          "exception": false,
          "start_time": "2024-02-21T15:59:55.042783",
          "status": "completed"
        },
        "tags": [],
        "id": "bebef744"
      },
      "source": [
        "Once we have our information inside the Database we can query It, and ask for data that matches our needs. The search is done inside the content of the document, and it dosn't look for the exact word, or phrase. The results will be based on the similarity between the search terms and the content of documents.\n",
        "\n",
        "The metadata is not used in the search, but they can be utilized for filtering or refining the results after the initial search.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b453d6c",
      "metadata": {
        "papermill": {
          "duration": 0.020024,
          "end_time": "2024-02-21T15:59:55.103621",
          "exception": false,
          "start_time": "2024-02-21T15:59:55.083597",
          "status": "completed"
        },
        "tags": [],
        "id": "4b453d6c"
      },
      "source": [
        "## Loading the model and creating the prompt\n",
        "TRANSFORMERS!!\n",
        "Time to use the library **transformers**, the most famous library from [hugging face](https://huggingface.co/) for working with language models.\n",
        "\n",
        "We are importing:\n",
        "* **Autotokenizer**: It is a utility class for tokenizing text inputs that are compatible with various pre-trained language models.\n",
        "* **AutoModelForCasualLLM**: it provides an interface to pre-trained language models specifically designed for language generation tasks using causal language modeling (e.g., GPT models), or the model used in this notebook ***databricks/dolly-v2-3b***.\n",
        "* **pipeline**: provides a simple interface for performing various natural language processing (NLP) tasks, such as text generation (our case) or text classification.\n",
        "\n",
        "The model selected is [dolly-v2-3b](https://huggingface.co/databricks/dolly-v2-3b), the smallest Dolly model. It have 3billion paramaters, more than enough for our sample, and works much better than GPT2.\n",
        "\n",
        "Please, feel free to test [different Models](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending), you need to search for NLP models trained for text-generation. My recomendation is choose \"small\" models, or we will run out of memory in kaggle.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92a68bf6",
      "metadata": {
        "papermill": {
          "duration": 55.839281,
          "end_time": "2024-02-21T16:00:50.963197",
          "exception": false,
          "start_time": "2024-02-21T15:59:55.123916",
          "status": "completed"
        },
        "tags": [],
        "id": "92a68bf6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_id = \"databricks/dolly-v2-3b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "lm_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c03c93f7",
      "metadata": {
        "papermill": {
          "duration": 0.024466,
          "end_time": "2024-02-21T16:00:51.011156",
          "exception": false,
          "start_time": "2024-02-21T16:00:50.98669",
          "status": "completed"
        },
        "tags": [],
        "id": "c03c93f7"
      },
      "source": [
        "The next step is to initialize the pipeline using the objects created above.\n",
        "\n",
        "The model's response is limited to 256 tokens, for this project I'm not interested in a longer response, but it can easily be extended to whatever length you want.\n",
        "\n",
        "Setting ***device_map*** to ***auto*** we are instructing the model to automaticaly select the most appropiate device: CPU or GPU for processing the text generation.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7660416b",
      "metadata": {
        "papermill": {
          "duration": 0.043207,
          "end_time": "2024-02-21T16:00:51.080338",
          "exception": false,
          "start_time": "2024-02-21T16:00:51.037131",
          "status": "completed"
        },
        "tags": [],
        "id": "7660416b"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=lm_model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4141db50",
      "metadata": {
        "papermill": {
          "duration": 0.022571,
          "end_time": "2024-02-21T16:00:51.125582",
          "exception": false,
          "start_time": "2024-02-21T16:00:51.103011",
          "status": "completed"
        },
        "tags": [],
        "id": "4141db50"
      },
      "source": [
        "## Creating the extended prompt\n",
        "To create the prompt we use the result from query the Vector Database  and the sentence introduced by the user.\n",
        "\n",
        "The prompt have two parts, the **relevant context** that is the information recovered from the database and the **user's question**.\n",
        "\n",
        "We only need to join the two parts together to create the prompt that we are going to send to the model.\n",
        "\n",
        "You can limit the lenght of the context passed to the model, because we can get some Memory problems with one of the datasets that contains a realy large text in the document part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "574efc79",
      "metadata": {
        "papermill": {
          "duration": 0.03515,
          "end_time": "2024-02-21T16:00:51.186906",
          "exception": false,
          "start_time": "2024-02-21T16:00:51.151756",
          "status": "completed"
        },
        "tags": [],
        "id": "574efc79"
      },
      "outputs": [],
      "source": [
        "# Construye el contexto a partir de las descripciones recuperadas\n",
        "context = \" \".join(results['documents'])\n",
        "\n",
        "# Pregunta del usuario\n",
        "question = \"¿Cuáles son las últimas novedades en tecnología?\"\n",
        "\n",
        "# Crea el prompt\n",
        "prompt_template = f\"Contexto relevante: {context}\\n\\nPregunta del usuario: {question}\\nRespuesta:\"\n",
        "\n",
        "# Genera la respuesta\n",
        "lm_response = pipe(prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5977f60c",
      "metadata": {
        "papermill": {
          "duration": 0.022946,
          "end_time": "2024-02-21T16:00:51.232628",
          "exception": false,
          "start_time": "2024-02-21T16:00:51.209682",
          "status": "completed"
        },
        "tags": [],
        "id": "5977f60c"
      },
      "source": [
        "Now all that remains is to send the prompt to the model and wait for its response!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b04b71",
      "metadata": {
        "papermill": {
          "duration": 27.680172,
          "end_time": "2024-02-21T16:01:18.936235",
          "exception": false,
          "start_time": "2024-02-21T16:00:51.256063",
          "status": "completed"
        },
        "tags": [],
        "id": "44b04b71"
      },
      "outputs": [],
      "source": [
        "lm_response = pipe(prompt_template)\n",
        "print(lm_response[0][\"generated_text\"])"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 836401,
          "sourceId": 1428159,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 3496946,
          "sourceId": 6104553,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 1977878,
          "sourceId": 7598394,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30527,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 489.921972,
      "end_time": "2024-02-21T16:01:21.828095",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-02-21T15:53:11.906123",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}